{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9e5005-55c3-4c7e-ab66-a3b3c1417a55",
   "metadata": {},
   "source": [
    "## Newton methods\n",
    "\n",
    "#### Problem:\n",
    "$$\n",
    "f(\\vec{x}) \\rightarrow min,\\\\\n",
    "f: \\Omega \\rightarrow \\mathbb{R}, \\\\\n",
    "\\Omega \\subset \\mathbb{R^n}, f(\\vec{x}) \\mbox{ is convex}, \\\\\n",
    "f(\\vec{x}) \\mbox{ - is twice diffirentiable on } \\Omega\\\\\n",
    "\\vec{x_*} \\in \\Omega, f_{min} = f(\\vec{x_*})\n",
    "$$\n",
    "\n",
    "We can greater efficency of finding $x_*$ if we use information not only about function gradient, but also Hessian $H(\\vec{x})$\n",
    "\n",
    "In simple variant on every k iteration function is approximated in neighborhood of point $\\vec{x}_{k-1}$ by quadratic function $\\phi_{k}(x)$ then $\\vec{x}_k$ is found and the procedure continiues\n",
    "\n",
    "By using Tailor series we can represent our function in neighborhood of point $x_{k}$ as\n",
    "$$\n",
    "f(\\vec{x}) = f(\\vec{x}_{k} + (\\nabla f(\\vec{x}_k), \\vec{x} - \\vec{x}_k) + \\frac{1}{2}(H(\\vec{x}_k)(\\vec{x} - \\vec{x}_k), \\vec{x} - \\vec{x}_k) + o(|\\vec{x} - \\vec{x}_k|)\n",
    "$$\n",
    "\n",
    "So our quadratic approximation $\\phi_k(\\vec{x})$ would be\n",
    "$$\n",
    "\\phi_{k+1}(\\vec{x}) = f(\\vec{x}_{k} + (\\nabla f(\\vec{x}_k), \\vec{x} - \\vec{x}_k) + \\frac{1}{2}(H(\\vec{x}_k)(\\vec{x} - \\vec{x}_k), \\vec{x} - \\vec{x}_k)\n",
    "$$\n",
    "\n",
    "If our $H(\\vec{x}_{k})$ is positive determined (function is convex), then $\\vec{x}_{k+1}$ is single minimum of quadratic approximation and can be found using: \n",
    "$$ \n",
    "\\nabla \\phi_{k+1}(\\vec{x}) = \\nabla f(\\vec{x}_k) + H(\\vec{x}_k)(\\vec{x} - \\vec{x}_k) = \\vec{0}\n",
    "$$\n",
    "\n",
    "Then we get\n",
    "$$\n",
    "\\vec{x}_{k+1} = \\vec{x}_{k}  - H^{-1}(\\vec{x}_{k}) \\nabla f(\\vec{x}_{k})\n",
    "$$\n",
    "\n",
    "If our dimension number $n$ of space $\\mathbb{R}$ is big enough, then finding $H^{-1}$ is very big problem. In this case it expedient to find minimum of $\\phi_k(\\vec{x})$ by using **gradient methods** or **conjugate directions method**\n",
    "$\\widetilde{\\vec{x}}_k = argmin\\{\\phi_{k}(\\vec{x})\\}$ is just an approximation, using this we can build *relaxational sequence* \n",
    "\n",
    "$$\n",
    "\\vec{x}_{k} = \\vec{x}_{k-1} + \\lambda_k(\\widetilde{\\vec{x}}_{k} - \\vec{x}_{k-1}) = \\vec{x}_{k-1} + \\lambda_k\\vec{p}_{k} \\\\ \n",
    "\\vec{p}_k = -H^{-1}(\\vec{x}_{k-1}) \\nabla f(\\vec{x}_{k-1}) \\mbox{ - direction of descent}\n",
    "$$\n",
    "\n",
    "We can find $\\lambda_k$ different ways, for example find $argmin\\{f(\\vec{x}_{k-1} + \\lambda_k\\vec{p}_k\\}$ or by method of step splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3abc9010-594e-40d8-8e55-7d4bf5d5da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mplib\n",
    "import math as m\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy import linalg\n",
    "from scipy import sparse\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import derivative\n",
    "\n",
    "from onedim_optimize import quadratic_approx, fibbonaci_method, middle_point_method, upgraded_newton, qubic_approx, brent_optimize\n",
    "from scipy.optimize import approx_fprime, line_search\n",
    "import matplotlib.animation as pltanimation\n",
    "from test_functions import *\n",
    "from animations import Animate3D\n",
    "\n",
    "def toOneParamFunc(f, x, w):\n",
    "    return lambda p: f(x + p*w) \n",
    "\n",
    "def argmin(f, a, b, eps, onedim_opti):\n",
    "#     fig, ax = plt.subplots()\n",
    "#     ax.plot(np.linspace(a, b, 1000), [f(y) for y in np.linspace(a, b, 1000)])\n",
    "    f_ev, j_ev = 0, 0\n",
    "#     x, f_ev =  upgraded_newton(f, a, b, eps)\n",
    "    x, f_ev = onedim_opti(f, a, b, eps)\n",
    "#     ax.scatter(x, f(x))\n",
    "    return x, f_ev\n",
    "\n",
    "def approx_gradient(f, eps):\n",
    "    return lambda x: approx_fprime(x, f, eps)\n",
    "\n",
    "def hessian_in_point(x, f, grad, eps):\n",
    "     gr = grad(x)\n",
    "     n = len(gr) \n",
    "     hes = []\n",
    "     for i in range (0, n):\n",
    "        x_delta = np.array(x[:])\n",
    "        x_delta[i] = x_delta[i] + eps\n",
    "        gr_delta = grad(x_delta)\n",
    "        partials = np.array([partial_der(gr_delta[j], part, eps, j) for j,part in enumerate(gr)])\n",
    "        hes.append(partials)\n",
    "     return np.array(hes)\n",
    "\n",
    "def hessian(f, grad, eps):\n",
    "    return lambda x: hessian_in_point(x, f, grad, eps) \n",
    "\n",
    "def optimization_result(title, fmin, xmin, K, f_ev, j_ev, h_ev = None, res=None):\n",
    "    print(f\"\"\"\n",
    "{title}\n",
    "Optimization {res}\n",
    "x minimum: {xmin},\n",
    "f minimum: {fmin},\n",
    "number of iterations: {K},\n",
    "number of function evaluations: {f_ev},\n",
    "number of gradient evaluations: {j_ev},\n",
    "{f\"number of hessian evaluations: {h_ev}\" if h_ev != None else ''}\n",
    "\"\"\") if res == 'succes' else print(f\"\"\"{title}\\nOptimization {res}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30fef9a",
   "metadata": {},
   "source": [
    "### Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c77ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_sqrt1 = [\n",
    "    danilov,\n",
    "    danilov_gradient,\n",
    "    np.array([-2, 2]),\n",
    "    0.001,\n",
    "    'Square root func 1 test. Starting point (-2, 2)' \n",
    "]\n",
    "\n",
    "test_sqrt2 = [\n",
    "    danilov,\n",
    "    danilov_gradient,\n",
    "    np.array([4, 3]),\n",
    "    0.001,\n",
    "    'Square root func 1 test. Starting point (4, 3)' \n",
    "]\n",
    "\n",
    "test_rosen1 = [\n",
    "    rosenbrok,\n",
    "    rosen_gradient,\n",
    "    np.array([-2, -1]),\n",
    "    1e-4,\n",
    "    'Rosenbrock1 test. Starting point (-2, -1)'\n",
    "]\n",
    "\n",
    "test_rosen2 = [\n",
    "    rosenbrok,\n",
    "    rosen_gradient,\n",
    "    np.array([-3, 4]),\n",
    "    1e-4,\n",
    "    'Rosenbrock2 test. Starting point (-3, 4)'\n",
    "]\n",
    "\n",
    "test_rosen3 = [\n",
    "    rosenbrok,\n",
    "    rosen_gradient,\n",
    "    np.array([3, 3]),\n",
    "    1e-4,\n",
    "    'Rosenbrock3 test. Starting point (3, 3)'\n",
    "]\n",
    "\n",
    "test_himmel1 = [\n",
    "    himmelblau,\n",
    "    himmel_gradient,\n",
    "    np.array([0, -4]),\n",
    "    1e-4,\n",
    "    'Himmelblau1 test. Starting point (0, -4)'\n",
    "]\n",
    "\n",
    "test_himmel2 = [\n",
    "    himmelblau,\n",
    "    himmel_gradient,\n",
    "    np.array([10, 21]),\n",
    "    1e-4,\n",
    "    'Himmelblau1 test. Starting point (10, 21)'\n",
    "]\n",
    "\n",
    "test_himmel3 = [\n",
    "    himmelblau,\n",
    "    himmel_gradient,\n",
    "    np.array([-5, 17]),\n",
    "    1e-4,\n",
    "    'Himmelblau1 test. Starting point (-5, 17)'\n",
    "]\n",
    "\n",
    "# test_rastrigin = [\n",
    "#     rastrigin,\n",
    "#     approx_gradient(rastrigin, np.float64(1e-8)),\n",
    "#     np.array([2, 1]),\n",
    "#     1e-4\n",
    "# ]\n",
    "\n",
    "# test_ackley = [\n",
    "#     ackley,\n",
    "#     approx_gradient(ackley, np.float64(1e-9)),\n",
    "#     np.array([1, 1]),\n",
    "#     1e-4\n",
    "# ]\n",
    "\n",
    "# test_sphere = [\n",
    "#     sphere,\n",
    "#     approx_gradient(sphere, np.float64(1e-9)),\n",
    "#     np.array([-3, 3]),\n",
    "#     1e-5,\n",
    "#     [[-3, 3], [0, 10]]\n",
    "# ]\n",
    "\n",
    "# test_beale = [\n",
    "#     beale,\n",
    "#     approx_gradient(beale, np.float64(1e-9)),\n",
    "#     np.array([3, 1.5]),\n",
    "#     1e-3,\n",
    "#     [[-0.01, 800], [2.9, 1.6]]\n",
    "# ]\n",
    "\n",
    "# test_goldstein = [\n",
    "#     goldstein_price,np.array([2, 1]),\n",
    "#     approx_gradient(goldstein_price, np.float64(1e-9)),\n",
    "#     np.array([-1.3, 1]),\n",
    "#     1e-5,\n",
    "#     [[-1.5, 1], [0, 50000]]\n",
    "# ]\n",
    "\n",
    "# test_booth = [\n",
    "#     booth,\n",
    "#     approx_gradient(booth, np.float64(1e-8)),\n",
    "#     np.array([5, 3]),\n",
    "#     1e-5,\n",
    "#     [[0, 8], [0, 700]]\n",
    "# ]\n",
    "\n",
    "# test_bukin = [\n",
    "#     bukin,\n",
    "#     approx_gradient(bukin, np.float64(1e-8)),\n",
    "#     np.array([-10.5, 1.5]),\n",
    "#     1e-5\n",
    "# ]\n",
    "\n",
    "# test_himmel = [\n",
    "#     himmelblau,\n",
    "#     approx_gradient(himmelblau, np.float64(1e-8)),\n",
    "#     np.array([0, -4]),\n",
    "#     1e-5,\n",
    "#     [[-4, 4], [-0.1, 280]]\n",
    "# ]\n",
    "\n",
    "# test_egg = [\n",
    "#     eggholder,\n",
    "#     approx_gradient(eggholder, np.float64(1e-8)),\n",
    "#     np.array([353, -200]),\n",
    "#     1e-7\n",
    "# ]\n",
    "\n",
    "# test_cross = [\n",
    "#     cross,\n",
    "#     approx_gradient(cross, np.float64(1e-8)),\n",
    "#     np.array([2, -2]),\n",
    "#     1e-4\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f50293-c0c6-45db-8e43-4fd79da71fcc",
   "metadata": {},
   "source": [
    "### Newton method\n",
    "\n",
    "The common Newton method is to find our $H^{-1}$ matrix and than build relaxetion sequence by this rule:\n",
    "$$\n",
    "\\vec{x}_{k+1} = \\vec{x}_{k}  - H^{-1}(\\vec{x}_{k}) \\nabla f(\\vec{x}_{k})\n",
    "$$\n",
    "\n",
    "But there is a problem with matrix $H$, it needs to be always positive determinated or $H^{-1}$ won't exist.\n",
    "\n",
    "To solve this problem, let's check if $H$ is positive determinated, if not, then let's pick $\\eta$_k, such that:\n",
    "$$\n",
    "\\widetilde{H}_k = \\eta_kI_n + H(\\vec{x}_{k-1})\n",
    "$$\n",
    "$\\widetilde{H}$ is positive determinated matrix, that we pick instead of $H$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be4fcc8-0942-462f-90c1-251d7490210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_newton(f, gr, x, epsilon, title, hess):\n",
    "    try:\n",
    "        anim = Animate3D(f, x, title)\n",
    "        w = -gr(x)\n",
    "        k = 0\n",
    "        f_ev, j_ev, h_ev = 0, 0, 0\n",
    "        while(norm(w) > epsilon):\n",
    "            H = hess(x)\n",
    "            j_ev += 1\n",
    "            h_ev += 1\n",
    "    #         print(x, w)\n",
    "            h = linalg.solve(H, w)\n",
    "            x = x + h\n",
    "            anim.add(x)\n",
    "            w = -gr(x)\n",
    "            k += 1\n",
    "            if k == 100:\n",
    "                return f(x), x, k, f_ev, j_ev, h_ev, anim, \"fail\"\n",
    "        return f(x), x, k, f_ev, j_ev, h_ev, anim, \"succes\"\n",
    "    except: \n",
    "        return f(x), x, k, f_ev, j_ev, h_ev, anim, \"fail\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44050517",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square root func 1 test. Starting point (-2, 2)\n",
      "Optimization fail\n",
      "Square root func 1 test. Starting point (4, 3)\n",
      "Optimization fail\n",
      "\n",
      "Rosenbrock1 test. Starting point (-2, -1)\n",
      "Optimization succes\n",
      "x minimum: [1. 1.],\n",
      "f minimum: 4.378375239202921e-27,\n",
      "number of iterations: 5,\n",
      "number of function evaluations: 0,\n",
      "number of gradient evaluations: 5,\n",
      "number of hessian evaluations: 5\n",
      "\n",
      "\n",
      "Rosenbrock2 test. Starting point (-3, 4)\n",
      "Optimization succes\n",
      "x minimum: [1. 1.],\n",
      "f minimum: 1.6568659570877452e-24,\n",
      "number of iterations: 5,\n",
      "number of function evaluations: 0,\n",
      "number of gradient evaluations: 5,\n",
      "number of hessian evaluations: 5\n",
      "\n",
      "\n",
      "Rosenbrock3 test. Starting point (3, 3)\n",
      "Optimization succes\n",
      "x minimum: [1. 1.],\n",
      "f minimum: 2.923222691909612e-28,\n",
      "number of iterations: 5,\n",
      "number of function evaluations: 0,\n",
      "number of gradient evaluations: 5,\n",
      "number of hessian evaluations: 5\n",
      "\n",
      "\n",
      "Himmelblau1 test. Starting point (0, -4)\n",
      "Optimization succes\n",
      "x minimum: [-0.12796134 -1.953715  ],\n",
      "f minimum: 178.33723920192747,\n",
      "number of iterations: 6,\n",
      "number of function evaluations: 0,\n",
      "number of gradient evaluations: 6,\n",
      "\n",
      "\n",
      "\n",
      "Himmelblau1 test. Starting point (10, 21)\n",
      "Optimization succes\n",
      "x minimum: [3. 2.],\n",
      "f minimum: 3.515878009693502e-17,\n",
      "number of iterations: 10,\n",
      "number of function evaluations: 0,\n",
      "number of gradient evaluations: 10,\n",
      "\n",
      "\n",
      "\n",
      "Himmelblau1 test. Starting point (-5, 17)\n",
      "Optimization succes\n",
      "x minimum: [-2.80511808  3.13131257],\n",
      "f minimum: 1.0506401369694166e-13,\n",
      "number of iterations: 8,\n",
      "number of function evaluations: 0,\n",
      "number of gradient evaluations: 8,\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/urban/source/repos/dzshki/optmehods/lab1/test_functions.py:29: RuntimeWarning: overflow encountered in power\n",
      "  danilov_hessian = lambda x: np.array([[np.divide(4, np.sqrt(x[0]**2 + x[1]**2 + 1)) - np.divide(4*x[0]**2, np.power(x[0]**2 + x[1]**2 + 1, 1.5)), -np.divide(4*x[0]**2, np.power(x[0]**2 + x[1]**2 + 1, 1.5))],\n",
      "/home/urban/source/repos/dzshki/optmehods/lab1/test_functions.py:30: RuntimeWarning: overflow encountered in power\n",
      "  [-np.divide(4*x[0]*x[1], np.power(x[0]**2 + x[1]**2 + 1, 1.5)), np.divide(4, np.sqrt(x[0]**2 + x[1]**2 + 1)) - np.divide(4*x[1]**2, np.power(x[0]**2 + x[1]**2 + 1, 1.5))]])\n"
     ]
    }
   ],
   "source": [
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = common_newton(*test_sqrt1, hess=danilov_hessian)\n",
    "optimization_result(test_sqrt1[4], fmin, xmin, K, f_ev, j_ev, h_ev, res)\n",
    "# a = anim.get_animation(duration=3000).save('examples/Sqrt/Sqrt1-Newton.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = common_newton(*test_sqrt2, hess=danilov_hessian)\n",
    "optimization_result(test_sqrt2[4], fmin, xmin, K, f_ev, j_ev, h_ev, res)\n",
    "# a = anim.get_animation(duration=3000).save('examples/Sqrt/Sqrt2-Newton.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = common_newton(*test_rosen1, hess=rosen_hessian)\n",
    "optimization_result(test_rosen1[4], fmin, xmin, K, f_ev, j_ev, h_ev, res)\n",
    "# a = anim.get_animation(duration=6000).save('examples/Rosenbrock/Rosenbrock1-Newton.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = common_newton(*test_rosen2, hess=rosen_hessian)\n",
    "optimization_result(test_rosen2[4], fmin, xmin, K, f_ev, j_ev, h_ev, res)\n",
    "# a = anim.get_animation(duration=6000).save('examples/Rosenbrock/Rosenbrock2-Newton.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = common_newton(*test_rosen3, hess=rosen_hessian)\n",
    "optimization_result(test_rosen3[4], fmin, xmin, K, f_ev, j_ev, h_ev, res)\n",
    "# a = anim.get_animation(duration=6000).save('examples/Rosenbrock/Rosenbrock3-Newton.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = common_newton(*test_himmel1, hess=himmel_hessian)\n",
    "optimization_result(test_himmel1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Himmelblau/Himmel1-Fastest-Newton.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = common_newton(*test_himmel2, hess=himmel_hessian)\n",
    "optimization_result(test_himmel2[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Himmelblau/Himmel2-Fastest-Newton.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = common_newton(*test_himmel3, hess=himmel_hessian)\n",
    "optimization_result(test_himmel3[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Himmelblau/Himmel3-Fastest-Newton.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e07ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_upgraded(f, gr, x, epsilon, title, hess, onedim_opt):\n",
    "    try:\n",
    "        anim = Animate3D(f, x, title)\n",
    "        w = -gr(x)\n",
    "        phi = toOneParamFunc(f, x, w)\n",
    "        k = 0\n",
    "        f_ev, j_ev, h_ev = 0, 0, 0\n",
    "    #     print(x)\n",
    "        while(norm(w) > epsilon):\n",
    "            H = hess(x)\n",
    "            h_ev += 1\n",
    "            j_ev += 1\n",
    "            h = linalg.solve(H, w)\n",
    "            phi = toOneParamFunc(f, x, h)\n",
    "            l, i = argmin(phi, 0, 600, np.divide(epsilon, 1e5), onedim_opt) \n",
    "            f_ev += i\n",
    "            x = x + l * h\n",
    "    #         print(x)\n",
    "            anim.add(x)\n",
    "            w = -gr(x)\n",
    "            k += 1\n",
    "            if k == 100:\n",
    "                return f(x), x, k, f_ev, j_ev, h_ev, anim, 'fail'\n",
    "        return f(x), x, k, f_ev, j_ev, h_ev, anim, 'succes'\n",
    "    except:\n",
    "        return f(x), x, k, f_ev, j_ev, h_ev, anim, 'fail'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea0fb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Square root func 1 test. Starting point (-2, 2)\n",
      "Optimization succes\n",
      "x minimum: [-0.30151656 -0.60299587],\n",
      "f minimum: 3.3166247913499816,\n",
      "number of iterations: 4,\n",
      "number of function evaluations: 70,\n",
      "number of gradient evaluations: 4,\n",
      "number of hessian evaluations: 4\n",
      "\n",
      "\n",
      "Square root func 1 test. Starting point (4, 3)\n",
      "Optimization succes\n",
      "x minimum: [-0.30146196 -0.60297794],\n",
      "f minimum: 3.316624795720703,\n",
      "number of iterations: 4,\n",
      "number of function evaluations: 88,\n",
      "number of gradient evaluations: 4,\n",
      "number of hessian evaluations: 4\n",
      "\n",
      "\n",
      "Rosenbrock1 test. Starting point (-2, -1)\n",
      "Optimization succes\n",
      "x minimum: [0.99999977 0.9999995 ],\n",
      "f minimum: 1.9783972876656746e-13,\n",
      "number of iterations: 14,\n",
      "number of function evaluations: 206,\n",
      "number of gradient evaluations: 14,\n",
      "number of hessian evaluations: 14\n",
      "\n",
      "\n",
      "Rosenbrock2 test. Starting point (-3, 4)\n",
      "Optimization succes\n",
      "x minimum: [0.9999996  0.99999923],\n",
      "f minimum: 3.2421921095134094e-13,\n",
      "number of iterations: 17,\n",
      "number of function evaluations: 261,\n",
      "number of gradient evaluations: 17,\n",
      "number of hessian evaluations: 17\n",
      "\n",
      "\n",
      "Rosenbrock3 test. Starting point (3, 3)\n",
      "Optimization succes\n",
      "x minimum: [1.00000057 1.0000012 ],\n",
      "f minimum: 6.198593774785128e-13,\n",
      "number of iterations: 12,\n",
      "number of function evaluations: 197,\n",
      "number of gradient evaluations: 12,\n",
      "number of hessian evaluations: 12\n",
      "\n",
      "\n",
      "Himmelblau1 test. Starting point (0, -4)\n",
      "Optimization succes\n",
      "x minimum: [ 3.58442834 -1.84812653],\n",
      "f minimum: 9.917117355672472e-18,\n",
      "number of iterations: 5,\n",
      "number of function evaluations: 71,\n",
      "number of gradient evaluations: 5,\n",
      "\n",
      "\n",
      "\n",
      "Himmelblau1 test. Starting point (10, 21)\n",
      "Optimization succes\n",
      "x minimum: [3.00000001 2.        ],\n",
      "f minimum: 2.0681373109462736e-15,\n",
      "number of iterations: 5,\n",
      "number of function evaluations: 71,\n",
      "number of gradient evaluations: 5,\n",
      "\n",
      "\n",
      "\n",
      "Himmelblau1 test. Starting point (-5, 17)\n",
      "Optimization succes\n",
      "x minimum: [-2.80511809  3.13131252],\n",
      "f minimum: 7.269365421624e-22,\n",
      "number of iterations: 4,\n",
      "number of function evaluations: 53,\n",
      "number of gradient evaluations: 4,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = newton_upgraded(*test_sqrt1, hess=danilov_hessian, onedim_opt=brent_optimize)\n",
    "optimization_result(test_sqrt1[4], fmin, xmin, K, f_ev, j_ev, h_ev, res)\n",
    "# a = anim.get_animation(duration=5000).save('examples/Sqrt/Sqrt1-Upgr-Newton.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = newton_upgraded(*test_sqrt2, hess=danilov_hessian, onedim_opt=brent_optimize)\n",
    "optimization_result(test_sqrt2[4], fmin, xmin, K, f_ev, j_ev, h_ev, res)\n",
    "# a = anim.get_animation(duration=5000).save('examples/Sqrt/Sqrt2-Upgr-Newton.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = newton_upgraded(*test_rosen1, hess=rosen_hessian, onedim_opt=brent_optimize)\n",
    "optimization_result(test_rosen1[4], fmin, xmin, K, f_ev, j_ev, h_ev, res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Rosenbrock/Rosenbrock1-Upgr-Newton.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = newton_upgraded(*test_rosen2, hess=rosen_hessian, onedim_opt=brent_optimize)\n",
    "optimization_result(test_rosen2[4], fmin, xmin, K, f_ev, j_ev, h_ev, res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Rosenbrock/Rosenbrock2-Upgr-Newton.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = newton_upgraded(*test_rosen3, hess=rosen_hessian, onedim_opt=brent_optimize)\n",
    "optimization_result(test_rosen3[4], fmin, xmin, K, f_ev, j_ev, h_ev, res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Rosenbrock/Rosenbrock3-Upgr-Newton.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = newton_upgraded(*test_himmel1, hess=himmel_hessian, onedim_opt=brent_optimize)\n",
    "optimization_result(test_himmel1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Himmelblau/Himmel1-Upgr-Newton.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = newton_upgraded(*test_himmel2, hess=himmel_hessian, onedim_opt=brent_optimize)\n",
    "optimization_result(test_himmel2[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Himmelblau/Himmel2-Upgr-Newton.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, h_ev, anim, res = newton_upgraded(*test_himmel3, hess=himmel_hessian, onedim_opt=brent_optimize)\n",
    "optimization_result(test_himmel3[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Himmelblau/Himmel3-Upgr-Newton.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e446c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_next_matrix(A, dw, dx):\n",
    "    y = dx\n",
    "    z = A.dot(dw)\n",
    "    first_part = np.dot(np.transpose([y]), [dx]) * np.divide(1, dw.dot(y))\n",
    "    sec_part =  np.dot(np.transpose([z]), [dw]).dot(A) * np.divide(1, z.dot(dw))\n",
    "    return A - first_part - sec_part\n",
    "\n",
    "def count_next_matrix_2(A, dw,dx):\n",
    "    I = np.identity(len(dx))\n",
    "    r = np.divide(1, np.dot(dx, dw))\n",
    "    C1 = (I - r*(np.dot(np.transpose([dw]), [dx])))\n",
    "    C2 = (I - r*(np.dot(np.transpose([dx]), [dw])))\n",
    "    first_part = C1.dot(A).dot(C2)\n",
    "    sec_part =  r*np.dot(np.transpose([dx]), [dx])\n",
    "    return first_part - sec_part\n",
    "\n",
    "def DFP(f, grad, x, epsilon, title, onedim_opt):\n",
    "    try:\n",
    "        w2 = -grad(x)\n",
    "        p = w2\n",
    "        f_ev = 0\n",
    "        j_ev = 1\n",
    "        anim = Animate3D(f, x, title)\n",
    "        phi = toOneParamFunc(f, x, w2)\n",
    "        x1 = x\n",
    "        k = 1\n",
    "#         n = 0\n",
    "        A = np.identity(len(x))\n",
    "        l, i = argmin(phi, 0, 600, np.divide(epsilon, 1e4), onedim_opt)\n",
    "        f_ev += i\n",
    "    #     print(x1, f(x1), l, w2)\n",
    "        x2 = x1 + l * p\n",
    "        anim.add(x2)\n",
    "        while(norm(w2) > epsilon):\n",
    "            w1 = w2\n",
    "            w2 = -grad(x2)\n",
    "            A = count_next_matrix(A, w2 - w1, x2 - x1)\n",
    "            p = np.dot(A, w2)\n",
    "            x1 = x2\n",
    "            phi = toOneParamFunc(f, x2, p)\n",
    "            l, i = argmin(phi, 0, 600, np.divide(epsilon, 1e4), onedim_opt)\n",
    "    #         print(x1, f(x1), l, w2)\n",
    "            x2 = x1 + l * p\n",
    "            anim.add(x2)\n",
    "            f_ev += i\n",
    "            j_ev += 1\n",
    "            k += 1\n",
    "    #         print(\"A\", A, \"\\ninverted Hessian\", np.linalg.inv(rosen_hessian(x2)))\n",
    "        return f(x2), x2, k, f_ev, j_ev, anim, 'succes'\n",
    "    except:\n",
    "        return f(x2), x2, k, f_ev, j_ev, anim, 'fail'\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "465eb635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Square root func 1 test. Starting point (-2, 2)\n",
      "Optimization succes\n",
      "x minimum: [-0.30151057 -0.6030228 ],\n",
      "f minimum: 3.3166247903563697,\n",
      "number of iterations: 5,\n",
      "number of function evaluations: 76,\n",
      "number of gradient evaluations: 5,\n",
      "\n",
      "\n",
      "\n",
      "Square root func 1 test. Starting point (4, 3)\n",
      "Optimization succes\n",
      "x minimum: [-0.30151134 -0.60302269],\n",
      "f minimum: 3.3166247903553994,\n",
      "number of iterations: 6,\n",
      "number of function evaluations: 102,\n",
      "number of gradient evaluations: 6,\n",
      "\n",
      "\n",
      "\n",
      "Rosenbrock1 test. Starting point (-2, -1)\n",
      "Optimization succes\n",
      "x minimum: [1. 1.],\n",
      "f minimum: 7.456949295253838e-26,\n",
      "number of iterations: 17,\n",
      "number of function evaluations: 317,\n",
      "number of gradient evaluations: 17,\n",
      "\n",
      "\n",
      "\n",
      "Rosenbrock2 test. Starting point (-3, 4)\n",
      "Optimization succes\n",
      "x minimum: [1. 1.],\n",
      "f minimum: 9.970462284894945e-29,\n",
      "number of iterations: 19,\n",
      "number of function evaluations: 354,\n",
      "number of gradient evaluations: 19,\n",
      "\n",
      "\n",
      "\n",
      "Rosenbrock3 test. Starting point (3, 3)\n",
      "Optimization succes\n",
      "x minimum: [1. 1.],\n",
      "f minimum: 2.6183455549920787e-21,\n",
      "number of iterations: 25,\n",
      "number of function evaluations: 484,\n",
      "number of gradient evaluations: 25,\n",
      "\n",
      "\n",
      "\n",
      "Himmelblau1 test. Starting point (0, -4)\n",
      "Optimization succes\n",
      "x minimum: [-2.80511809  3.13131252],\n",
      "f minimum: 1.135959703518257e-27,\n",
      "number of iterations: 6,\n",
      "number of function evaluations: 88,\n",
      "number of gradient evaluations: 6,\n",
      "\n",
      "\n",
      "\n",
      "Himmelblau1 test. Starting point (0, -4)\n",
      "Optimization succes\n",
      "x minimum: [-3.77931025 -3.28318599],\n",
      "f minimum: 3.1869980570928877e-28,\n",
      "number of iterations: 7,\n",
      "number of function evaluations: 112,\n",
      "number of gradient evaluations: 7,\n",
      "\n",
      "\n",
      "\n",
      "Himmelblau1 test. Starting point (0, -4)\n",
      "Optimization succes\n",
      "x minimum: [-3.77931025 -3.28318599],\n",
      "f minimum: 6.497594209727079e-23,\n",
      "number of iterations: 6,\n",
      "number of function evaluations: 99,\n",
      "number of gradient evaluations: 6,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fmin, xmin, K, f_ev, j_ev, anim, res = DFP(*test_sqrt1, onedim_opt=brent_optimize)\n",
    "optimization_result(test_sqrt1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=5000).save('examples/Sqrt/Sqrt1-DFP.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = DFP(*test_sqrt2, onedim_opt=brent_optimize)\n",
    "optimization_result(test_sqrt2[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=5000).save('examples/Sqrt/Sqrt2-DFP.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = DFP(*test_rosen1, onedim_opt=brent_optimize)\n",
    "optimization_result(test_rosen1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Rosenbrock/Rosenbrock1-DFP.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = DFP(*test_rosen2, onedim_opt=brent_optimize)\n",
    "optimization_result(test_rosen2[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=10000).save('examples/Rosenbrock/Rosenbrock2-DFP.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = DFP(*test_rosen3, onedim_opt=brent_optimize)\n",
    "optimization_result(test_rosen3[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Rosenbrock/Rosenbrock3-DFP.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = DFP(*test_himmel1, onedim_opt=brent_optimize)\n",
    "optimization_result(test_himmel1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Himmelblau/Himmel1-DFP.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = DFP(*test_himmel2, onedim_opt=brent_optimize)\n",
    "optimization_result(test_himmel1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Himmelblau/Himmel2-DFP.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = DFP(*test_himmel3, onedim_opt=brent_optimize)\n",
    "optimization_result(test_himmel1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Himmelblau/Himmel3-DFP.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51638824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
