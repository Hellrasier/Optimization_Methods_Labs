{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0268044-01de-4c3d-8990-6b05bc30cdae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gradient methods\n",
    "\n",
    "#### Proplem:\n",
    "$$\n",
    "f(\\vec{x}) \\rightarrow min,\\\\\n",
    "f: \\Omega \\rightarrow \\mathbb{R}, \\\\\n",
    "\\Omega \\subset \\mathbb{R^n}, f(\\vec{x}) \\mbox{ is convex}, \\\\\n",
    "f(\\vec{x}) \\mbox{ - is diffirentiable on } \\Omega\\\\\n",
    "\\vec{x_*} \\in \\Omega, f_{min} = f(\\vec{x_*})\n",
    "$$\n",
    "\n",
    "<em>**Definition**</em>.\n",
    "\n",
    "Sequnce $\\{\\vec{x_k}\\}$ is named **Relaxational**, if $\\forall k \\in \\mathbb{N}:  f(\\vec{x_k}) < f(\\vec{x}_{k-1})$ \n",
    "\n",
    "$\\{\\vec{x}_l\\}$ convergece to $\\vec{x}_* \\in \\mathbb{R}^n$ by Bolzanoâ€“Weierstrass theorem \n",
    "\n",
    "Let's choose our relaxational sequence by this equation:\n",
    "$$\n",
    "\\vec{x}_k = \\vec{x}_{k-1} + \\beta_k\\vec{u}_k\n",
    "$$\n",
    "where $\\vec{u}_{k}$ is unit vector, which defines the direction of descent and $\\beta_k \\geq 0$ - length of descent step\n",
    "\n",
    "<em>**Lemma**</em>.\n",
    "\n",
    "$f(\\vec{x})$ - is differentiable on $\\Omega \\subset \\mathbb{R}^n$ and $\\exists L > 0$, such that $\\forall \\vec{x}, \\vec{y} \\in \\Omega$:\n",
    "$$\n",
    "||\\nabla f(\\vec{x}) - \\nabla f(\\vec{y})|| \\leq  L ||\\vec{x} = \\vec{y}|| \n",
    "$$\n",
    "Then:\n",
    "$$\n",
    "f(\\vec{x}) - f(\\vec{y}) \\geq (\\nabla f(\\vec{x}), \\vec{x} - \\vec{y}) - \\frac{L}{2}||\\vec{x}-\\vec{y}||^2\n",
    "$$\n",
    "<em>**Definition**</em>.\n",
    "\n",
    "$\\vec{w}(\\vec{x}) = - \\nabla f(\\vec{x})$ is called **antigradient**\n",
    "\n",
    "If we take our $\\vec{u}_k = \\frac{\\vec{w}_k}{||\\vec{w}_k||}$, from our lemma we have, that: \n",
    "\n",
    "$$\n",
    "f(x_{k}) - f(x_{k+1}) \\geq (\\nabla f(x_k), \\vec{x_k} - \\vec{x_k} - \\beta_k \\frac{\\vec{w_k}}{||\\vec{w_k}||}) - \\frac{L}{2} || \\vec{x_k} - \\vec{x_k} - \\beta_k \\frac{\\vec{w_k}}{||\\vec{w_k}||} ||^2 = \\beta_k||\\nabla f(\\vec{x}_k)|| - \\beta_k \\frac{L}{2} \n",
    "$$\n",
    "As we can see gradient must be always posistive (and $> \\frac{L}{2}$),  so that we have a convergece, we get this when function is convex\n",
    "\n",
    "All methods in which $\\vec{u}_k = \\frac{\\vec{w}_k}{||\\vec{w}_k||}$, are named ***gradient methods***, the methods vary on the way we choose our $\\beta_k > 0$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68777a30-a1ca-4e9d-bf2c-ef35d7345a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mplib\n",
    "import math as m\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from functools import reduce\n",
    "\n",
    "from onedim_optimize import quadratic_approx, newton_method, fibbonaci_method\n",
    "from scipy.optimize import approx_fprime\n",
    "\n",
    "from scipy.misc import derivative\n",
    "\n",
    "def der(f, x, n):\n",
    "    return derivative(f, x, dx=1e-6, n=n)\n",
    "\n",
    "\n",
    "def toOneParamFunc(f, x, w):\n",
    "    return lambda p: f(x + p*w) \n",
    "\n",
    "def argmin(f, a, b, eps):\n",
    "    xmin, k = fibbonaci_method(f, a, b, eps)\n",
    "    return xmin, k\n",
    "\n",
    "def approx_gradient(f, eps):\n",
    "    return lambda x: approx_fprime(x, f, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c44b76-b160-47a4-a808-7d3a82ae1dd9",
   "metadata": {},
   "source": [
    "### Test functions\n",
    "\n",
    "#### Rosenbrock banana function:\n",
    "$$\n",
    "f(x_1, x_2, ..., x_N) = \\sum^{N/2}_{i=1}[100(x^2_{2i-1} - x_{2i})^2 + (x_{2i-1} - 1)^2]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a2fc117-d476-4c87-a60f-37dd21fec44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(vec):\n",
    "    build_ros_terms = lambda i, x: 100*(x**2 - vec[i+1])**2 + (x - 1)**2 if i % 2 == 0 else 0 \n",
    "    ros_terms = map(build_ros_terms, enumerate(vec))\n",
    "    sum = lambda a, b: a + b\n",
    "    return reduce(sum, ros_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da7e1e-bc40-4eca-96a5-e9add234d8a0",
   "metadata": {},
   "source": [
    "### Fastest descent method\n",
    "\n",
    "We will construct relaxational sequence, using this rule:\n",
    "$$\n",
    "\\vec{x}_{k+1} = \\vec{x}_k + \\lambda_k\\vec{w}_K\n",
    "$$\n",
    "\n",
    "Where $\\lambda_k$ is found from\n",
    "$$\n",
    "\\lambda_k = argmin\\{\\psi_k(\\lambda)\\} \\\\\n",
    "\\psi_k(\\lambda) = f(\\vec{x}_{k-1} + \\lambda\\vec{w}_k)\n",
    "$$\n",
    "\n",
    "Finding minimum of $\\psi_k(\\lambda)$ is a pretty complex task of one-dimension minimization. But it is guaranteed that $\\{|\\vec{w}_k|\\}$ convergace to 0.\n",
    "\n",
    "So at start we pick some small $\\epsilon$ and continuing procedure while $|\\vec{w}_k\\| > \\epsilon$, than on some N iteration we pick our $x_* = x_N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19911d95-3937-4870-a8ac-ef815ece89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastest_descent(f, gr, x, epsilon):\n",
    "    n = len(x)\n",
    "    w = -gr(x) \n",
    "    phi = toOneParamFunc(f, x, w)\n",
    "    l, i = argmin(phi, 0, 1, epsilon)\n",
    "    n += i\n",
    "    k = 1\n",
    "    print(x, f(x), l, norm(w))\n",
    "    x = x + l*w\n",
    "    while(norm(w) > epsilon):\n",
    "        w = -gr(x) \n",
    "        phi = toOneParamFunc(f, x, w)\n",
    "        l, i = argmin(phi, 0, 1, epsilon)\n",
    "        n += i\n",
    "        k += 1\n",
    "        print(x, f(x), l, norm(w))\n",
    "        x = x + l*w\n",
    "    return f(x), x, k, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cdbf93e-3bf1-4f62-a866-b2809c946690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50 41] 390.67353942759587 0.8611055555555555 6.109475309776838\n",
      "[46.47575064 37.09400868] 358.5531835649015 0.8611055555555555 6.101374413275004\n",
      "[42.92293512 33.22344605] 326.5214139811794 0.8611055555555555 6.091919866988924\n",
      "[39.33848929 29.39329994] 294.5932166774629 0.8611055555555555 6.080754565866896\n",
      "[35.71870287 25.60983081] 262.7878077458083 0.8611055555555555 6.06730697439827\n",
      "[32.05906284 21.88110532] 231.13083688529028 0.8611055555555555 6.050751828636447\n",
      "[28.35396219 18.21779663] 199.6575078183351 0.8611055555555555 6.029801821920918\n",
      "[24.59630344 14.63451087] 168.41816371048645 0.8611055555555555 6.002299784785708\n",
      "[20.77691124 11.1521538 ] 137.48889438681735 0.881936111111111 5.964386744828725\n",
      "[16.78947904  7.7213852 ] 106.25996272482165 0.881936111111111 5.906467775345529\n",
      "[12.70717279  4.48568094] 75.72941052317098 0.8611055555555555 5.8092011935232675\n",
      "[8.60698079 1.62005967] 47.10721056038919 0.8611055555555555 5.616566236099833\n",
      "[ 4.38274676 -0.73518071] 21.1327958142819 0.881936111111111 5.034263993057506\n",
      "[ 0.10654924 -1.92968466] 4.951234725058791 0.6249958333333333 1.9554763401741653\n",
      "[-0.64085914 -0.96269606] 3.5492819341454007 0.5555499999999999 0.8525993652124118\n",
      "[-0.26493376 -0.67453743] 3.325935738815587 0.33332777777777767 0.23296738325179464\n",
      "[-0.31223475 -0.61295141] 3.316880403243856 0.40277499999999994 0.03522812668708247\n",
      "[-0.30055906 -0.60488875] 3.316631264092986 0.3194388888888888 0.006265165638820266\n",
      "\n",
      "x minimum: [-0.30175179 -0.60328166],\n",
      "f minimum: 3.316624937808101,\n",
      "number of iterations: 18\n",
      "number of one-dimension minimization iterations: 200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1 = lambda x: 6*x[0]**2 - 4*x[0]*x[1] + 3*x[1]**2 + 4*m.sqrt(5)*(x[0] + 2*x[1]) + 22\n",
    "f2 = lambda x: (x[0]**2 - x[1])**2 + (x[0] - 1)**2\n",
    "danilov = lambda x: x[0] + 2*x[1] + 4*m.sqrt(1 + x[0]**2 + x[1]**2)\n",
    "\n",
    "# rosenbrock_test = [\n",
    "#     rosenbrock,\n",
    "#     approx_gradient(rosenbrock, 1e-8),\n",
    "#     np.array([-2, 1]),\n",
    "#     0.01,\n",
    "# ]\n",
    "\n",
    "test1 = [\n",
    "    f1,\n",
    "    approx_gradient(f1, 1e-8),\n",
    "    np.array([-2, 1]),\n",
    "    0.01,\n",
    "]\n",
    "test2 = [\n",
    "    f2,\n",
    "    approx_gradient(f2, 1e-8),\n",
    "    np.array([-1, -2]),\n",
    "    0.001,\n",
    "]\n",
    "\n",
    "test_danilov = [\n",
    "    danilov,\n",
    "    approx_gradient(danilov, 1e-8),\n",
    "    np.array([50, 41]),\n",
    "    0.01,\n",
    "]\n",
    "\n",
    "fmin, xmin, K, N = fastest_descent(*test_danilov)\n",
    "print(f\"\"\"\n",
    "x minimum: {xmin},\n",
    "f minimum: {fmin},\n",
    "number of iterations: {K}\n",
    "number of one-dimension minimization iterations: {N}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2c72e-20bd-4ee9-a5e4-520d68c55139",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conjugate gradient method\n",
    "\n",
    "#### Problem \n",
    "\n",
    "$$\n",
    "f(\\vec{x}) = \\frac{1}{2}(Q\\vec{x}, \\vec{x}) + (\\vec{c}, \\vec{x}) \\rightarrow min\n",
    "$$\n",
    "\n",
    "$Q$ is positive determined n-dimsensional matrix, $c \\in \\mathbb{R}$ - constant\n",
    "\n",
    "This function has single point of minimum $x_* = -Q^{-1}\\vec{c}$\n",
    "\n",
    "To find the inverted matrix $Q^{-1}$ we can use\n",
    "$$\n",
    "Q^{-1} = \\sum^n_{i=1}\\frac{p^i(p^i)^T}{(Qp^i, p^i)}\n",
    "$$\n",
    "Where $p^i \\in \\mathbb{R}$ is conjugate vector of matrix $Q$\n",
    "\n",
    "But constructing a system of conjugate vectors is a pretty complex problem.\n",
    "\n",
    "So we do another way, let's construct system of conjugate vectors on every iteration\n",
    "\n",
    "$\\vec{x}_0$ is a starting point, antrigradient in this point is $\\vec{w}_1 = -Qx_0 - c$ and let's choose $\\vec{p}_1 = \\vec{w}$\n",
    "\n",
    "Using $\\vec{x}_k = \\vec{x}_{k-1} + \\lambda_k\\vec{w}_k$\n",
    "\n",
    "We can find that \n",
    "$$\\lambda_1 = \\frac{|\\vec{w}_1|^2}{(Q\\vec{w}_1, \\vec{w}_1)} = \\frac{|\\vec{p}_1|^2}{(Q\\vec{p}_1, \\vec{p}_1)}$$\n",
    "(from minimization of quadratic function)\n",
    "\n",
    "And so $x_1 = x_0 + \\lambda_1\\vec{p}_1$\n",
    "\n",
    "On second iteration (k = 2) we evaluate antigradient $\\vec{w}_2 = -Q\\vec{x_1} - c$\n",
    "\n",
    "Let's assume, that\n",
    "$$\\vec{p}_2 = \\gamma_1\\vec{p}_1 + \\vec{w}_2$$\n",
    "\n",
    "If we product scalarly this equation on $Q\\vec{p}_1 \\not = 0$ and demand that $\\vec{p}_1, \\vec{p}_2$ are conjugate (ortogonal) over the matrix $Q$ ($(Q\\vec{p}_1, \\vec{p_2}) = 0$), we can find $\\gamma_1$\n",
    "$$\\gamma_1 = -\\frac{(Q\\vec{p}_1, \\vec{w}_2)}{(Q\\vec{p}_1, \\vec{p}_1)}$$\n",
    "\n",
    "Contniuing constructing this system of conjugate vectors, we can say, that on every k iteration we have system of equations:\n",
    "$$\n",
    "\\begin{cases}\n",
    "    p_{k+1} = \\gamma\\vec{p_k} + \\vec{w}_{k+1} \\\\\n",
    "    \\gamma_k = - \\frac{(Q\\vec{p}_k, \\vec{w}_{k+1})}{(Q\\vec{p}_k, \\vec{p}_k)} \\\\\n",
    "    \\vec{w}_{k+1} = \\vec{w}_k = \\lambda_kQ\\vec{p}_k \\\\\n",
    "    (Q\\vec{p}_{k+1}, \\vec{p}_i) = 0 \\\\\n",
    "    (\\vec{w}_{k+1}, \\vec{w}_i) = 0, i = \\overline{1, k} \\\\\n",
    "\\end{cases} \\\\\n",
    "\\mbox{also } \\\\\n",
    "\\lambda_k = \\frac{(\\vec{w}_k, \\vec{p}_k)}{(Q\\vec{p}_k, \\vec{p}_k)},\\\\\n",
    "\\vec{x}_k = \\vec{x_1} + \\lambda_k\\vec{p}_k\n",
    "$$\n",
    "\n",
    "With n steps we can find all $\\vec{p}_k$ conjugate vectors and evaluate our minimum $x_* = -Q^{-1}\\vec{c}$\n",
    "\n",
    "To use this method in our problems (non-quadratic function optimization, we need to remove matrix $Q$ from system of equations\n",
    "\n",
    "We can do this, by if on every iteration by doing minimization process:\n",
    "$$\n",
    "\\psi_k(\\lambda) = f(x_{k-1} + \\lambda)\n",
    "$$\n",
    "\n",
    "In fundament of constructing conjuguate directions $\\vec{p}_{k+1} = \\gamma_k\\vec{p}_k + \\vec{w}_{k+1}$ we assume, that $(\\vec{w}_{k+1}, \\vec{w}_i) = 0$\n",
    "\n",
    "Using this we can show that:\n",
    "$$\n",
    "\\begin{cases}\n",
    "    (Q\\vec{p}_k, \\vec{w}_{k+1}) = - \\frac{1}{\\lambda_k}|\\vec{w}_{k+1}|^2 \\\\\n",
    "    (Q\\vec{p}_k, \\vec{p}_{k}) = \\frac{1}{\\lambda_k}(\\vec{w}_k, \\vec{p}_k)\n",
    "\\end{cases} \\\\\n",
    "\\mbox{so from our system of equations we can evaluate $\\gamma$ using one of theese formulas: } \\\\\n",
    "\\gamma_k = \\frac{|\\vec{w}_{k+1}|^2}{|\\vec{w}_k|^2} \\\\\n",
    "\\gamma_k = \\frac{(\\vec{w}_{k+1} - \\vec{w}_k, \\vec{w}_{k+1})}{|\\vec{w}_k|^2} \\\\\n",
    "\\mbox{also if function twice differentiable, we can use Hessian instead of matrix Q:} \\\\\n",
    "\\gamma_k = - \\frac{(H(\\vec{x}_k)\\vec{p}_k, \\vec{w}_{k+1})}{(H(\\vec{x}_k)\\vec{p}_k, \\vec{p}_k)} \\\\\n",
    "$$\n",
    "\n",
    "This method is called ***conjaguate gradients method***\n",
    "\n",
    "Also as every $\\gamma_k$ is different and we need to minimize $\\psi_k(\\lambda)$ this turns us to inevitably errors, to minimize errors, we need to do **restarts** (set $\\gamma_k = 0$). It is common to restart every $n$ times, where $n$ is our dimension number. Also, with non-quadratic functions our procedure of optimization in general don't take $n$ steps, so we choose our $\\epsilon$ and iterate through $\\{\\vec{x}_k\\}$ till our |$\\vec{w}_{k+1|} < \\epsilon$, and then $x_{k-1} \\approx x_*$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "872bd742-b7b8-477b-8406-82ab2cb9ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(f, gr, x, epsilon):\n",
    "    w = -gr(x) \n",
    "    p = w\n",
    "    phi = toOneParamFunc(f, x, p)\n",
    "    l, i = argmin(phi, 0, 1, epsilon)\n",
    "    n = i\n",
    "    print(x, f(x), l, norm(w))\n",
    "    x = x + l*p\n",
    "    k = 1\n",
    "    while norm(p) > epsilon:\n",
    "        w_2 = -gr(x)\n",
    "#         gamma = 0\n",
    "#         if k % n != 0:\n",
    "            gamma = np.divide(np.dot(w_2 - w, w_2), np.power(norm(w), 2))\n",
    "        p = gamma*p + w_2\n",
    "        phi = toOneParamFunc(f, x, p)\n",
    "        l, i = argmin(phi, 0, 1, epsilon) \n",
    "        n += i\n",
    "        print(x, f(x), l, norm(w_2))\n",
    "        x = x + l*p\n",
    "        w = w_2\n",
    "        k += 1\n",
    "    return f(x), x, k, n\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d6073a2-9bea-47cc-ba89-1b8db5589627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -2] 13 0.08641208515967441 17.08800732430959\n",
      "[ 0.38259335 -1.48152749] 3.0312661795548737 0.3926112085159674 3.4898519294762607\n",
      "[ 0.16352927 -0.10041696] 0.7158526356196627 0.491546737633062 1.6099780373832036\n",
      "[0.86026478 0.55864271] 0.05243652710301816 0.2886663243581716 0.5005165110397883\n",
      "[0.87224152 0.7688523 ] 0.016386983559418717 0.3876018159048215 0.28404902059240583\n",
      "[0.9939935  0.97026447] 0.0003514459836750528 0.1477771195992486 0.06851875075807531\n",
      "[0.99792458 0.9963406 ] 4.544682442950059e-06 0.24420792736380711 0.006172668084462344\n",
      "[1.0000079  1.00004817] 1.1105122826242846e-09 0.12773956167814654 0.0001308176732386466\n",
      "\n",
      "x minimum: [1.00000128 1.00000229],\n",
      "f minimum: 1.7281815932334995e-12,\n",
      "number of iterations: 8\n",
      "number of one-dimension minimization iterations: 128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1 = lambda x: 6*x[0]**2 - 4*x[0]*x[1] + 3*x[1]**2 + 4*m.sqrt(5)*(x[0] + 2*x[1]) + 22\n",
    "f2 = lambda x: (x[0]**2 - x[1])**2 + (x[0] - 1)**2\n",
    "danilov = lambda x: x[0] + 2*x[1] + 4*m.sqrt(1 + x[0]**2 + x[1]**2)\n",
    "\n",
    "test1 = [\n",
    "    f1,\n",
    "    approx_gradient(f1, 1e-8),\n",
    "    np.array([-2, 1]),\n",
    "    0.01,\n",
    "]\n",
    "test2 = [\n",
    "    f2,\n",
    "    approx_gradient(f2, 1e-8),\n",
    "    np.array([-1, -2]),\n",
    "    0.001,\n",
    "]\n",
    "\n",
    "test_danilov = [\n",
    "    danilov,\n",
    "    approx_gradient(danilov, 1e-8),\n",
    "    np.array([50, 41]),\n",
    "    0.01,\n",
    "]\n",
    "\n",
    "fmin, xmin, K, N = conjugate_gradient(*test2)\n",
    "print(f\"\"\"\n",
    "x minimum: {xmin},\n",
    "f minimum: {fmin},\n",
    "number of iterations: {K}\n",
    "number of one-dimension minimization iterations: {N}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2de9ad-43ca-44ff-b5bb-af346ba1aaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
