{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0268044-01de-4c3d-8990-6b05bc30cdae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gradient methods\n",
    "\n",
    "#### Proplem:\n",
    "$$\n",
    "f(\\vec{x}) \\rightarrow min,\\\\\n",
    "f: \\Omega \\rightarrow \\mathbb{R}, \\\\\n",
    "\\Omega \\subset \\mathbb{R^n}, f(\\vec{x}) \\mbox{ is convex}, \\\\\n",
    "f(\\vec{x}) \\mbox{ - is diffirentiable on } \\Omega\\\\\n",
    "\\vec{x_*} \\in \\Omega, f_{min} = f(\\vec{x_*})\n",
    "$$\n",
    "\n",
    "<em>**Definition**</em>.\n",
    "\n",
    "Sequnce $\\{\\vec{x_k}\\}$ is named **Relaxational**, if $\\forall k \\in \\mathbb{N}:  f(\\vec{x_k}) < f(\\vec{x}_{k-1})$ \n",
    "\n",
    "$\\{\\vec{x}_l\\}$ convergece to $\\vec{x}_* \\in \\mathbb{R}^n$ by Bolzanoâ€“Weierstrass theorem \n",
    "\n",
    "Let's choose our relaxational sequence by this equation:\n",
    "$$\n",
    "\\vec{x}_k = \\vec{x}_{k-1} + \\beta_k\\vec{u}_k\n",
    "$$\n",
    "where $\\vec{u}_{k}$ is unit vector, which defines the direction of descent and $\\beta_k \\geq 0$ - length of descent step\n",
    "\n",
    "<em>**Lemma**</em>.\n",
    "\n",
    "$f(\\vec{x})$ - is differentiable on $\\Omega \\subset \\mathbb{R}^n$ and $\\exists L > 0$, such that $\\forall \\vec{x}, \\vec{y} \\in \\Omega$:\n",
    "$$\n",
    "||\\nabla f(\\vec{x}) - \\nabla f(\\vec{y})|| \\leq  L ||\\vec{x} = \\vec{y}|| \n",
    "$$\n",
    "Then:\n",
    "$$\n",
    "f(\\vec{x}) - f(\\vec{y}) \\geq (\\nabla f(\\vec{x}), \\vec{x} - \\vec{y}) - \\frac{L}{2}||\\vec{x}-\\vec{y}||^2\n",
    "$$\n",
    "<em>**Definition**</em>.\n",
    "\n",
    "$\\vec{w}(\\vec{x}) = - \\nabla f(\\vec{x})$ is called **antigradient**\n",
    "\n",
    "If we take our $\\vec{u}_k = \\frac{\\vec{w}_k}{||\\vec{w}_k||}$, from our lemma we have, that: \n",
    "\n",
    "$$\n",
    "f(x_{k}) - f(x_{k+1}) \\geq (\\nabla f(x_k), \\vec{x_k} - \\vec{x_k} - \\beta_k \\frac{\\vec{w_k}}{||\\vec{w_k}||}) - \\frac{L}{2} || \\vec{x_k} - \\vec{x_k} - \\beta_k \\frac{\\vec{w_k}}{||\\vec{w_k}||} ||^2 = \\beta_k||\\nabla f(\\vec{x}_k)|| - \\beta_k \\frac{L}{2} \n",
    "$$\n",
    "As we can see gradient must be always posistive (and $> \\frac{L}{2}$),  so that we have a convergece, we get this when function is convex\n",
    "\n",
    "All methods in which $\\vec{u}_k = \\frac{\\vec{w}_k}{||\\vec{w}_k||}$, are named ***gradient methods***, the methods vary on the way we choose our $\\beta_k > 0$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68777a30-a1ca-4e9d-bf2c-ef35d7345a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mplib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2c72e-20bd-4ee9-a5e4-520d68c55139",
   "metadata": {},
   "source": [
    "### Conjugate gradient method\n",
    "\n",
    "#### Problem \n",
    "\n",
    "$$\n",
    "f(\\vec{x}) = \\frac{1}{2}(Q\\vec{x}, \\vec{x}) + (\\vec{c}, \\vec{x}) \\rightarrow min\n",
    "$$\n",
    "\n",
    "$Q$ is positive determined n-dimsensional matrix, $c \\in \\mathbb{R}$ - constant\n",
    "\n",
    "This function has single point of minimum $x_* = -Q^{-1}\\vec{c}$\n",
    "\n",
    "To find the inverted matrix $Q^{-1}$ we can use\n",
    "$$\n",
    "Q^{-1} = \\sum^n_{i=1}\\frac{p^i(p^i)^T}{(Qp^i, p^i)}\n",
    "$$\n",
    "Where $p^i \\in \\mathbb{R}$ is conjugate vector of matrix $Q$\n",
    "\n",
    "But constructing a system of conjugate vectors is a pretty complex problem.\n",
    "\n",
    "So we do another way, let's construct system of conjugate vectors on every iteration\n",
    "\n",
    "$\\vec{x}_0$ is a starting point, antrigradient in this point is $\\vec{w}_1 = -Qx_0 - c$ and let's choose $\\vec{p}_1 = \\vec{w}$\n",
    "\n",
    "Using $\\vec{x}_k = \\vec{x}_{k-1} + \\lambda_k\\vec{w}_k$\n",
    "\n",
    "We can find that \n",
    "$$\\lambda_1 = \\frac{|\\vec{w}_1|^2}{(Q\\vec{w}_1, \\vec{w}_1)} = \\frac{|\\vec{p}_1|^2}{Q\\vec{p}_1, \\vec{p}_1)}$$\n",
    "(from minimization of quadratic function)\n",
    "\n",
    "And so $x_1 = x_0 + \\lambda_1\\vec{p}_1$\n",
    "\n",
    "On second iteration (k = 2) we evaluate antigradient $\\vec{w}_2 = -Q\\vec{x_1} - c$\n",
    "\n",
    "Let's assume, that\n",
    "$$\\vec{p}_2 = \\gamma_1\\vec{p}_1 + \\vec{w}_2$$\n",
    "\n",
    "If we product scalarly this equation on $Q\\vec{p}_1 \\not = 0$ and demand that $\\vec{p}_1, \\vec{p}_2$ are conjugate (ortogonal) over the matrix $Q$ ($(Q\\vec{p}_1, \\vec{p_2}) = 0$), we can find $\\gamma_1$\n",
    "$$\\gamma_1 = -\\frac{(Q\\vec{p}_1, \\vec{w}_2)}{(Q\\vec{p}_1, \\vec{p}_1)}$$\n",
    "\n",
    "Contniuing constructing this system of conjugate vectors, we can say, that on every k iteration we have system of equations:\n",
    "$$\n",
    "\\begin{cases}\n",
    "    p_{k+1} = \\gamma\\vec{p_k} + \\vec{w}_{k+1} \\\\\n",
    "    \\gamma_k = - \\frac{(Q\\vec{p}_k, \\vec{w}_{k+1})}{(Q\\vec{p}_k, \\vec{p}_k)} \\\\\n",
    "    \\vec{w}_{k+1} = \\vec{w}_k = \\lambda_kQ\\vec{p}_k \\\\\n",
    "    (Q\\vec{p}_{k+1}, \\vec{p}_i) = 0 \\\\\n",
    "    (\\vec{w}_{k+1}, \\vec{w}_i) = 0, i = \\overline{1, k} \\\\\n",
    "\\end{cases} \\\\\n",
    "\\mbox{also } \\\\\n",
    "\\lambda_k = \\frac{(\\vec{w}_k, \\vec{p}_k)}{(Q\\vec{p}_k, \\vec{p}_k)},\\\\\n",
    "\\vec{x}_k = \\vec{x_1} + \\lambda_k\\vec{p}_k\n",
    "$$\n",
    "\n",
    "With n steps we can find all $\\vec{p}_k$ conjugate vectors and evaluate our minimum $x_* = -Q^{-1}\\vec{c}$,  \n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "\n",
    "To use this method in our problems (non-quadratic function optimization, we need to remove matrix $Q$ from system of equations\n",
    "\n",
    "We can do this, by if on every iteration by doing minimization process:\n",
    "$$\n",
    "\\psi_k(\\lambda) = f(x_{k-1} + \\lambda)\n",
    "$$\n",
    "\n",
    "In fundament of constructing conjuguate directions $\\vec{p}_{k+1} + \\gamma_k\\vec{p}_k + \\vec{w}_{k+1}$ we assume, that $(\\vec{w}_{k+1}, \\vec{w}_i) = 0$\n",
    "\n",
    "Using this we can show that:\n",
    "$$\n",
    "\\begin{cases}\n",
    "    (Q\\vec{p}_k, \\vec{w}_{k+1}) = - \\frac{1}{\\lambda_k}|\\vec{w}_{k+1}|^2 \\\\\n",
    "    (Q\\vec{p}_k, \\vec{p}_{k}) = \\frac{1}{\\lambda_k}(\\vec{w}_k, \\vec{p}_k)\n",
    "\\end{cases} \\\\\n",
    "\\mbox{so from our system of equations we can evaluate $\\gamma$ using one of theese formulas: } \\\\\n",
    "\\gamma_k = \\frac{|\\vec{w}_{k+1}|^2}{|\\vec{w}_k|^2} \\\\\n",
    "\\gamma_k = \\frac{(\\vec{w}_{k+1} - \\vec{w}_k, \\vec{w}_{k+1})}{|\\vec{w}_k|^2} \\\\\n",
    "\\mbox{also if function twice differentiable, we can use Hessian instead of matrix Q:} \\\\\n",
    "\\gamma_k = - \\frac{(H(\\vec{x}_k)\\vec{p}_k, \\vec{w}_{k+1})}{(H(\\vec{x}_k)\\vec{p}_k, \\vec{p}_k)} \\\\\n",
    "$$\n",
    "\n",
    "This method is called ***conjaguate gradients method***\n",
    "\n",
    "Also as every $\\gamma_k$ is different and we need to minimize $\\psi_k(\\lambda)$ this turns us to inevitably errors, to minimize errors, we need to do **restarts** (set $\\gamma_k = 0$). It is common to restart every $n$ times, where $n$ is our dimension number. Also, with non-quadratic functions our procedure of optimization in general don't take $n$ steps, so we choose our $\\epsilon$ and iterate through $\\{\\vec{x}_k\\}$ till our $\\vec{w}_{k+1} < \\epsilon$, and then $x_{k-1} \\approx x_*$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f633d-0183-402f-bb7b-1e10a6d007ba",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
