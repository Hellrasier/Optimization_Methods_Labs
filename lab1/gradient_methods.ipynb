{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0268044-01de-4c3d-8990-6b05bc30cdae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gradient methods\n",
    "\n",
    "#### Proplem:\n",
    "$$\n",
    "f(\\vec{x}) \\rightarrow min,\\\\\n",
    "f: \\Omega \\rightarrow \\mathbb{R}, \\\\\n",
    "\\Omega \\subset \\mathbb{R^n}, f(\\vec{x}) \\mbox{ is convex}, \\\\\n",
    "f(\\vec{x}) \\mbox{ - is diffirentiable on } \\Omega\\\\\n",
    "\\vec{x_*} \\in \\Omega, f_{min} = f(\\vec{x_*})\n",
    "$$\n",
    "\n",
    "<em>**Definition**</em>.\n",
    "\n",
    "Sequnce $\\{\\vec{x_k}\\}$ is named **Relaxational**, if $\\forall k \\in \\mathbb{N}:  f(\\vec{x_k}) < f(\\vec{x}_{k-1})$ \n",
    "\n",
    "$\\{\\vec{x}_l\\}$ convergece to $\\vec{x}_* \\in \\mathbb{R}^n$ by Bolzanoâ€“Weierstrass theorem \n",
    "\n",
    "Let's choose our relaxational sequence by this equation:\n",
    "$$\n",
    "\\vec{x}_k = \\vec{x}_{k-1} + \\beta_k\\vec{u}_k\n",
    "$$\n",
    "where $\\vec{u}_{k}$ is unit vector, which defines the direction of descent and $\\beta_k \\geq 0$ - length of descent step\n",
    "\n",
    "<em>**Lemma**</em>.\n",
    "\n",
    "$f(\\vec{x})$ - is differentiable on $\\Omega \\subset \\mathbb{R}^n$ and $\\exists L > 0$, such that $\\forall \\vec{x}, \\vec{y} \\in \\Omega$:\n",
    "$$\n",
    "||\\nabla f(\\vec{x}) - \\nabla f(\\vec{y})|| \\leq  L ||\\vec{x} = \\vec{y}|| \n",
    "$$\n",
    "Then:\n",
    "$$\n",
    "f(\\vec{x}) - f(\\vec{y}) \\geq (\\nabla f(\\vec{x}), \\vec{x} - \\vec{y}) - \\frac{L}{2}||\\vec{x}-\\vec{y}||^2\n",
    "$$\n",
    "<em>**Definition**</em>.\n",
    "\n",
    "$\\vec{w}(\\vec{x}) = - \\nabla f(\\vec{x})$ is called **antigradient**\n",
    "\n",
    "If we take our $\\vec{u}_k = \\frac{\\vec{w}_k}{||\\vec{w}_k||}$, from our lemma we have, that: \n",
    "\n",
    "$$\n",
    "f(x_{k}) - f(x_{k+1}) \\geq (\\nabla f(x_k), \\vec{x_k} - \\vec{x_k} - \\beta_k \\frac{\\vec{w_k}}{||\\vec{w_k}||}) - \\frac{L}{2} || \\vec{x_k} - \\vec{x_k} - \\beta_k \\frac{\\vec{w_k}}{||\\vec{w_k}||} ||^2 = \\beta_k||\\nabla f(\\vec{x}_k)|| - \\beta_k \\frac{L}{2} \n",
    "$$\n",
    "As we can see gradient must be always posistive (and $> \\frac{L}{2}$),  so that we have a convergece, we get this when function is convex\n",
    "\n",
    "All methods in which $\\vec{u}_k = \\frac{\\vec{w}_k}{||\\vec{w}_k||}$, are named ***gradient methods***, the methods vary on the way we choose our $\\beta_k > 0$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68777a30-a1ca-4e9d-bf2c-ef35d7345a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mplib\n",
    "import math as m\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from onedim_optimize import newton_method\n",
    "from scipy.optimize import approx_fprime\n",
    "\n",
    "def toOneParamFunc(f, x, w):\n",
    "    return lambda p: f(x + p*w) \n",
    "\n",
    "def argmin(f, x, eps):\n",
    "    f_x, xmin, k = newton_method(f, x, eps)\n",
    "    return xmin, k\n",
    "\n",
    "def approx_gradient(f, eps):\n",
    "    return lambda x: approx_fprime(x, f, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c44b76-b160-47a4-a808-7d3a82ae1dd9",
   "metadata": {},
   "source": [
    "### Test functions\n",
    "\n",
    "#### Rosenbrock banana function:\n",
    "$$\n",
    "f(x_1, x_2, ..., x_N) = \\sum^{N/2}_{i=1}[100(x^2_{2i-1} - x_{2i})^2 + (x_{2i-1} - 1)^2]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2fc117-d476-4c87-a60f-37dd21fec44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    build_ros_terms = lambda vec: x.map\n",
    "    return lambda x: reduce(func, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da7e1e-bc40-4eca-96a5-e9add234d8a0",
   "metadata": {},
   "source": [
    "### Fastest descent method\n",
    "\n",
    "We will construct relaxational sequence, using this rule:\n",
    "$$\n",
    "\\vec{x}_{k+1} = \\vec{x}_k + \\lambda_k\\vec{w}_K\n",
    "$$\n",
    "\n",
    "Where $\\lambda_k$ is found from\n",
    "$$\n",
    "\\lambda_k = argmin\\{\\psi_k(\\lambda)\\} \\\\\n",
    "\\psi_k(\\lambda) = f(\\vec{x}_{k-1} + \\lambda\\vec{w}_k)\n",
    "$$\n",
    "\n",
    "Finding minimum of $\\psi_k(\\lambda)$ is a pretty complex task of one-dimension minimization. But it is guaranteed that $\\{|\\vec{w}_k|\\}$ convergace to 0.\n",
    "\n",
    "So at start we pick some small $\\epsilon$ and continuing procedure while $|\\vec{w}_k\\| > \\epsilon$, than on some N iteration we pick our $x_* = x_N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19911d95-3937-4870-a8ac-ef815ece89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastest_descent(f, gr, x, epsilon):\n",
    "    n = len(x)\n",
    "    w = -gr(x) \n",
    "    phi = toOneParamFunc(f, x, w)\n",
    "    l, i = argmin(phi, 0, epsilon)\n",
    "    n += i\n",
    "    k = 1\n",
    "    print(x, f(x), l, norm(w))\n",
    "    x = x + l*w\n",
    "    while(norm(w) > epsilon):\n",
    "        w = -gr(x) \n",
    "        phi = toOneParamFunc(f, x, w)\n",
    "        l, i = argmin(phi, 0, epsilon)\n",
    "        n += i\n",
    "        k += 1\n",
    "        print(x, f(x), l, norm(w))\n",
    "        x = x + l*w\n",
    "    return f(x), x, k, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cdbf93e-3bf1-4f62-a866-b2809c946690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -2] 13 0.08615979586424602 17.08800732430959\n",
      "[ 0.37855672 -1.48304122] 3.031194410271926 0.33477672577953216 3.473881365831169\n",
      "[-0.02979422 -0.39411538] 1.2165035678374376 0.2941390100226454 2.2499202860491687\n",
      "[ 0.58985773 -0.16174376] 0.42798620486760236 0.2584189708764911 1.0886694780047377\n",
      "[0.49107435 0.10167608] 0.278459413443343 0.27187016106120226 0.7944599505351536\n",
      "[0.69331181 0.17751586] 0.1859669109978042 0.23312279277088116 0.6475623544330313\n",
      "[0.64030522 0.31886538] 0.1376841732552048 0.24263049498239633 0.5190459669552201\n",
      "[0.75822298 0.36308498] 0.10332261616893182 0.21937038274234105 0.4524420985482393\n",
      "[0.72337283 0.45601778] 0.08104521598502114 0.22334395023759432 0.3830555194902436\n",
      "[0.80347863 0.48605775] 0.06406732979570523 0.21055433552507619 0.34073566558359303\n",
      "[0.7782876  0.55323307] 0.05191248179446088 0.2099527933707892 0.29902875145012625\n",
      "[0.83707207 0.57527749] 0.042273719770566404 0.20437179805116157 0.2678808582982234\n",
      "[0.81784881 0.6265389 ] 0.0349715428231308 0.20017647743101713 0.24115352669696047\n",
      "[0.86304841 0.64348895] 0.029030318063653082 0.19978543087086972 0.21651307235957665\n",
      "[0.84785999 0.68399089] 0.024362894375542402 0.19276047426539825 0.19864970327923678\n",
      "[0.88371368 0.69743619] 0.020497041652376372 0.19625075366682723 0.17838555484146687\n",
      "[0.8714213  0.73021543] 0.01738276731172284 0.18696956780383048 0.1660914452386958\n",
      "[0.90049806 0.74111936] 0.014769520386371016 0.19345087864821206 0.14904482594149834\n",
      "[0.89037404 0.76811636] 0.012625452137073563 0.18234579792212238 0.1404022547868803\n",
      "[0.91434568 0.77710585] 0.010808484716222913 0.1911871168115002 0.12585802998595833\n",
      "[0.90589668 0.79963617] 0.00929696572059466 0.1785889160609607 0.1196862818819374\n",
      "[0.92591034 0.80714141] 0.00800616114285707 0.18932763766152205 0.10716028553609666\n",
      "[0.9187865  0.82613799] 0.006920736517917882 0.17549362456126186 0.10270097857600895\n",
      "[0.93566227 0.83246652] 0.005988117087321639 0.18778107989158668 0.09184261644485368\n",
      "[0.92960659 0.8486147 ] 0.0051971505859801665 0.17291446277533667 0.08859242110932151\n",
      "[0.94395009 0.85399362] 0.004514158344902878 0.1864818394847415 0.07913510851342984\n",
      "[0.93876838 0.86781123] 0.003930882616170082 0.17074540478515593 0.07675121018635277\n",
      "[0.95103886 0.87241276] 0.003425174619725259 0.18538138413920027 0.06848497363665969\n",
      "[0.94658096 0.88430022] 0.0029908418982473295 0.16890720637604295 0.06672896807600782\n",
      "[0.9571343  0.88825781] 0.0026129936421857313 0.18444296725321546 0.059484088578659985\n",
      "[0.95328189 0.89853064] 0.0022869428507707465 0.16733937882462246 0.05818735839228691\n",
      "[0.96239893 0.90194962] 0.0020024888682293523 0.1836381889394491 0.051823997064773725\n",
      "[0.95905725 0.91086051] 0.0017560591193140722 0.16599490570616332 0.05086562491037426\n",
      "[0.96696305 0.91382527] 0.0015405522345165277 0.18294471265545043 0.04526688728341257\n",
      "[0.9640552 0.9215793] 0.0013532301190755495 0.16483667342136507 0.04455916959136656\n",
      "[0.97093249 0.92415837] 0.001189079297025211 0.182344703395275 0.03962625859540452\n",
      "[0.96839532 0.93092392] 0.0010459920435173396 0.1638349798057606 0.03910498318043055\n",
      "[0.97439413 0.93317356] 0.0009203852494108627 0.1818237605007745 0.03475367689359027\n",
      "[0.97217528 0.93909023] 0.0008106306563222068 0.16296576528845544 0.03437146867450149\n",
      "[0.97741998 0.94105708] 0.0007141396605612577 0.18137008922459325 0.03052948270253099\n",
      "[0.97547568 0.94624163] 0.0006296507007241827 0.16220934604834092 0.030251167951956252\n",
      "[0.98007024 0.94796468] 0.0005552757083616249 0.18097396612934477 0.026856127214730463\n",
      "[0.97836361 0.95251545] 0.0004900348370827484 0.16154948981242492 0.026655459401986166\n",
      "[0.98239558 0.95402752] 0.00043253925526340483 0.180627206210623 0.023653306776428087\n",
      "[0.98089535 0.95802789] 0.00038202634693897873 0.16097263059766323 0.023510613243277\n",
      "[0.98443892 0.95935681] 0.00033746676859871047 0.18032316155112485 0.020854324304894912\n",
      "[0.98311843 0.96287787] 0.00029826592573625196 0.16046738304971747 0.020754840038273565\n",
      "[0.98623683 0.96404735] 0.0002636557635473931 0.18005600818457862 0.0184033833194539\n",
      "[0.98507325 0.96714997] 0.0002331719917905693 0.1600241901411083 0.01833598301352433\n",
      "[0.9878206  0.96818031] 0.00020623805851248828 0.17982088704809004 0.01625347645413726\n",
      "[0.98679428 0.97091691] 0.00018249095125460926 0.1596349131330229 0.01620977569925351\n",
      "[0.98921714 0.97182556] 0.00016149553664302532 0.17961362477713216 0.014364769572849314\n",
      "[0.98831113 0.97424136] 0.00014296766969422702 0.15929261396006217 0.014338445630624568\n",
      "[0.99044968 0.97504341] 0.00012657736276520248 0.17943065496442318 0.012703326931056838\n",
      "[0.98964926 0.97717761] 0.00011210198717370826 0.15899134281595884 0.012689616295078998\n",
      "[0.99153831 0.97788609] 9.929021145416197e-05 0.17926888620812634 0.011240103114098357\n",
      "[0.99083072 0.97977276] 8.796745828681629e-05 0.15872599832035747 0.011235421460570882\n",
      "[0.9925005  0.98039902] 7.794156244735498e-05 0.1791256399300063 0.009950134775395483\n",
      "[0.9918746  0.98206783] 6.907552666524486e-05 0.1584921769315381 0.00995179240473454\n",
      "[0.99335143 0.98262172] 6.122191331563605e-05 0.1789985950359944 0.008811887423621807\n",
      "[0.99279751 0.98409858] 5.4273147198403396e-05 0.158286069829853 0.0088178738124236\n",
      "[0.99410436 0.98458873] 4.8115756360144854e-05 0.1788857090547376 0.007806723605564266\n",
      "[0.99361393 0.98589629] 4.266522207231509e-05 0.1581043815214139 0.007815543641028879\n",
      "[0.9947709  0.98633024] 3.783396980888774e-05 0.17878521347432674 0.00691846605578471\n",
      "[0.9943365  0.98748837] 3.355556384646353e-05 0.15794423580240458 0.006929016573074884\n",
      "[0.99536119 0.98787272] 2.9762247802814906e-05 0.17869554694702955 0.006133036504367797\n",
      "[0.9949763  0.98889885] 2.64017783948837e-05 0.15780314003074958 0.006144511165844014\n",
      "[0.99588416 0.98923938] 2.3421615238784162e-05 0.17861532953538095 0.005438155332198646\n",
      "[0.99554301 0.99014885] 2.078066088936095e-05 0.15767890741667065 0.005449972447853641\n",
      "[0.99634762 0.99045066] 1.843809209770062e-05 0.1785433829893508 0.0048230893190402985\n",
      "[0.99604517 0.99125693] 1.636156934917735e-05 0.1575696301768774 0.004834837667506824\n",
      "[0.99675846 0.9915245 ] 1.4519316468984213e-05 0.17847862342215873 0.004278440789014641\n",
      "[0.99649026 0.99223946] 1.288587888707461e-05 0.15747367026051215 0.004289836296611274\n",
      "[0.99712275 0.99247673] 1.1436482468677779e-05 0.1784200443590357 0.00379596797619307\n",
      "[0.99688486 0.99311085] 1.0151087476741311e-05 0.15738960440831443 0.003806820727626026\n",
      "[0.99744584 0.9933213 ] 9.010350737172633e-06 0.17836678814464063 0.0033684316845678393\n",
      "[0.99723479 0.99388383] 7.998492586830429e-06 0.15731616468693801 0.0033786226548442673\n",
      "[0.99773244 0.99407053] 7.100390503047199e-06 0.17831809378530183 0.0029894654761969536\n",
      "[0.99754518 0.99456964] 6.303617447645165e-06 0.15725225067714288 0.0029989280281756524\n",
      "[0.99798671 0.9947353 ] 5.596336614231211e-06 0.1782732691938724 0.002653464403449871\n",
      "[0.99782053 0.99517819] 4.9687601805748615e-06 0.15719692710202396 0.0026621697780455974\n",
      "[0.99821234 0.9953252 ] 4.411613232943407e-06 0.17823163773733244 0.002355489079482031\n",
      "[0.99806485 0.99571826] 3.917186034176649e-06 0.15714939789355759 0.002363434980200145\n",
      "[0.99841258 0.99584875] 3.4782040302006685e-06 0.17819258863119639 0.002091182354483518\n",
      "[0.99828166 0.99619763] 3.088594029271292e-06 0.15710896363580273 0.0020983852208449395\n",
      "[0.99859031 0.99631346] 2.7426453576672357e-06 0.17815557189434922 0.0018566975801665333\n",
      "[0.99847409 0.99662315] 2.435573869175057e-06 0.15707504298883307 0.001863186735194228\n",
      "[0.99874808 0.99672598] 2.1628929185228777e-06 0.17812004861939468 0.0016486367309945474\n",
      "[0.99864489 0.99700091] 1.920833826601448e-06 0.15704713297570122 0.0016544495563095621\n",
      "[0.99888814 0.99709222] 1.7058689818251101e-06 0.1780855310358696 0.0014639958100743711\n",
      "[0.99879652 0.9973363 ] 1.5150297255194392e-06 0.15702483202625203 0.0014691746571306096\n",
      "[0.9990125  0.99741738] 1.345540560531978e-06 0.17805151962653493 0.001300118033867912\n",
      "[0.99893113 0.9976341 ] 1.1950631634112725e-06 0.15700782986458348 0.0013047075009419414\n",
      "[0.99912291 0.9977061 ] 1.061412371540459e-06 0.17801750843395234 0.0011546523821676497\n",
      "[0.99905065 0.99789853] 9.427465311540021e-07 0.15699587840013507 0.0011586975100633498\n",
      "[0.99922095 0.99796248] 8.37344190535544e-07 0.17798304449016233 0.0010255173417738719\n",
      "[0.99915678 0.99813335] 7.437550489017493e-07 0.15698879841889313 0.001029062681512701\n",
      "[0.99930801 0.99819015] 6.606221638785081e-07 0.1779476406284351 0.0009108695012399728\n",
      "\n",
      "x minimum: [0.99925102 0.99834189],\n",
      "f minimum: 5.868036049493666e-07,\n",
      "number of iterations: 98\n",
      "number of one-dimension minimization iterations: 233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1 = lambda x: 6*x[0]**2 - 4*x[0]*x[1] + 3*x[1]**2 + 4*m.sqrt(5)*(x[0] + 2*x[1]) + 22\n",
    "f2 = lambda x: (x[0]**2 - x[1])**2 + (x[0] - 1)**2\n",
    "\n",
    "test1 = [\n",
    "    f1,\n",
    "    approx_gradient(f1, 1e-8),\n",
    "    np.array([-2, 1]),\n",
    "    0.01,\n",
    "]\n",
    "test2 = [\n",
    "    f2,\n",
    "    approx_gradient(f2, 1e-8),\n",
    "    np.array([-1, -2]),\n",
    "    0.001,\n",
    "]\n",
    "\n",
    "gradient = approx_gradient(test, 1e-8)\n",
    "fmin, xmin, K, N = fastest_descent(*test1)\n",
    "print(f\"\"\"\n",
    "x minimum: {xmin},\n",
    "f minimum: {fmin},\n",
    "number of iterations: {K}\n",
    "number of one-dimension minimization iterations: {N}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2c72e-20bd-4ee9-a5e4-520d68c55139",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conjugate gradient method\n",
    "\n",
    "#### Problem \n",
    "\n",
    "$$\n",
    "f(\\vec{x}) = \\frac{1}{2}(Q\\vec{x}, \\vec{x}) + (\\vec{c}, \\vec{x}) \\rightarrow min\n",
    "$$\n",
    "\n",
    "$Q$ is positive determined n-dimsensional matrix, $c \\in \\mathbb{R}$ - constant\n",
    "\n",
    "This function has single point of minimum $x_* = -Q^{-1}\\vec{c}$\n",
    "\n",
    "To find the inverted matrix $Q^{-1}$ we can use\n",
    "$$\n",
    "Q^{-1} = \\sum^n_{i=1}\\frac{p^i(p^i)^T}{(Qp^i, p^i)}\n",
    "$$\n",
    "Where $p^i \\in \\mathbb{R}$ is conjugate vector of matrix $Q$\n",
    "\n",
    "But constructing a system of conjugate vectors is a pretty complex problem.\n",
    "\n",
    "So we do another way, let's construct system of conjugate vectors on every iteration\n",
    "\n",
    "$\\vec{x}_0$ is a starting point, antrigradient in this point is $\\vec{w}_1 = -Qx_0 - c$ and let's choose $\\vec{p}_1 = \\vec{w}$\n",
    "\n",
    "Using $\\vec{x}_k = \\vec{x}_{k-1} + \\lambda_k\\vec{w}_k$\n",
    "\n",
    "We can find that \n",
    "$$\\lambda_1 = \\frac{|\\vec{w}_1|^2}{(Q\\vec{w}_1, \\vec{w}_1)} = \\frac{|\\vec{p}_1|^2}{(Q\\vec{p}_1, \\vec{p}_1)}$$\n",
    "(from minimization of quadratic function)\n",
    "\n",
    "And so $x_1 = x_0 + \\lambda_1\\vec{p}_1$\n",
    "\n",
    "On second iteration (k = 2) we evaluate antigradient $\\vec{w}_2 = -Q\\vec{x_1} - c$\n",
    "\n",
    "Let's assume, that\n",
    "$$\\vec{p}_2 = \\gamma_1\\vec{p}_1 + \\vec{w}_2$$\n",
    "\n",
    "If we product scalarly this equation on $Q\\vec{p}_1 \\not = 0$ and demand that $\\vec{p}_1, \\vec{p}_2$ are conjugate (ortogonal) over the matrix $Q$ ($(Q\\vec{p}_1, \\vec{p_2}) = 0$), we can find $\\gamma_1$\n",
    "$$\\gamma_1 = -\\frac{(Q\\vec{p}_1, \\vec{w}_2)}{(Q\\vec{p}_1, \\vec{p}_1)}$$\n",
    "\n",
    "Contniuing constructing this system of conjugate vectors, we can say, that on every k iteration we have system of equations:\n",
    "$$\n",
    "\\begin{cases}\n",
    "    p_{k+1} = \\gamma\\vec{p_k} + \\vec{w}_{k+1} \\\\\n",
    "    \\gamma_k = - \\frac{(Q\\vec{p}_k, \\vec{w}_{k+1})}{(Q\\vec{p}_k, \\vec{p}_k)} \\\\\n",
    "    \\vec{w}_{k+1} = \\vec{w}_k = \\lambda_kQ\\vec{p}_k \\\\\n",
    "    (Q\\vec{p}_{k+1}, \\vec{p}_i) = 0 \\\\\n",
    "    (\\vec{w}_{k+1}, \\vec{w}_i) = 0, i = \\overline{1, k} \\\\\n",
    "\\end{cases} \\\\\n",
    "\\mbox{also } \\\\\n",
    "\\lambda_k = \\frac{(\\vec{w}_k, \\vec{p}_k)}{(Q\\vec{p}_k, \\vec{p}_k)},\\\\\n",
    "\\vec{x}_k = \\vec{x_1} + \\lambda_k\\vec{p}_k\n",
    "$$\n",
    "\n",
    "With n steps we can find all $\\vec{p}_k$ conjugate vectors and evaluate our minimum $x_* = -Q^{-1}\\vec{c}$\n",
    "\n",
    "To use this method in our problems (non-quadratic function optimization, we need to remove matrix $Q$ from system of equations\n",
    "\n",
    "We can do this, by if on every iteration by doing minimization process:\n",
    "$$\n",
    "\\psi_k(\\lambda) = f(x_{k-1} + \\lambda)\n",
    "$$\n",
    "\n",
    "In fundament of constructing conjuguate directions $\\vec{p}_{k+1} = \\gamma_k\\vec{p}_k + \\vec{w}_{k+1}$ we assume, that $(\\vec{w}_{k+1}, \\vec{w}_i) = 0$\n",
    "\n",
    "Using this we can show that:\n",
    "$$\n",
    "\\begin{cases}\n",
    "    (Q\\vec{p}_k, \\vec{w}_{k+1}) = - \\frac{1}{\\lambda_k}|\\vec{w}_{k+1}|^2 \\\\\n",
    "    (Q\\vec{p}_k, \\vec{p}_{k}) = \\frac{1}{\\lambda_k}(\\vec{w}_k, \\vec{p}_k)\n",
    "\\end{cases} \\\\\n",
    "\\mbox{so from our system of equations we can evaluate $\\gamma$ using one of theese formulas: } \\\\\n",
    "\\gamma_k = \\frac{|\\vec{w}_{k+1}|^2}{|\\vec{w}_k|^2} \\\\\n",
    "\\gamma_k = \\frac{(\\vec{w}_{k+1} - \\vec{w}_k, \\vec{w}_{k+1})}{|\\vec{w}_k|^2} \\\\\n",
    "\\mbox{also if function twice differentiable, we can use Hessian instead of matrix Q:} \\\\\n",
    "\\gamma_k = - \\frac{(H(\\vec{x}_k)\\vec{p}_k, \\vec{w}_{k+1})}{(H(\\vec{x}_k)\\vec{p}_k, \\vec{p}_k)} \\\\\n",
    "$$\n",
    "\n",
    "This method is called ***conjaguate gradients method***\n",
    "\n",
    "Also as every $\\gamma_k$ is different and we need to minimize $\\psi_k(\\lambda)$ this turns us to inevitably errors, to minimize errors, we need to do **restarts** (set $\\gamma_k = 0$). It is common to restart every $n$ times, where $n$ is our dimension number. Also, with non-quadratic functions our procedure of optimization in general don't take $n$ steps, so we choose our $\\epsilon$ and iterate through $\\{\\vec{x}_k\\}$ till our |$\\vec{w}_{k+1|} < \\epsilon$, and then $x_{k-1} \\approx x_*$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "872bd742-b7b8-477b-8406-82ab2cb9ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(f, gr, x, epsilon):\n",
    "    w = -gr(x) \n",
    "    p = w\n",
    "    phi = toOneParamFunc(f, x, p)\n",
    "    l, i = argmin(phi, 0, epsilon)\n",
    "    n = i\n",
    "    print(x, f(x), l, norm(w))\n",
    "    x = x + l*p\n",
    "    k = 1\n",
    "    while norm(p) > epsilon:\n",
    "        w_2 = -gr(x)\n",
    "        gamma = 0\n",
    "        if k % n != 0:\n",
    "            gamma = (np.dot(w_2 - w, w_2))/norm(w)**2\n",
    "        p = gamma*p + w_2\n",
    "        phi = toOneParamFunc(f, x, p)\n",
    "        l, i = argmin(phi, 0, epsilon) \n",
    "        n += i\n",
    "        print(x, f(x), l, norm(w_2))\n",
    "        x = x + l*p\n",
    "        w = w_2\n",
    "        k += 1\n",
    "    return f(x), x, k, n\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d6073a2-9bea-47cc-ba89-1b8db5589627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -2] 13 0.08615979586424602 17.08800732430959\n",
      "[ 0.37855672 -1.48304122] 3.031194410271926 0.39429340464599333 3.473881365831169\n",
      "[ 0.15834029 -0.10275169] 0.7247298758120327 0.4906820879947748 1.6226263306501927\n",
      "[0.85929719 0.55729545] 0.052593117788155466 0.2896826851184612 0.4974964403391633\n",
      "[0.87074064 0.76602386] 0.016769364424484637 0.3915138203121558 0.2862355375850555\n",
      "[0.9942033  0.97084451] 0.000343210345130694 0.1459897693910945 0.0681676619727319\n",
      "[0.99797699 0.99638207] 4.272342498360597e-06 0.25380186009263545 0.0058008328114193455\n",
      "[0.99999607 0.99997434] 3.323150132996604e-10 0.12582217179919694 7.26986869561118e-05\n",
      "\n",
      "x minimum: [0.99999997 0.99999994],\n",
      "f minimum: 1.0093783894492261e-15,\n",
      "number of iterations: 8\n",
      "number of one-dimension minimization iterations: 22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1 = lambda x: 6*x[0]**2 - 4*x[0]*x[1] + 3*x[1]**2 + 4*m.sqrt(5)*(x[0] + 2*x[1]) + 22\n",
    "f2 = lambda x: (x[0]**2 - x[1])**2 + (x[0] - 1)**2\n",
    "\n",
    "test1 = [\n",
    "    f1,\n",
    "    approx_gradient(f1, 1e-8),\n",
    "    np.array([-2, 1]),\n",
    "    0.01,\n",
    "]\n",
    "test2 = [\n",
    "    f2,\n",
    "    approx_gradient(f2, 1e-8),\n",
    "    np.array([-1, -2]),\n",
    "    0.001,\n",
    "]\n",
    "\n",
    "fmin, xmin, K, N = conjugate_gradient(*test2)\n",
    "print(f\"\"\"\n",
    "x minimum: {xmin},\n",
    "f minimum: {fmin},\n",
    "number of iterations: {K}\n",
    "number of one-dimension minimization iterations: {N}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2de9ad-43ca-44ff-b5bb-af346ba1aaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
