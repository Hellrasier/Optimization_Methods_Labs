{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9e5005-55c3-4c7e-ab66-a3b3c1417a55",
   "metadata": {},
   "source": [
    "## Newton methods\n",
    "\n",
    "#### Problem:\n",
    "$$\n",
    "f(\\vec{x}) \\rightarrow min,\\\\\n",
    "f: \\Omega \\rightarrow \\mathbb{R}, \\\\\n",
    "\\Omega \\subset \\mathbb{R^n}, f(\\vec{x}) \\mbox{ is convex}, \\\\\n",
    "f(\\vec{x}) \\mbox{ - is twice diffirentiable on } \\Omega\\\\\n",
    "\\vec{x_*} \\in \\Omega, f_{min} = f(\\vec{x_*})\n",
    "$$\n",
    "\n",
    "We can greater efficency of finding $x_*$ if we use information not only about function gradient, but also Hessian $H(\\vec{x})$\n",
    "\n",
    "In simple variant on every k iteration function is approximated in neighborhood of point $\\vec{x}_{k-1}$ by quadratic function $\\phi_{k}(x)$ then $\\vec{x}_k$ is found and the procedure continiues\n",
    "\n",
    "By using Tailor series we can represent our function in neighborhood of point $x_{k}$ as\n",
    "$$\n",
    "f(\\vec{x}) = f(\\vec{x}_{k} + (\\nabla f(\\vec{x}_k), \\vec{x} - \\vec{x}_k) + \\frac{1}{2}(H(\\vec{x}_k)(\\vec{x} - \\vec{x}_k), \\vec{x} - \\vec{x}_k) + o(|\\vec{x} - \\vec{x}_k|)\n",
    "$$\n",
    "\n",
    "So our quadratic approximation $\\phi_k(\\vec{x})$ would be\n",
    "$$\n",
    "\\phi_{k+1}(\\vec{x}) = f(\\vec{x}_{k} + (\\nabla f(\\vec{x}_k), \\vec{x} - \\vec{x}_k) + \\frac{1}{2}(H(\\vec{x}_k)(\\vec{x} - \\vec{x}_k), \\vec{x} - \\vec{x}_k)\n",
    "$$\n",
    "\n",
    "If our $H(\\vec{x}_{k})$ is positive determined (function is convex), then $\\vec{x}_{k+1}$ is single minimum of quadratic approximation and can be found using: \n",
    "$$ \n",
    "\\nabla \\phi_{k+1}(\\vec{x}) = \\nabla f(\\vec{x}_k) + H(\\vec{x}_k)(\\vec{x} - \\vec{x}_k) = \\vec{0}\n",
    "$$\n",
    "\n",
    "Then we get\n",
    "$$\n",
    "\\vec{x}_{k+1} = \\vec{x}_{k}  - H^{-1}(\\vec{x}_{k}) \\nabla f(\\vec{x}_{k})\n",
    "$$\n",
    "\n",
    "If our dimension number $n$ of space $\\mathbb{R}$ is big enough, then finding $H^{-1}$ is very big problem. In this case it expedient to find minimum of $\\phi_k(\\vec{x})$ by using **gradient methods** or **conjugate directions method**\n",
    "$\\widetilde{\\vec{x}}_k = argmin\\{\\phi_{k}(\\vec{x})\\}$ is just an approximation, using this we can build *relaxational sequence* \n",
    "\n",
    "$$\n",
    "\\vec{x}_{k} = \\vec{x}_{k-1} + \\lambda_k(\\widetilde{\\vec{x}}_{k} - \\vec{x}_{k-1}) = \\vec{x}_{k-1} + \\lambda_k\\vec{p}_{k} \\\\ \n",
    "\\vec{p}_k = -H^{-1}(\\vec{x}_{k-1}) \\nabla f(\\vec{x}_{k-1}) \\mbox{ - direction of descent}\n",
    "$$\n",
    "\n",
    "We can find $\\lambda_k$ different ways, for example find $argmin\\{f(\\vec{x}_{k-1} + \\lambda_k\\vec{p}_k\\}$ or by method of step splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3abc9010-594e-40d8-8e55-7d4bf5d5da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mplib\n",
    "import math as m\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy import linalg\n",
    "from scipy import sparse\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import derivative\n",
    "\n",
    "from onedim_optimize import quadratic_approx, fibbonaci_method, middle_point_method, newton_modified, qubic_approx\n",
    "from scipy.optimize import approx_fprime, line_search\n",
    "\n",
    "from test_functions import rastrigin, ackley, sphere, beale, goldstein_price, booth, bukin, himmelblau, eggholder, cross\n",
    "\n",
    "def der(f, x, n):\n",
    "    return derivative(f, x, dx=np.float64(1e-7), n=n)\n",
    "\n",
    "def upgraded_newton(f, a, b, epsilon):\n",
    "    der1 = der(f, a, 1)\n",
    "    sec_der = der(f, a, 2)\n",
    "    if sec_der <= 0:\n",
    "        print(\"Starting Fibbonaci\")\n",
    "        an, bn, k = fibbonaci_method(f, a, b, epsilon/100)\n",
    "        if f((an+bn)/2) > f(b):\n",
    "            return b, k+1\n",
    "        else:\n",
    "            return (an+bn)/2, k+1\n",
    "    x0 = a\n",
    "    x1 = x0 - np.divide(der1, sec_der)\n",
    "    der2 = der(f, x1, 1)\n",
    "    k = 0\n",
    "    while(m.fabs(x0 - x1) > epsilon):\n",
    "        if der2 - der1 == 0:\n",
    "            an, bn, k = fibbonaci_method(f, x1, b, epsilon/100)\n",
    "            return (an+bn)/2, k\n",
    "        x2 = x1 - np.divide(x1 - x0, der2 - der1) * der2\n",
    "        x0 = x1\n",
    "        x1 = x2\n",
    "        der1 = der2\n",
    "        der2 = der(f, x1, 1)\n",
    "        k += 1\n",
    "    if x1 > b:\n",
    "        x1 = b\n",
    "    return x1, k \n",
    "\n",
    "def toOneParamFunc(f, x, w):\n",
    "    return lambda p: f(x + p*w) \n",
    "\n",
    "def build_plot(f, rng, name=\"\"):\n",
    "    X = np.array([i * (rng[1] - rng[0])/100 + rng[0] for i in range(0, 101)])\n",
    "    Y = np.array([f(x) for x in X])\n",
    "    plt.suptitle(name)\n",
    "    plt.plot(X, Y)\n",
    "    return plt\n",
    "\n",
    "def argmin(f, a, b, eps):\n",
    "#     plt = build_plot(f, [a, b])\n",
    "#     plt2 = build_plot(lambda x: der(f, x, 1), [a, b])\n",
    "#     a, b, k = fibbonaci_method(f, a, b, eps)\n",
    "#     K = k\n",
    "#     x = (a+b)/2\n",
    "    x, k =  upgraded_newton(f, a, b, eps)\n",
    "    K = k\n",
    "#     plt.scatter([x], (f(x)))\n",
    "#     plt.show()\n",
    "#     plt2.show()\n",
    "    return x, K\n",
    "\n",
    "\n",
    "\n",
    "def approx_gradient(f, eps):\n",
    "    return lambda x: approx_fprime(x, f, eps)\n",
    "\n",
    "def partial_der(f_dx, f_x, dx, j):\n",
    "    p = np.divide(f_dx - f_x, dx)\n",
    "    return p\n",
    "\n",
    "def hessian_in_point(x, f, grad, eps):\n",
    "     gr = grad(x)\n",
    "     n = len(gr) \n",
    "     hes = []\n",
    "     for i in range (0, n):\n",
    "        x_delta = np.array(x[:])\n",
    "        x_delta[i] = x_delta[i] + eps\n",
    "        gr_delta = grad(x_delta)\n",
    "        partials = np.array([partial_der(gr_delta[j], part, eps, j) for j,part in enumerate(gr)])\n",
    "        hes.append(partials)\n",
    "     return np.array(hes)\n",
    "\n",
    "def hessian(f, grad, eps):\n",
    "    return lambda x: hessian_in_point(x, f, grad, eps) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30fef9a",
   "metadata": {},
   "source": [
    "### Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22c77ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rosenbrok = lambda x: (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2 \n",
    "danilov = lambda x: x[0] + 2*x[1] + 4*m.sqrt(1 + x[0]**2 + x[1]**2)\n",
    "\n",
    "# test1 = [\n",
    "#     f1,\n",
    "#     approx_gradient(f1, 1e-8),\n",
    "#     np.array([-2, 1]),\n",
    "#     0.01,\n",
    "# ]\n",
    "# test2 = [\n",
    "#     f2,\n",
    "#     approx_gradient(f2, 1e-8),\n",
    "#     np.array([-1, -2]),\n",
    "#     0.001,[ 99936.00079581 -69503.99452882]\n",
    "# ]\n",
    "\n",
    "test_danilov = [\n",
    "    danilov,\n",
    "    approx_gradient(danilov, 1e-8),\n",
    "    np.array([10, 5]),\n",
    "    1e-5,\n",
    "]\n",
    "\n",
    "test_rosen = [\n",
    "    rosenbrok,\n",
    "    approx_gradient(rosenbrok, np.float64(1e-8)),\n",
    "    np.array([-2, 2]),\n",
    "    0.001\n",
    "]\n",
    "\n",
    "\n",
    "test_rastrigin = [\n",
    "    rastrigin,\n",
    "    approx_gradient(rastrigin, np.float64(1e-10)),\n",
    "    np.array([2, 1]),\n",
    "    1e-4\n",
    "]\n",
    "\n",
    "test_ackley = [\n",
    "    ackley,\n",
    "    approx_gradient(ackley, np.float64(1e-10)),\n",
    "    np.array([1, 1]),\n",
    "    1e-4\n",
    "]\n",
    "\n",
    "test_sphere = [\n",
    "    sphere,\n",
    "    approx_gradient(sphere, np.float64(1e-10)),\n",
    "    np.array([100, -255]),\n",
    "    1e-5\n",
    "]\n",
    "\n",
    "test_beale = [\n",
    "    beale,\n",
    "    approx_gradient(beale, np.float64(1e-10)),\n",
    "    np.array([2, 2]),\n",
    "    1e-5\n",
    "]\n",
    "\n",
    "test_goldstein = [\n",
    "    goldstein_price,\n",
    "    approx_gradient(goldstein_price, np.float64(1e-9)),\n",
    "    np.array([-3, -1]),\n",
    "    1e-5\n",
    "]\n",
    "\n",
    "test_booth = [\n",
    "    booth,\n",
    "    approx_gradient(booth, np.float64(1e-8)),\n",
    "    np.array([30, 50]),\n",
    "    1e-5\n",
    "]\n",
    "\n",
    "test_bukin = [\n",
    "    bukin,\n",
    "    approx_gradient(bukin, np.float64(1e-10)),\n",
    "    np.array([-10.5, 1.5]),\n",
    "    1e-5\n",
    "]\n",
    "\n",
    "test_himmel = [\n",
    "    himmelblau,\n",
    "    approx_gradient(himmelblau, np.float64(1e-10)),\n",
    "    np.array([56, 41]),\n",
    "    1e-5\n",
    "]\n",
    "\n",
    "test_egg = [\n",
    "    eggholder,\n",
    "    approx_gradient(eggholder, np.float64(1e-10)),\n",
    "    np.array([353, -200]),\n",
    "    1e-7\n",
    "]\n",
    "\n",
    "test_cross = [\n",
    "    cross,\n",
    "    approx_gradient(cross, np.float64(1e-10)),\n",
    "    np.array([2, -2]),\n",
    "    1e-5\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f50293-c0c6-45db-8e43-4fd79da71fcc",
   "metadata": {},
   "source": [
    "### Newton method\n",
    "\n",
    "The common Newton method is to find our $H^{-1}$ matrix and than build relaxetion sequence by this rule:\n",
    "$$\n",
    "\\vec{x}_{k+1} = \\vec{x}_{k}  - H^{-1}(\\vec{x}_{k}) \\nabla f(\\vec{x}_{k})\n",
    "$$\n",
    "\n",
    "But there is a problem with matrix $H$, it needs to be always positive determinated or $H^{-1}$ won't exist.\n",
    "\n",
    "To solve this problem, let's check if $H$ is positive determinated, if not, then let's pick $\\eta$_k, such that:\n",
    "$$\n",
    "\\widetilde{H}_k = \\eta_kI_n + H(\\vec{x}_{k-1})\n",
    "$$\n",
    "$\\widetilde{H}$ is positive determinated matrix, that we pick instead of $H$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be4fcc8-0942-462f-90c1-251d7490210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_newton(f, gr, x, epsilon):\n",
    "    hess = hessian(f2, gr, 1e-6)\n",
    "    w = -gr(x)\n",
    "    k = 0\n",
    "    while(norm(w) > epsilon):\n",
    "        H = hess(x)\n",
    "        print(x, w)\n",
    "        print(H)\n",
    "        h = linalg.solve(H, w)\n",
    "        x = x + h\n",
    "        w = -gr(x)\n",
    "        k += 1\n",
    "    return f(x), x, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44050517",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_109886/782377166.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mfmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommon_newton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m print(f\"\"\"\n\u001b[1;32m     32\u001b[0m \u001b[0mx\u001b[0m \u001b[0mminimum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test2' is not defined"
     ]
    }
   ],
   "source": [
    "f2 = lambda x: (x[0]**2 - x[1])**2 + (x[0] - 1)**2\n",
    "danilov = lambda x: x[0] + 2*x[1] + 4*m.sqrt(1 + x[0]**2 + x[1]**2)\n",
    "rosenbrok = lambda x: (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2 \n",
    "\n",
    "\n",
    "# test2 = [\n",
    "#     f2,\n",
    "#     approx_gradient(f2, 1e-8),\n",
    "# #     hessian(f2, approx_gradient(f2, 1e-6), 1e-6),\n",
    "#     np.array([-1, -2]),\n",
    "#     0.001,\n",
    "# ]\n",
    "\n",
    "# test_danilov = [\n",
    "#     danilov,\n",
    "#     approx_gradient(danilov, np.float64(1e-8)),\n",
    "# #     hessian(danilov, approx_gradient(danilov, np.float64(1e-6)), np.float64(1e-6)),\n",
    "#     np.array([-2, -1]),\n",
    "#     0.01,\n",
    "# ]\n",
    "\n",
    "# test_rosen = [\n",
    "#     rosenbrok,\n",
    "#     approx_gradient(rosenbrok, np.float64(1e-8)),\n",
    "# #     hessian(rosenbrok,  approx_gradient(rosenbrok, np.float64(1e-8)), np.float64(1e-6)),\n",
    "#     np.array([10, 20]),\n",
    "#     np.float64(1e-4)\n",
    "# ]\n",
    "\n",
    "fmin, xmin, K = common_newton(*test2)\n",
    "print(f\"\"\"\n",
    "x minimum: {xmin},\n",
    "f minimum: {fmin},\n",
    "number of iterations: {K}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e07ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_upgraded(f, gr, x, epsilon):\n",
    "    hess = hessian(f2, gr, 1e-6)\n",
    "    w = -gr(x)\n",
    "    phi = toOneParamFunc(f, x, w)\n",
    "    k = 0\n",
    "    n = 0\n",
    "    while(norm(w) > epsilon):\n",
    "        H = hess(x)\n",
    "        print(x, w)\n",
    "        print(H)\n",
    "        h = linalg.solve(H, w)\n",
    "        phi = toOneParamFunc(f, x, h)\n",
    "        l, i = argmin(phi, 0, 1, epsilon) \n",
    "        n += i\n",
    "        x = x + l * h\n",
    "        w = -gr(x)\n",
    "        k += 1\n",
    "    return f(x), x, k, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = lambda x: (x[0]**2 - x[1])**2 + (x[0] - 1)**2\n",
    "danilov = lambda x: x[0] + 2*x[1] + 4*m.sqrt(1 + x[0]**2 + x[1]**2)\n",
    "\n",
    "test2 = [\n",
    "    f2,\n",
    "    approx_gradient(f2, np.float64(1e-8)),\n",
    "#     hessian(f2, approx_gradient(f2, np.float64(1e-6)), np.float64(1e-6)),\n",
    "    np.array([-1, -2]),\n",
    "    np.float64(1e-3),\n",
    "]\n",
    "\n",
    "test_danilov = [\n",
    "    danilov,\n",
    "    approx_gradient(danilov, np.float64(1e-8)),\n",
    "#     hessian(danilov, approx_gradient(danilov, np.float64(1e-8)), np.float64(1e-8)),\n",
    "    np.array([-1, -2]),\n",
    "    0.01,\n",
    "]\n",
    "\n",
    "fmin, xmin, K, N = newton_upgraded(*test2)\n",
    "print(f\"\"\"\n",
    "x minimum: {xmin},\n",
    "f minimum: {fmin},\n",
    "number of iterations: {K},\n",
    "number of one-dimension minimization iterations: {N}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e446c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_next_matrix(A, dw,dx):\n",
    "    y = dx\n",
    "    z = A.dot(dw)\n",
    "    first_part = np.dot(np.transpose([y]), [dx]).dot(np.divide(1, y.dot(dw)))\n",
    "    sec_part =  np.dot(np.transpose([z]), [z]).dot(np.divide(1, z.dot(dw)))\n",
    "    return A - first_part - sec_part\n",
    "\n",
    "def count_next_matrix_2(A, dw,dx):\n",
    "    I = np.identity(len(dx))\n",
    "    r = np.divide(1, np.dot(dx, dw))\n",
    "    C1 = (I - r*(np.dot(np.transpose([dw]), [dx])))\n",
    "    C2 = (I - r*(np.dot(np.transpose([dx]), [dw])))\n",
    "    first_part = C1.dot(A).dot(C2)\n",
    "    sec_part =  r*np.dot(np.transpose([dx]), [dx])\n",
    "    return first_part - sec_part\n",
    "\n",
    "def DFP(f, grad, x, epsilon):\n",
    "    w2 = -grad(x)\n",
    "    phi = toOneParamFunc(f, x, w2)\n",
    "    x1 = x\n",
    "    k = 1\n",
    "    fev, jev = 0, 1\n",
    "    A = np.identity(len(x))\n",
    "    l, f_lin, j_lin = argmin(phi, 0, 1, np.divide(epsilon, 1e3))\n",
    "    fev += f_lin\n",
    "    jev += j_lin\n",
    "    print(x1, f(x1), l, w2)\n",
    "    x2 = x1 + l * w2\n",
    "    while(norm(w2) > epsilon):\n",
    "        w1 = w2\n",
    "        w2 = -grad(x2)\n",
    "        jev += 1\n",
    "        A = count_next_matrix_2(A, w2 - w1, x2 - x1)\n",
    "        p = A.dot(w2)\n",
    "        x1 = x2\n",
    "        phi = toOneParamFunc(f, x2, p)\n",
    "        l, f_lin, j_lin = argmin(phi, 0, 1, np.divide(epsilon, 1e3))\n",
    "        print(x1, f(x1), l, p)\n",
    "        x2 = x1 + l * p\n",
    "        k += 1\n",
    "        n += i\n",
    "    return f(x2), x2, k, n\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "465eb635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_109886/1297883247.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDFP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest_booth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m print(f\"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0mminimum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mf\u001b[0m \u001b[0mminimum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mfmin\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_109886/3178161343.py\u001b[0m in \u001b[0;36mDFP\u001b[0;34m(f, grad, x, epsilon)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mfev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_lin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj_lin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mfev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mf_lin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mjev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mj_lin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "fmin, xmin, K, N = DFP(*test_booth)\n",
    "\n",
    "print(f\"\"\"\n",
    "x minimum: {xmin},\n",
    "f minimum: {fmin},\n",
    "number of iterations: {K},\n",
    "number of one-dimension minimization iterations: {N}\n",
    "\"\"\")\n",
    "\n",
    "print(optimize.minimize(rosenbrok, np.array([-2, 2]), jac=approx_gradient(booth, np.float64(1e-8)), method='BFGS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51638824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
