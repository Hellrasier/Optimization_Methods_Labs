{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9e5005-55c3-4c7e-ab66-a3b3c1417a55",
   "metadata": {},
   "source": [
    "## Newton methods\n",
    "\n",
    "#### Problem:\n",
    "$$\n",
    "f(\\vec{x}) \\rightarrow min,\\\\\n",
    "f: \\Omega \\rightarrow \\mathbb{R}, \\\\\n",
    "\\Omega \\subset \\mathbb{R^n}, f(\\vec{x}) \\mbox{ is convex}, \\\\\n",
    "f(\\vec{x}) \\mbox{ - is twice diffirentiable on } \\Omega\\\\\n",
    "\\vec{x_*} \\in \\Omega, f_{min} = f(\\vec{x_*})\n",
    "$$\n",
    "\n",
    "We can greater efficency of finding $x_*$ if we use information not only about function gradient, but also Hessian $H(\\vec{x})$\n",
    "\n",
    "In simple variant on every k iteration function is approximated in neighborhood of point $\\vec{x}_{k-1}$ by quadratic function $\\phi_{k}(x)$ then $\\vec{x}_k$ is found and the procedure continiues\n",
    "\n",
    "By using Tailor series we can represent our function in neighborhood of point $x_{k}$ as\n",
    "$$\n",
    "f(\\vec{x}) = f(\\vec{x}_{k} + (\\nabla f(\\vec{x}_k), \\vec{x} - \\vec{x}_k) + \\frac{1}{2}(H(\\vec{x}_k)(\\vec{x} - \\vec{x}_k), \\vec{x} - \\vec{x}_k) + o(|\\vec{x} - \\vec{x}_k|)\n",
    "$$\n",
    "\n",
    "So our quadratic approximation $\\phi_k(\\vec{x})$ would be\n",
    "$$\n",
    "\\phi_{k+1}(\\vec{x}) = f(\\vec{x}_{k} + (\\nabla f(\\vec{x}_k), \\vec{x} - \\vec{x}_k) + \\frac{1}{2}(H(\\vec{x}_k)(\\vec{x} - \\vec{x}_k), \\vec{x} - \\vec{x}_k)\n",
    "$$\n",
    "\n",
    "If our $H(\\vec{x}_{k})$ is positive determined (function is convex), then $\\vec{x}_{k+1}$ is single minimum of quadratic approximation and can be found using: \n",
    "$$ \n",
    "\\nabla \\phi_{k+1}(\\vec{x}) = \\nabla f(\\vec{x}_k) + H(\\vec{x}_k)(\\vec{x} - \\vec{x}_k) = \\vec{0}\n",
    "$$\n",
    "\n",
    "Then we get\n",
    "$$\n",
    "\\vec{x}_{k+1} = \\vec{x}_{k}  - H^{-1}(\\vec{x}_{k}) \\nabla f(\\vec{x}_{k})\n",
    "$$\n",
    "\n",
    "If our dimension number $n$ of space $\\mathbb{R}$ is big enough, then finding $H^{-1}$ is very big problem. In this case it expedient to find minimum of $\\phi_k(\\vec{x})$ by using **gradient methods** or **conjugate directions method**\n",
    "$\\widetilde{\\vec{x}}_k = argmin\\{\\phi_{k}(\\vec{x})\\}$ is just a approximation, using this we can build *relaxational sequence* \n",
    "\n",
    "$$\n",
    "\\vec{x}_{k} = \\vec{x}_{k-1} + \\lambda_k(\\widetilde{\\vec{x}}_{k} - \\vec{x}_{k-1}) = \\vec{x}_{k-1} + \\lambda_k\\vec{p}_{k} \\\\ \n",
    "\\vec{p}_k = -H^{-1}(\\vec{x}_{k-1}) \\nabla f(\\vec{x}_{k-1}) \\mbox{ - direction of descent}\n",
    "$$\n",
    "\n",
    "We can find $\\lambda_k$ different ways, for example find $argmin\\{f(\\vec{x}_{k-1} + \\lambda_k\\vec{p}_k\\}$ or by method of step splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc9010-594e-40d8-8e55-7d4bf5d5da6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
