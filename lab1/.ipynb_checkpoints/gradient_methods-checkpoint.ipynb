{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0268044-01de-4c3d-8990-6b05bc30cdae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gradient methods\n",
    "\n",
    "#### Problem:\n",
    "$$\n",
    "f(\\vec{x}) \\rightarrow min,\\\\\n",
    "f: \\Omega \\rightarrow \\mathbb{R}, \\\\\n",
    "\\Omega \\subset \\mathbb{R^n}, f(\\vec{x}) \\mbox{ is convex}, \\\\\n",
    "f(\\vec{x}) \\mbox{ - is diffirentiable on } \\Omega\\\\\n",
    "\\vec{x_*} \\in \\Omega, f_{min} = f(\\vec{x_*})\n",
    "$$\n",
    "\n",
    "<em>**Definition**</em>.\n",
    "\n",
    "Sequnce $\\{\\vec{x_k}\\}$ is named **Relaxational**, if $\\forall k \\in \\mathbb{N}:  f(\\vec{x_k}) < f(\\vec{x}_{k-1})$ \n",
    "\n",
    "$\\{\\vec{x}_l\\}$ convergece to $\\vec{x}_* \\in \\mathbb{R}^n$ by Bolzanoâ€“Weierstrass theorem \n",
    "\n",
    "Let's choose our relaxational sequence by this equation:\n",
    "$$\n",
    "\\vec{x}_k = \\vec{x}_{k-1} + \\beta_k\\vec{u}_k\n",
    "$$\n",
    "where $\\vec{u}_{k}$ is unit vector, which defines the direction of descent and $\\beta_k \\geq 0$ - length of descent step\n",
    "\n",
    "<em>**Lemma**</em>.\n",
    "\n",
    "$f(\\vec{x})$ - is differentiable on $\\Omega \\subset \\mathbb{R}^n$ and $\\exists L > 0$, such that $\\forall \\vec{x}, \\vec{y} \\in \\Omega$:\n",
    "$$\n",
    "||\\nabla f(\\vec{x}) - \\nabla f(\\vec{y})|| \\leq  L ||\\vec{x} = \\vec{y}|| \n",
    "$$\n",
    "Then:\n",
    "$$\n",
    "f(\\vec{x}) - f(\\vec{y}) \\geq (\\nabla f(\\vec{x}), \\vec{x} - \\vec{y}) - \\frac{L}{2}||\\vec{x}-\\vec{y}||^2\n",
    "$$\n",
    "<em>**Definition**</em>.\n",
    "\n",
    "$\\vec{w}(\\vec{x}) = - \\nabla f(\\vec{x})$ is called **antigradient**\n",
    "\n",
    "If we take our $\\vec{u}_k = \\frac{\\vec{w}_k}{||\\vec{w}_k||}$, from our lemma we have, that: \n",
    "\n",
    "$$\n",
    "f(x_{k}) - f(x_{k+1}) \\geq (\\nabla f(x_k), \\vec{x_k} - \\vec{x_k} - \\beta_k \\frac{\\vec{w_k}}{||\\vec{w_k}||}) - \\frac{L}{2} || \\vec{x_k} - \\vec{x_k} - \\beta_k \\frac{\\vec{w_k}}{||\\vec{w_k}||} ||^2 = \\beta_k||\\nabla f(\\vec{x}_k)|| - \\beta_k \\frac{L}{2} \n",
    "$$\n",
    "As we can see gradient must be always posistive (and $> \\frac{L}{2}$),  so that we have a convergece, we get this when function is convex\n",
    "\n",
    "All methods in which $\\vec{u}_k = \\frac{\\vec{w}_k}{||\\vec{w}_k||}$, are named ***gradient methods***, the methods vary on the way we choose our $\\beta_k > 0$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68777a30-a1ca-4e9d-bf2c-ef35d7345a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mplib\n",
    "import math as m\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from onedim_optimize import quadratic_approx, fibbonaci_method, middle_point_method, upgraded_newton, qubic_approx, brent_optimize\n",
    "from scipy.optimize import approx_fprime, minimize\n",
    "import matplotlib.animation as pltanimation\n",
    "from animations import Animate3D\n",
    "\n",
    "from test_functions import *\n",
    "from scipy.misc import derivative\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "def toOneParamFunc(f, x, w):\n",
    "    return lambda p: f(x + p*w) \n",
    "\n",
    "def argmin(f, a, b, eps, onedim_opti):\n",
    "#     fig, ax = plt.subplots()\n",
    "#     ax.plot(np.linspace(a, b, 1000), [f(y) for y in np.linspace(a, b, 1000)])\n",
    "    f_ev, j_ev = 0, 0\n",
    "    x, f_ev = onedim_opti(f, a, b, eps)\n",
    "#     ax.scatter(x, f(x))\n",
    "    return x, f_ev\n",
    "\n",
    "def approx_gradient(f, eps):\n",
    "    return lambda x: approx_fprime(x, f, eps)\n",
    "\n",
    "def optimization_result(title, fmin, xmin, K, f_ev, j_ev, h_ev = None, res=None):\n",
    "    print(f\"\"\"\n",
    "{title}\n",
    "Optimization {res}\n",
    "x minimum: {xmin},\n",
    "f minimum: {fmin},\n",
    "number of iterations: {K},\n",
    "number of function evaluations: {f_ev},\n",
    "number of gradient evaluations: {j_ev},\n",
    "{f\"number of hessian evaluations: {h_ev}\" if h_ev != None else ''}\n",
    "\"\"\") if res == 'succes' else print(f\"\"\"{title}\\nOptimization {res}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a2fc117-d476-4c87-a60f-37dd21fec44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sqrt1 = [\n",
    "    danilov,\n",
    "    danilov_gradient,\n",
    "    np.array([-2, 2]),\n",
    "    0.001,\n",
    "    'Square root func 1 test. Starting point (-2, 2)' \n",
    "]\n",
    "\n",
    "test_sqrt2 = [\n",
    "    danilov,\n",
    "    danilov_gradient,\n",
    "    np.array([4, 3]),\n",
    "    0.001,\n",
    "    'Square root func 1 test. Starting point (4, 3)' \n",
    "]\n",
    "\n",
    "test_rosen1 = [\n",
    "    rosenbrok,\n",
    "    rosen_gradient,\n",
    "    np.array([-2, -1]),\n",
    "    1e-4,\n",
    "    'Rosenbrock1 test. Starting point (-2, -1)'\n",
    "]\n",
    "\n",
    "test_rosen2 = [\n",
    "    rosenbrok,\n",
    "    rosen_gradient,\n",
    "    np.array([-3, 4]),\n",
    "    1e-4,\n",
    "    'Rosenbrock2 test. Starting point (-3, 4)'\n",
    "]\n",
    "\n",
    "test_rosen3 = [\n",
    "    rosenbrok,\n",
    "    rosen_gradient,\n",
    "    np.array([3, 3]),\n",
    "    1e-4,\n",
    "    'Rosenbrock3 test. Starting point (3, 3)'\n",
    "]\n",
    "\n",
    "\n",
    "test_himmel1 = [\n",
    "    himmelblau,\n",
    "    himmel_gradient,\n",
    "    np.array([0, -4]),\n",
    "    1e-4,\n",
    "    'Himmelblau1 test. Starting point (0, -4)'\n",
    "]\n",
    "\n",
    "test_himmel2 = [\n",
    "    himmelblau,\n",
    "    himmel_gradient,\n",
    "    np.array([10, 21]),\n",
    "    1e-4,\n",
    "    'Himmelblau1 test. Starting point (10, 21)'\n",
    "]\n",
    "\n",
    "test_himmel3 = [\n",
    "    himmelblau,\n",
    "    himmel_gradient,\n",
    "    np.array([-5, 17]),\n",
    "    1e-4,\n",
    "    'Himmelblau1 test. Starting point (-5, 17)'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# test_rastrigin = [\n",
    "#     rastrigin,\n",
    "#     approx_gradient(rastrigin, np.float64(1e-8)),\n",
    "#     np.array([2, 1]),\n",
    "#     1e-4\n",
    "# ]\n",
    "\n",
    "# test_ackley = [\n",
    "#     ackley,\n",
    "#     approx_gradient(ackley, np.float64(1e-9)),\n",
    "#     np.array([1, 1]),\n",
    "#     1e-4\n",
    "# ]\n",
    "\n",
    "# test_sphere = [\n",
    "#     sphere,\n",
    "#     approx_gradient(sphere, np.float64(1e-9)),\n",
    "#     np.array([-3, 3]),\n",
    "#     1e-5,\n",
    "#     [[-3, 3], [0, 10]]\n",
    "# ]\n",
    "\n",
    "# test_beale = [\n",
    "#     beale,\n",
    "#     approx_gradient(beale, np.float64(1e-9)),\n",
    "#     np.array([3, 1.5]),\n",
    "#     1e-3,\n",
    "#     [[-0.01, 800], [2.9, 1.6]]\n",
    "# ]\n",
    "\n",
    "# test_goldstein = [\n",
    "#     goldstein_price,np.array([2, 1]),\n",
    "#     approx_gradient(goldstein_price, np.float64(1e-9)),\n",
    "#     np.array([-1.3, 1]),\n",
    "#     1e-5,\n",
    "#     [[-1.5, 1], [0, 50000]]\n",
    "# ]\n",
    "\n",
    "# test_booth = [\n",
    "#     booth,\n",
    "#     approx_gradient(booth, np.float64(1e-8)),\n",
    "#     np.array([5, 3]),\n",
    "#     1e-5,\n",
    "#     [[0, 8], [0, 700]]\n",
    "# ]\n",
    "\n",
    "# test_bukin = [\n",
    "#     bukin,\n",
    "#     approx_gradient(bukin, np.float64(1e-8)),\n",
    "#     np.array([-10.5, 1.5]),\n",
    "#     1e-5\n",
    "# ]\n",
    "\n",
    "# test_himmel = [\n",
    "#     himmelblau,\n",
    "#     approx_gradient(himmelblau, np.float64(1e-8)),\n",
    "#     np.array([0, -4]),\n",
    "#     1e-5,\n",
    "#     [[-4, 4], [-0.1, 280]]\n",
    "# ]\n",
    "\n",
    "# test_egg = [\n",
    "#     eggholder,\n",
    "#     approx_gradient(eggholder, np.float64(1e-8)),\n",
    "#     np.array([353, -200]),\n",
    "#     1e-7\n",
    "# ]\n",
    "\n",
    "# test_cross = [\n",
    "#     cross,\n",
    "#     approx_gradient(cross, np.float64(1e-8)),\n",
    "#     np.array([2, -2]),\n",
    "#     1e-4\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da7e1e-bc40-4eca-96a5-e9add234d8a0",
   "metadata": {},
   "source": [
    "### Fastest descent method\n",
    "\n",
    "We will construct relaxational sequence, using this rule:\n",
    "$$\n",
    "\\vec{x}_{k+1} = \\vec{x}_k + \\lambda_k\\vec{w}_K\n",
    "$$\n",
    "\n",
    "Where $\\lambda_k$ is found from\n",
    "$$\n",
    "\\lambda_k = argmin\\{\\psi_k(\\lambda)\\} \\\\\n",
    "\\psi_k(\\lambda) = f(\\vec{x}_{k-1} + \\lambda\\vec{w}_k)\n",
    "$$\n",
    "\n",
    "Finding minimum of $\\psi_k(\\lambda)$ is a pretty complex task of one-dimension minimization. But it is guaranteed that $\\{|\\vec{w}_k|\\}$ convergace to 0.\n",
    "\n",
    "So at start we pick some small $\\epsilon$ and continuing procedure while $|\\vec{w}_k\\| > \\epsilon$, than on some N iteration we pick our $x_* = x_N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19911d95-3937-4870-a8ac-ef815ece89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastest_descent(f, gr, x, epsilon, title, onedim_opt):\n",
    "    try:\n",
    "        anim = Animate3D(f, x, title)\n",
    "        f_ev = 0\n",
    "        j_ev = 0\n",
    "        w = -gr(x) \n",
    "        phi = toOneParamFunc(f, x, w)\n",
    "        anim.add(x)\n",
    "        l, i = argmin(phi, 0, 40, np.divide(epsilon, 1e5), onedim_opt)\n",
    "        f_ev += i\n",
    "        j_ev += 1\n",
    "        k = 1\n",
    "    #     print(x, f(x), l, norm(w))\n",
    "        x = x + l*w\n",
    "        anim.add(x)\n",
    "        while(norm(w) > epsilon):\n",
    "            w = -gr(x) \n",
    "            phi = toOneParamFunc(f, x, w)\n",
    "            l, i = argmin(phi, 0, 40, np.divide(epsilon, 1e5), onedim_opt)\n",
    "            f_ev += i\n",
    "            j_ev += 1\n",
    "            k += 1\n",
    "    #         print(x, f(x), l, norm(w))\n",
    "            x = x + l*w\n",
    "            anim.add(x)\n",
    "        return f(x), x, k, f_ev, j_ev, anim, 'succes'\n",
    "    except:\n",
    "        return f(x), x, k, f_ev, j_ev, anim, 'fail'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cdbf93e-3bf1-4f62-a866-b2809c946690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Square root func 1 test. Starting point (-2, 2)\n",
      "Optimization succes\n",
      "x minimum: [-0.30155591 -0.60304849],\n",
      "f minimum: 3.3166247937947464,\n",
      "number of iterations: 7,\n",
      "number of function evaluations: 156,\n",
      "number of gradient evaluations: 7,\n",
      "\n",
      "\n",
      "\n",
      "Square root func 1 test. Starting point (4, 3)\n",
      "Optimization succes\n",
      "x minimum: [-0.30152657 -0.60304188],\n",
      "f minimum: 3.316624791052517,\n",
      "number of iterations: 6,\n",
      "number of function evaluations: 113,\n",
      "number of gradient evaluations: 6,\n",
      "\n",
      "\n",
      "\n",
      "Rosenbrock1 test. Starting point (-2, -1)\n",
      "Optimization succes\n",
      "x minimum: [0.99990498 0.99980927],\n",
      "f minimum: 9.076871977019374e-09,\n",
      "number of iterations: 5617,\n",
      "number of function evaluations: 331403,\n",
      "number of gradient evaluations: 5617,\n",
      "\n",
      "\n",
      "\n",
      "Rosenbrock2 test. Starting point (-3, 4)\n",
      "Optimization succes\n",
      "x minimum: [1.00009013 1.00018037],\n",
      "f minimum: 8.123851580709908e-09,\n",
      "number of iterations: 16886,\n",
      "number of function evaluations: 354833,\n",
      "number of gradient evaluations: 16886,\n",
      "\n",
      "\n",
      "\n",
      "Rosenbrock3 test. Starting point (3, 3)\n",
      "Optimization succes\n",
      "x minimum: [1.00010648 1.00021272],\n",
      "f minimum: 1.1345621316891302e-08,\n",
      "number of iterations: 3784,\n",
      "number of function evaluations: 79393,\n",
      "number of gradient evaluations: 3784,\n",
      "\n",
      "\n",
      "\n",
      "Rosenbrock3 test. Starting point (3, 3)\n",
      "Optimization succes\n",
      "x minimum: [-2.80511807  3.13131252],\n",
      "f minimum: 9.422638729609582e-15,\n",
      "number of iterations: 7,\n",
      "number of function evaluations: 130,\n",
      "number of gradient evaluations: 7,\n",
      "\n",
      "\n",
      "\n",
      "Rosenbrock3 test. Starting point (3, 3)\n",
      "Optimization succes\n",
      "x minimum: [-3.77931016 -3.28318598],\n",
      "f minimum: 4.706802181638255e-13,\n",
      "number of iterations: 13,\n",
      "number of function evaluations: 274,\n",
      "number of gradient evaluations: 13,\n",
      "\n",
      "\n",
      "\n",
      "Rosenbrock3 test. Starting point (3, 3)\n",
      "Optimization succes\n",
      "x minimum: [-3.77931051 -3.28318607],\n",
      "f minimum: 3.5560635873723914e-12,\n",
      "number of iterations: 13,\n",
      "number of function evaluations: 247,\n",
      "number of gradient evaluations: 13,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_sqrt1, onedim_opt=brent_optimize)\n",
    "optimization_result(test_sqrt1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=5000).save('examples/Sqrt/Sqrt1-Fastest-Desc.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_sqrt2, onedim_opt=brent_optimize)\n",
    "optimization_result(test_sqrt2[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=5000).save('examples/Sqrt/Sqrt2-Fastest-Desc.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_rosen1, onedim_opt=fibbonaci_method)\n",
    "optimization_result(test_rosen1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Rosenbrock/Rosenbrock1-Fastest-Desc.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_rosen2, onedim_opt=fibbonaci_method)\n",
    "optimization_result(test_rosen2[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=10000).save('examples/Rosenbrock/Rosenbrock2-Fastest-Desc.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_rosen3, onedim_opt=fibbonaci_method)\n",
    "optimization_result(test_rosen3[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Rosenbrock/Rosenbrock3-Fastest-Desc.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_himmel1, onedim_opt=brent_optimize)\n",
    "optimization_result(test_rosen3[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Himmelblau/Himmel1-Fastest-Desc.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_himmel2, onedim_opt=brent_optimize)\n",
    "optimization_result(test_rosen3[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Himmelblau/Himmel2-Fastest-Desc.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_himmel3, onedim_opt=brent_optimize)\n",
    "optimization_result(test_rosen3[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Himmelblau/Himmel3-Fastest-Desc.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2c72e-20bd-4ee9-a5e4-520d68c55139",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conjugate gradient method\n",
    "\n",
    "#### Problem \n",
    "\n",
    "$$\n",
    "f(\\vec{x}) = \\frac{1}{2}(Q\\vec{x}, \\vec{x}) + (\\vec{c}, \\vec{x}) \\rightarrow min\n",
    "$$\n",
    "\n",
    "$Q$ is positive determined n-dimsensional matrix, $c \\in \\mathbb{R}$ - constant\n",
    "\n",
    "This function has single point of minimum $x_* = -Q^{-1}\\vec{c}$\n",
    "\n",
    "To find the inverted matrix $Q^{-1}$ we can use\n",
    "$$\n",
    "Q^{-1} = \\sum^n_{i=1}\\frac{p^i(p^i)^T}{(Qp^i, p^i)}\n",
    "$$\n",
    "Where $p^i \\in \\mathbb{R}$ is conjugate vector of matrix $Q$\n",
    "\n",
    "But constructing a system of conjugate vectors is a pretty complex problem.\n",
    "\n",
    "So we do another way, let's construct system of conjugate vectors on every iteration\n",
    "\n",
    "$\\vec{x}_0$ is a starting point, antrigradient in this point is $\\vec{w}_1 = -Qx_0 - c$ and let's choose $\\vec{p}_1 = \\vec{w}$\n",
    "\n",
    "Using $\\vec{x}_k = \\vec{x}_{k-1} + \\lambda_k\\vec{w}_k$\n",
    "\n",
    "We can find that \n",
    "$$\\lambda_1 = \\frac{|\\vec{w}_1|^2}{(Q\\vec{w}_1, \\vec{w}_1)} = \\frac{|\\vec{p}_1|^2}{(Q\\vec{p}_1, \\vec{p}_1)}$$\n",
    "(from minimization of quadratic function)\n",
    "\n",
    "And so $x_1 = x_0 + \\lambda_1\\vec{p}_1$\n",
    "\n",
    "On second iteration (k = 2) we evaluate antigradient $\\vec{w}_2 = -Q\\vec{x_1} - c$\n",
    "\n",
    "Let's assume, that\n",
    "$$\\vec{p}_2 = \\gamma_1\\vec{p}_1 + \\vec{w}_2$$\n",
    "\n",
    "If we product scalarly this equation on $Q\\vec{p}_1 \\not = 0$ and demand that $\\vec{p}_1, \\vec{p}_2$ are conjugate (ortogonal) over the matrix $Q$ ($(Q\\vec{p}_1, \\vec{p_2}) = 0$), we can find $\\gamma_1$\n",
    "$$\\gamma_1 = -\\frac{(Q\\vec{p}_1, \\vec{w}_2)}{(Q\\vec{p}_1, \\vec{p}_1)}$$\n",
    "\n",
    "Contniuing constructing this system of conjugate vectors, we can say, that on every k iteration we have system of equations:\n",
    "$$\n",
    "\\begin{cases}\n",
    "    p_{k+1} = \\gamma\\vec{p_k} + \\vec{w}_{k+1} \\\\\n",
    "    \\gamma_k = - \\frac{(Q\\vec{p}_k, \\vec{w}_{k+1})}{(Q\\vec{p}_k, \\vec{p}_k)} \\\\\n",
    "    \\vec{w}_{k+1} = \\vec{w}_k = \\lambda_kQ\\vec{p}_k \\\\\n",
    "    (Q\\vec{p}_{k+1}, \\vec{p}_i) = 0 \\\\\n",
    "    (\\vec{w}_{k+1}, \\vec{w}_i) = 0, i = \\overline{1, k} \\\\\n",
    "\\end{cases} \\\\\n",
    "\\mbox{also } \\\\\n",
    "\\lambda_k = \\frac{(\\vec{w}_k, \\vec{p}_k)}{(Q\\vec{p}_k, \\vec{p}_k)},\\\\\n",
    "\\vec{x}_k = \\vec{x_1} + \\lambda_k\\vec{p}_k\n",
    "$$\n",
    "\n",
    "With n steps we can find all $\\vec{p}_k$ conjugate vectors and evaluate our minimum $x_* = -Q^{-1}\\vec{c}$\n",
    "\n",
    "To use this method in our problems (non-quadratic function optimization, we need to remove matrix $Q$ from system of equations\n",
    "\n",
    "We can do this, by if on every iteration by doing minimization process:\n",
    "$$\n",
    "\\psi_k(\\lambda) = f(x_{k-1} + \\lambda)\n",
    "$$\n",
    "\n",
    "In fundament of constructing conjuguate directions $\\vec{p}_{k+1} = \\gamma_k\\vec{p}_k + \\vec{w}_{k+1}$ we assume, that $(\\vec{w}_{k+1}, \\vec{w}_i) = 0$\n",
    "\n",
    "Using this we can show that:\n",
    "$$\n",
    "\\begin{cases}\n",
    "    (Q\\vec{p}_k, \\vec{w}_{k+1}) = - \\frac{1}{\\lambda_k}|\\vec{w}_{k+1}|^2 \\\\\n",
    "    (Q\\vec{p}_k, \\vec{p}_{k}) = \\frac{1}{\\lambda_k}(\\vec{w}_k, \\vec{p}_k)\n",
    "\\end{cases} \\\\\n",
    "\\mbox{so from our system of equations we can evaluate $\\gamma$ using one of theese formulas: } \\\\\n",
    "\\gamma_k = \\frac{|\\vec{w}_{k+1}|^2}{|\\vec{w}_k|^2} \\\\\n",
    "\\gamma_k = \\frac{(\\vec{w}_{k+1} - \\vec{w}_k, \\vec{w}_{k+1})}{|\\vec{w}_k|^2} \\\\\n",
    "\\mbox{also if function twice differentiable, we can use Hessian instead of matrix Q:} \\\\\n",
    "\\gamma_k = - \\frac{(H(\\vec{x}_k)\\vec{p}_k, \\vec{w}_{k+1})}{(H(\\vec{x}_k)\\vec{p}_k, \\vec{p}_k)} \\\\\n",
    "$$\n",
    "\n",
    "This method is called ***conjaguate gradients method***\n",
    "\n",
    "Also as every $\\gamma_k$ is different and we need to minimize $\\psi_k(\\lambda)$ this turns us to inevitably errors, to minimize errors, we need to do **restarts** (set $\\gamma_k = 0$). It is common to restart every $n$ times, where $n$ is our dimension number. Also, with non-quadratic functions our procedure of optimization in general don't take $n$ steps, so we choose our $\\epsilon$ and iterate through $\\{\\vec{x}_k\\}$ till our |$\\vec{w}_{k+1|} < \\epsilon$, and then $x_{k-1} \\approx x_*$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "872bd742-b7b8-477b-8406-82ab2cb9ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(f, gr, x, epsilon, title, onedim_opt):\n",
    "    try:\n",
    "        anim = Animate3D(f, x, title)\n",
    "        w = -gr(x) \n",
    "        p = w\n",
    "        j_ev = 1\n",
    "        f_ev = 0\n",
    "        phi = toOneParamFunc(f, x, p)\n",
    "        l, i = argmin(phi, 0, 500, np.divide(epsilon, 1e3), onedim_opt)\n",
    "        f_ev += i\n",
    "    #     print(x, f(x), l, p)\n",
    "        x = x + l*p\n",
    "        anim.add(x)\n",
    "        j_ev += 1\n",
    "        k = 1\n",
    "        while norm(w) > epsilon:\n",
    "            w_2 = -gr(x)\n",
    "            gamma = np.divide(np.dot(w_2 - w, w_2), np.power(norm(w), 2))\n",
    "            p = gamma*p + w_2\n",
    "            phi = toOneParamFunc(f, x, p)\n",
    "            l, i = argmin(phi, 0, 500, np.divide(epsilon, 1e3), onedim_opt) \n",
    "    #         print(x, f(x), l, p)\n",
    "            x = x + l*p\n",
    "            anim.add(x)\n",
    "            w = w_2\n",
    "            j_ev += 1\n",
    "            f_ev += i\n",
    "            k += 1\n",
    "        return f(x), x, k+1, f_ev, j_ev, anim, 'succes'\n",
    "    except:\n",
    "        return f(x), x, k+1, f_ev, j_ev, anim, 'fail'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6073a2-9bea-47cc-ba89-1b8db5589627",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Square root func 1 test. Starting point (-2, 2)\n",
      "Optimization succes\n",
      "x minimum: [-0.30151058 -0.6030228 ],\n",
      "f minimum: 3.3166247903563653,\n",
      "number of iterations: 6,\n",
      "number of function evaluations: 91,\n",
      "number of gradient evaluations: 6,\n",
      "\n",
      "\n",
      "\n",
      "Square root func 1 test. Starting point (4, 3)\n",
      "Optimization succes\n",
      "x minimum: [-0.30151136 -0.60302269],\n",
      "f minimum: 3.3166247903554,\n",
      "number of iterations: 7,\n",
      "number of function evaluations: 136,\n",
      "number of gradient evaluations: 7,\n",
      "\n",
      "\n",
      "\n",
      "Rosenbrock1 test. Starting point (-2, -1)\n",
      "Optimization succes\n",
      "x minimum: [1. 1.],\n",
      "f minimum: 2.815130258966867e-24,\n",
      "number of iterations: 18,\n",
      "number of function evaluations: 356,\n",
      "number of gradient evaluations: 18,\n",
      "\n",
      "\n",
      "\n",
      "Rosenbrock2 test. Starting point (-3, 4)\n",
      "Optimization succes\n",
      "x minimum: [1. 1.],\n",
      "f minimum: 8.542910501561122e-25,\n",
      "number of iterations: 14,\n",
      "number of function evaluations: 305,\n",
      "number of gradient evaluations: 14,\n",
      "\n",
      "\n",
      "\n",
      "Rosenbrock3 test. Starting point (3, 3)\n",
      "Optimization succes\n",
      "x minimum: [1. 1.],\n",
      "f minimum: 3.1713530224938986e-21,\n",
      "number of iterations: 26,\n",
      "number of function evaluations: 522,\n",
      "number of gradient evaluations: 26,\n",
      "\n",
      "\n",
      "\n",
      "Himmelblau1 test. Starting point (0, -4)\n",
      "Optimization succes\n",
      "x minimum: [-2.80511809  3.13131252],\n",
      "f minimum: 4.585153668489987e-23,\n",
      "number of iterations: 7,\n",
      "number of function evaluations: 115,\n",
      "number of gradient evaluations: 7,\n",
      "\n",
      "\n",
      "\n",
      "Himmelblau1 test. Starting point (10, 21)\n",
      "Optimization succes\n",
      "x minimum: [-3.77931025 -3.28318599],\n",
      "f minimum: 4.1836114770467326e-23,\n",
      "number of iterations: 8,\n",
      "number of function evaluations: 137,\n",
      "number of gradient evaluations: 8,\n",
      "\n",
      "\n",
      "\n",
      "Himmelblau1 test. Starting point (-5, 17)\n",
      "Optimization succes\n",
      "x minimum: [-3.77931025 -3.28318599],\n",
      "f minimum: 1.153209283170618e-22,\n",
      "number of iterations: 7,\n",
      "number of function evaluations: 114,\n",
      "number of gradient evaluations: 7,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_sqrt1, onedim_opt=brent_optimize)\n",
    "optimization_result(test_sqrt1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=5000).save('examples/Sqrt/Sqrt1-Conj-grad.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_sqrt2, onedim_opt=brent_optimize)\n",
    "optimization_result(test_sqrt2[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=5000).save('examples/Sqrt/Sqrt2-Conj-grad.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_rosen1, onedim_opt=brent_optimize)\n",
    "optimization_result(test_rosen1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=10000).save('examples/Rosenbrock/Rosenbrock1-Conj-grad.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_rosen2, onedim_opt=brent_optimize)\n",
    "optimization_result(test_rosen2[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=10000).save('examples/Rosenbrock/Rosenbrock2-Conj-grad.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_rosen3, onedim_opt=brent_optimize)\n",
    "optimization_result(test_rosen3[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=10000).save('examples/Rosenbrock/Rosenbrock3-Conj-grad.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_himmel1, onedim_opt=brent_optimize)\n",
    "optimization_result(test_himmel1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Himmelblau/Himmel1-Conj-grad.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_himmel2, onedim_opt=brent_optimize)\n",
    "optimization_result(test_himmel2[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Himmelblau/Himmel2-Conj-grad.gif')\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_himmel3, onedim_opt=brent_optimize)\n",
    "optimization_result(test_himmel3[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# a = anim.get_animation(duration=8000).save('examples/Himmelblau/Himmel3-Conj-grad.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70df562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
