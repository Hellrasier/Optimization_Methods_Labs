{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9e5005-55c3-4c7e-ab66-a3b3c1417a55",
   "metadata": {},
   "source": [
    "## Newton methods\n",
    "\n",
    "#### Problem:\n",
    "$$\n",
    "f(\\vec{x}) \\rightarrow min,\\\\\n",
    "f: \\Omega \\rightarrow \\mathbb{R}, \\\\\n",
    "\\Omega \\subset \\mathbb{R^n}, f(\\vec{x}) \\mbox{ is convex}, \\\\\n",
    "f(\\vec{x}) \\mbox{ - is twice diffirentiable on } \\Omega\\\\\n",
    "\\vec{x_*} \\in \\Omega, f_{min} = f(\\vec{x_*})\n",
    "$$\n",
    "\n",
    "We can greater efficency of finding $x_*$ if we use information not only about function gradient, but also Hessian $H(\\vec{x})$\n",
    "\n",
    "In simple variant on every k iteration function is approximated in neighborhood of point $\\vec{x}_{k-1}$ by quadratic function $\\phi_{k}(x)$ then $\\vec{x}_k$ is found and the procedure continiues\n",
    "\n",
    "By using Tailor series we can represent our function in neighborhood of point $x_{k}$ as\n",
    "$$\n",
    "f(\\vec{x}) = f(\\vec{x}_{k} + (\\nabla f(\\vec{x}_k), \\vec{x} - \\vec{x}_k) + \\frac{1}{2}(H(\\vec{x}_k)(\\vec{x} - \\vec{x}_k), \\vec{x} - \\vec{x}_k) + o(|\\vec{x} - \\vec{x}_k|)\n",
    "$$\n",
    "\n",
    "So our quadratic approximation $\\phi_k(\\vec{x})$ would be\n",
    "$$\n",
    "\\phi_{k+1}(\\vec{x}) = f(\\vec{x}_{k} + (\\nabla f(\\vec{x}_k), \\vec{x} - \\vec{x}_k) + \\frac{1}{2}(H(\\vec{x}_k)(\\vec{x} - \\vec{x}_k), \\vec{x} - \\vec{x}_k)\n",
    "$$\n",
    "\n",
    "If our $H(\\vec{x}_{k})$ is positive determined (function is convex), then $\\vec{x}_{k+1}$ is single minimum of quadratic approximation and can be found using: \n",
    "$$ \n",
    "\\nabla \\phi_{k+1}(\\vec{x}) = \\nabla f(\\vec{x}_k) + H(\\vec{x}_k)(\\vec{x} - \\vec{x}_k) = \\vec{0}\n",
    "$$\n",
    "\n",
    "Then we get\n",
    "$$\n",
    "\\vec{x}_{k+1} = \\vec{x}_{k}  - H^{-1}(\\vec{x}_{k}) \\nabla f(\\vec{x}_{k})\n",
    "$$\n",
    "\n",
    "If our dimension number $n$ of space $\\mathbb{R}$ is big enough, then finding $H^{-1}$ is very big problem. In this case it expedient to find minimum of $\\phi_k(\\vec{x})$ by using **gradient methods** or **conjugate directions method**\n",
    "$\\widetilde{\\vec{x}}_k = argmin\\{\\phi_{k}(\\vec{x})\\}$ is just an approximation, using this we can build *relaxational sequence* \n",
    "\n",
    "$$\n",
    "\\vec{x}_{k} = \\vec{x}_{k-1} + \\lambda_k(\\widetilde{\\vec{x}}_{k} - \\vec{x}_{k-1}) = \\vec{x}_{k-1} + \\lambda_k\\vec{p}_{k} \\\\ \n",
    "\\vec{p}_k = -H^{-1}(\\vec{x}_{k-1}) \\nabla f(\\vec{x}_{k-1}) \\mbox{ - direction of descent}\n",
    "$$\n",
    "\n",
    "We can find $\\lambda_k$ different ways, for example find $argmin\\{f(\\vec{x}_{k-1} + \\lambda_k\\vec{p}_k\\}$ or by method of step splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3abc9010-594e-40d8-8e55-7d4bf5d5da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mplib\n",
    "import math as m\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from onedim_optimize import newton_method\n",
    "from scipy.optimize import approx_fprime\n",
    "\n",
    "def toOneParamFunc(f, x, w):\n",
    "    return lambda p: f(x + p*w) \n",
    "\n",
    "def argmin(f, x, eps):\n",
    "    f_x, xmin, k = newton_method(f, x, eps)\n",
    "    return xmin, k\n",
    "\n",
    "def approx_gradient(f, eps):\n",
    "    return lambda x: approx_fprime(x, f, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f50293-c0c6-45db-8e43-4fd79da71fcc",
   "metadata": {},
   "source": [
    "### Newton method\n",
    "\n",
    "The common Newton method is to find our $H^{-1}$ matrix and than build relaxetion sequence by this rule:\n",
    "$$\n",
    "\\vec{x}_{k+1} = \\vec{x}_{k}  - H^{-1}(\\vec{x}_{k}) \\nabla f(\\vec{x}_{k})\n",
    "$$\n",
    "\n",
    "But there is a problem with matrix $H$, it needs to be always positive determinated or $H^{-1}$ won't exist.\n",
    "\n",
    "To solve this problem, let's check if $H$ is positive determinated, if not, then let's pick $\\eta$_k, such that:\n",
    "$$\n",
    "\\widetilde{H}_k = \\eta_kI_n + H(\\vec{x}_{k-1})\n",
    "$$\n",
    "$\\widetilde{H}$ is positive determinated matrix, that we pick instead of $H$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be4fcc8-0942-462f-90c1-251d7490210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def common_newton(f, gr, x, epsilon):\n",
    "    w = -gr(x)\n",
    "    hessian = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
