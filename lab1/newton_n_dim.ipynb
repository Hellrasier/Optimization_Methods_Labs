{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f9e5005-55c3-4c7e-ab66-a3b3c1417a55",
   "metadata": {},
   "source": [
    "## Newton methods\n",
    "\n",
    "#### Problem:\n",
    "$$\n",
    "f(\\vec{x}) \\rightarrow min,\\\\\n",
    "f: \\Omega \\rightarrow \\mathbb{R}, \\\\\n",
    "\\Omega \\subset \\mathbb{R^n}, f(\\vec{x}) \\mbox{ is convex}, \\\\\n",
    "f(\\vec{x}) \\mbox{ - is twice diffirentiable on } \\Omega\\\\\n",
    "\\vec{x_*} \\in \\Omega, f_{min} = f(\\vec{x_*})\n",
    "$$\n",
    "\n",
    "We can greater efficency of finding $x_*$ if we use information not only about function gradient, but also Hessian $H(\\vec{x})$\n",
    "\n",
    "In simple variant on every k iteration function is approximated in neighborhood of point $\\vec{x}_{k-1}$ by quadratic function $\\phi_{k}(x)$ then $\\vec{x}_k$ is found and the procedure continiues\n",
    "\n",
    "By using Tailor series we can represent our function in neighborhood of point $x_{k}$ as\n",
    "$$\n",
    "f(\\vec{x}) = f(\\vec{x}_{k} + (\\nabla f(\\vec{x}_k), \\vec{x} - \\vec{x}_k) + \\frac{1}{2}(H(\\vec{x}_k)(\\vec{x} - \\vec{x}_k), \\vec{x} - \\vec{x}_k) + o(|\\vec{x} - \\vec{x}_k|)\n",
    "$$\n",
    "\n",
    "So our quadratic approximation $\\phi_k(\\vec{x})$ would be\n",
    "$$\n",
    "\\phi_{k+1}(\\vec{x}) = f(\\vec{x}_{k} + (\\nabla f(\\vec{x}_k), \\vec{x} - \\vec{x}_k) + \\frac{1}{2}(H(\\vec{x}_k)(\\vec{x} - \\vec{x}_k), \\vec{x} - \\vec{x}_k)\n",
    "$$\n",
    "\n",
    "If our $H(\\vec{x}_{k})$ is positive determined (function is convex), then $\\vec{x}_{k+1}$ is single minimum of quadratic approximation and can be found using: \n",
    "$$ \n",
    "\\nabla \\phi_{k+1}(\\vec{x}) = \\nabla f(\\vec{x}_k) + H(\\vec{x}_k)(\\vec{x} - \\vec{x}_k) = \\vec{0}\n",
    "$$\n",
    "\n",
    "Then we get\n",
    "$$\n",
    "\\vec{x}_{k+1} = \\vec{x}_{k}  - H^{-1}(\\vec{x}_{k}) \\nabla f(\\vec{x}_{k})\n",
    "$$\n",
    "\n",
    "If our dimension number $n$ of space $\\mathbb{R}$ is big enough, then finding $H^{-1}$ is very big problem. In this case it expedient to find minimum of $\\phi_k(\\vec{x})$ by using **gradient methods** or **conjugate directions method**\n",
    "$\\widetilde{\\vec{x}}_k = argmin\\{\\phi_{k}(\\vec{x})\\}$ is just an approximation, using this we can build *relaxational sequence* \n",
    "\n",
    "$$\n",
    "\\vec{x}_{k} = \\vec{x}_{k-1} + \\lambda_k(\\widetilde{\\vec{x}}_{k} - \\vec{x}_{k-1}) = \\vec{x}_{k-1} + \\lambda_k\\vec{p}_{k} \\\\ \n",
    "\\vec{p}_k = -H^{-1}(\\vec{x}_{k-1}) \\nabla f(\\vec{x}_{k-1}) \\mbox{ - direction of descent}\n",
    "$$\n",
    "\n",
    "We can find $\\lambda_k$ different ways, for example find $argmin\\{f(\\vec{x}_{k-1} + \\lambda_k\\vec{p}_k\\}$ or by method of step splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3abc9010-594e-40d8-8e55-7d4bf5d5da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mplib\n",
    "import math as m\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy import linalg\n",
    "from scipy import sparse\n",
    "\n",
    "from onedim_optimize import quadratic_approx, newton_method, fibbonaci_method, middle_point_method, newton_modified\n",
    "from scipy.optimize import approx_fprime\n",
    "\n",
    "def toOneParamFunc(f, x, w):\n",
    "    return lambda p: f(x + p*w) \n",
    "\n",
    "def argmin(f, a, b, eps):\n",
    "    fmin, xmin, k = middle_point_method(f, a, b, eps)\n",
    "    return xmin, k\n",
    "\n",
    "def approx_gradient(f, eps):\n",
    "    return lambda x: approx_fprime(x, f, eps)\n",
    "\n",
    "def partial_der(f_dx, f_x, dx, j):\n",
    "    p = np.divide(f_dx - f_x, dx)\n",
    "    return p\n",
    "\n",
    "def hessian_in_point(x, f, grad, eps):\n",
    "     gr = grad(x)\n",
    "     n = len(gr) \n",
    "     hes = []\n",
    "     for i in range (0, n):\n",
    "        x_delta = np.array(x[:])\n",
    "        x_delta[i] = x_delta[i] + eps\n",
    "        gr_delta = grad(x_delta)\n",
    "        partials = np.array([partial_der(gr_delta[j], part, eps, j) for j,part in enumerate(gr)])\n",
    "        hes.append(partials)\n",
    "     return np.array(hes)\n",
    "\n",
    "def hessian(f, grad, eps):\n",
    "    return lambda x: hessian_in_point(x, f, grad, eps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1a1a12b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.54010138 -0.76916251]\n",
      " [-0.76916251  1.53832502]]\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: np.float64(x[0] + 2*x[1] + 4*m.sqrt(1 + x[0]**2 + x[1]**2))\n",
    "eps = np.float64(1e-6)\n",
    "x = np.array([1.0, 1.0])\n",
    "\n",
    "hes = hessian(f, approx_gradient(f, eps), eps)\n",
    "print(hes(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f50293-c0c6-45db-8e43-4fd79da71fcc",
   "metadata": {},
   "source": [
    "### Newton method\n",
    "\n",
    "The common Newton method is to find our $H^{-1}$ matrix and than build relaxetion sequence by this rule:\n",
    "$$\n",
    "\\vec{x}_{k+1} = \\vec{x}_{k}  - H^{-1}(\\vec{x}_{k}) \\nabla f(\\vec{x}_{k})\n",
    "$$\n",
    "\n",
    "But there is a problem with matrix $H$, it needs to be always positive determinated or $H^{-1}$ won't exist.\n",
    "\n",
    "To solve this problem, let's check if $H$ is positive determinated, if not, then let's pick $\\eta$_k, such that:\n",
    "$$\n",
    "\\widetilde{H}_k = \\eta_kI_n + H(\\vec{x}_{k-1})\n",
    "$$\n",
    "$\\widetilde{H}$ is positive determinated matrix, that we pick instead of $H$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9be4fcc8-0942-462f-90c1-251d7490210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_newton(f, gr, hess, x, epsilon):\n",
    "    w = -gr(x)\n",
    "    k = 0\n",
    "    while(norm(w) > epsilon):\n",
    "        H = hess(x)\n",
    "        print(x, w)\n",
    "        print(H)\n",
    "        h = linalg.solve(H, w)\n",
    "        x = x + h\n",
    "        w = -gr(x)\n",
    "        k += 1\n",
    "    return f(x), x, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44050517",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -2] [15.99999982  6.        ]\n",
      "[[13999994.0009634   2000000.00133227]\n",
      " [ 3999998.00026629  2000000.00088818]]\n",
      "[-0.999999 -1.999999] [15.99997424  5.99999414]\n",
      "[[21.9984031   4.0003556 ]\n",
      " [ 4.0003556   2.00373051]]\n",
      "[-0.71300944  0.4214513 ] [3.67394975 0.1738623 ]\n",
      "[[6.41531273 2.85194091]\n",
      " [2.85194091 2.0001778 ]]\n",
      "[ 0.74557234 -1.57133491] [-5.83510961  4.25442608]\n",
      "[[14.95337187 -2.98427949]\n",
      " [-2.98427949  2.0001778 ]]\n",
      "[0.79438043 0.62851106] [0.40320246 0.00505842]\n",
      "[[ 7.05846492 -3.17752769]\n",
      " [-3.17752769  1.99999045]]\n",
      "[0.99896687 0.95608136] [-0.16517459  0.08370687]\n",
      "[[10.15092177 -3.99586961]\n",
      " [-3.99586961  1.99999999]]\n",
      "\n",
      "x minimum: [0.9999202  0.99983949],\n",
      "f minimum: 6.368974403837277e-09,\n",
      "number of iterations: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f2 = lambda x: (x[0]**2 - x[1])**2 + (x[0] - 1)**2\n",
    "danilov = lambda x: x[0] + 2*x[1] + 4*m.sqrt(1 + x[0]**2 + x[1]**2)\n",
    "\n",
    "test2 = [\n",
    "    f2,\n",
    "    approx_gradient(f2, 1e-8),\n",
    "    hessian(f2, approx_gradient(f2, 1e-6), 1e-6),\n",
    "    np.array([-1, -2]),\n",
    "    0.001,\n",
    "]\n",
    "\n",
    "test_danilov = [\n",
    "    danilov,\n",
    "    approx_gradient(danilov, np.float64(1e-8)),\n",
    "    hessian(danilov, approx_gradient(danilov, np.float64(1e-6)), np.float64(1e-6)),\n",
    "    np.array([-2, -1]),\n",
    "    0.01,\n",
    "]\n",
    "\n",
    "fmin, xmin, K = common_newton(*test2)\n",
    "print(f\"\"\"\n",
    "x minimum: {xmin},\n",
    "f minimum: {fmin},\n",
    "number of iterations: {K}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1e07ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_upgraded(f, gr, hess, x, epsilon):\n",
    "    w = -gr(x)\n",
    "    phi = toOneParamFunc(f, x, w)\n",
    "    k = 0\n",
    "    n = 0\n",
    "    while(norm(w) > epsilon):\n",
    "        H = hess(x)\n",
    "        print(x, w)\n",
    "        print(H)\n",
    "        h = linalg.solve(H, w)\n",
    "        phi = toOneParamFunc(f, x, h)\n",
    "        l, i = argmin(phi, 0, 1, epsilon) \n",
    "        n += i\n",
    "        x = x + l * h\n",
    "        w = -gr(x)\n",
    "        k += 1\n",
    "    return f(x), x, k, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea0fb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -2] [15.99999982  6.        ]\n",
      "[[13999994.0009634   2000000.00133227]\n",
      " [ 3999998.00026629  2000000.00088818]]\n",
      "\n",
      "x minimum: [nan nan],\n",
      "f minimum: nan,\n",
      "number of iterations: 1,\n",
      "number of one-dimension minimization iterations: 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_231940/1391528290.py:1: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f2 = lambda x: (x[0]**2 - x[1])**2 + (x[0] - 1)**2\n"
     ]
    }
   ],
   "source": [
    "f2 = lambda x: (x[0]**2 - x[1])**2 + (x[0] - 1)**2\n",
    "danilov = lambda x: x[0] + 2*x[1] + 4*m.sqrt(1 + x[0]**2 + x[1]**2)\n",
    "\n",
    "test2 = [\n",
    "    f2,\n",
    "    approx_gradient(f2, np.float64(1e-8)),\n",
    "    hessian(f2, approx_gradient(f2, np.float64(1e-6)), np.float64(1e-6)),\n",
    "    np.array([-1, -2]),\n",
    "    np.float64(1e-3),\n",
    "]\n",
    "\n",
    "test_danilov = [\n",
    "    danilov,\n",
    "    approx_gradient(danilov, np.float64(1e-8)),\n",
    "    hessian(danilov, approx_gradient(danilov, np.float64(1e-8)), np.float64(1e-8)),\n",
    "    np.array([-1, -2]),\n",
    "    0.01,\n",
    "]\n",
    "\n",
    "fmin, xmin, K, N = newton_upgraded(*test2)\n",
    "print(f\"\"\"\n",
    "x minimum: {xmin},\n",
    "f minimum: {fmin},\n",
    "number of iterations: {K},\n",
    "number of one-dimension minimization iterations: {N}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e446c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_next_matrix(A, dw,dx):\n",
    "    y = dx\n",
    "    z = A.dot(dw)\n",
    "    first_part = np.dot(np.transpose([y]), [dx]) * np.divide(1, dw.dot(y))\n",
    "    sec_part =   np.dot(np.transpose([z]), [A.dot(dw)]) * np.divide(1, dw.dot(z))\n",
    "    return A - first_part - sec_part\n",
    "\n",
    "def DFP(f, grad, x, epsilon):\n",
    "    w2 = -grad(x)\n",
    "    phi = toOneParamFunc(f, x, w2)\n",
    "    x1 = x\n",
    "    k = 1\n",
    "    n = 0\n",
    "    A = np.identity(len(x))\n",
    "    l, i = argmin(phi, 0, 1, epsilon)\n",
    "    n += i\n",
    "    x2 = x1 + l * w2\n",
    "    print(A)\n",
    "    while(norm(w2) > epsilon):\n",
    "        w1 = w2\n",
    "        w2 = -grad(x2)\n",
    "#         if k % len(x) != 0 :\n",
    "        print(A, x2)\n",
    "        A = count_next_matrix(A, w2 - w1, x2 - x1)\n",
    "#         else:\n",
    "#             A = np.identity(len(x))\n",
    "        p = A.dot(w2)\n",
    "        x1 = x2\n",
    "        phi = toOneParamFunc(f, x2, p)\n",
    "        l, i = argmin(phi, 0, 1, epsilon)\n",
    "        x2 = x1 + l * p\n",
    "        k += 1\n",
    "        n += i\n",
    "    return f(x), x, k, n\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "465eb635",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'float' and 'function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_231940/584128003.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m ]\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mfmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDFP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m print(f\"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0mx\u001b[0m \u001b[0mminimum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_231940/3699321521.py\u001b[0m in \u001b[0;36mDFP\u001b[0;34m(f, grad, x, epsilon)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mDFP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoOneParamFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_231940/3804438342.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapprox_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmiddle_point_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpartial_der\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_dx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/repos/dzshki/optmehods/lab1/onedim_optimize.py\u001b[0m in \u001b[0;36mmiddle_point_method\u001b[0;34m(f, a, b, eps)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0miteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mderivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/source/repos/dzshki/optmehods/lab1/onedim_optimize.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmiddle_point_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mcondition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'float' and 'function'"
     ]
    }
   ],
   "source": [
    "f2 = lambda x: (x[0]**2 - x[1])**2 + (x[0] - 1)**2\n",
    "danilov = lambda x: x[0] + 2*x[1] + 4*m.sqrt(1 + x[0]**2 + x[1]**2)\n",
    "\n",
    "test2 = [\n",
    "    f2,\n",
    "    approx_gradient(f2, np.float64(1e-8)),\n",
    "    np.array([-1, -2]),\n",
    "    np.float64(0.001),\n",
    "]\n",
    "\n",
    "test_danilov = [\n",
    "    danilov,\n",
    "    approx_gradient(danilov, np.float64(1e-8)),\n",
    "    np.array([-1, -2]),\n",
    "    0.01,\n",
    "]\n",
    "\n",
    "fmin, xmin, K, N = DFP(*test2)\n",
    "print(f\"\"\"\n",
    "x minimum: {xmin},\n",
    "f minimum: {fmin},\n",
    "number of iterations: {K},\n",
    "number of one-dimension minimization iterations: {N}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51638824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
