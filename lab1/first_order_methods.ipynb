{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0268044-01de-4c3d-8990-6b05bc30cdae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gradient methods\n",
    "\n",
    "#### Problem:\n",
    "$$\n",
    "f(\\vec{x}) \\rightarrow min,\\\\\n",
    "f: \\Omega \\rightarrow \\mathbb{R}, \\\\\n",
    "\\Omega \\subset \\mathbb{R^n}, f(\\vec{x}) \\mbox{ is convex}, \\\\\n",
    "f(\\vec{x}) \\mbox{ - is diffirentiable on } \\Omega\\\\\n",
    "\\vec{x_*} \\in \\Omega, f_{min} = f(\\vec{x_*})\n",
    "$$\n",
    "\n",
    "<em>**Definition**</em>.\n",
    "\n",
    "Sequnce $\\{\\vec{x_k}\\}$ is named **Relaxational**, if $\\forall k \\in \\mathbb{N}:  f(\\vec{x_k}) < f(\\vec{x}_{k-1})$ \n",
    "\n",
    "$\\{\\vec{x}_l\\}$ convergece to $\\vec{x}_* \\in \\mathbb{R}^n$ by Bolzanoâ€“Weierstrass theorem \n",
    "\n",
    "Let's choose our relaxational sequence by this equation:\n",
    "$$\n",
    "\\vec{x}_k = \\vec{x}_{k-1} + \\beta_k\\vec{u}_k\n",
    "$$\n",
    "where $\\vec{u}_{k}$ is unit vector, which defines the direction of descent and $\\beta_k \\geq 0$ - length of descent step\n",
    "\n",
    "<em>**Lemma**</em>.\n",
    "\n",
    "$f(\\vec{x})$ - is differentiable on $\\Omega \\subset \\mathbb{R}^n$ and $\\exists L > 0$, such that $\\forall \\vec{x}, \\vec{y} \\in \\Omega$:\n",
    "$$\n",
    "||\\nabla f(\\vec{x}) - \\nabla f(\\vec{y})|| \\leq  L ||\\vec{x} = \\vec{y}|| \n",
    "$$\n",
    "Then:\n",
    "$$\n",
    "f(\\vec{x}) - f(\\vec{y}) \\geq (\\nabla f(\\vec{x}), \\vec{x} - \\vec{y}) - \\frac{L}{2}||\\vec{x}-\\vec{y}||^2\n",
    "$$\n",
    "<em>**Definition**</em>.\n",
    "\n",
    "$\\vec{w}(\\vec{x}) = - \\nabla f(\\vec{x})$ is called **antigradient**\n",
    "\n",
    "If we take our $\\vec{u}_k = \\frac{\\vec{w}_k}{||\\vec{w}_k||}$, from our lemma we have, that: \n",
    "\n",
    "$$\n",
    "f(x_{k}) - f(x_{k+1}) \\geq (\\nabla f(x_k), \\vec{x_k} - \\vec{x_k} - \\beta_k \\frac{\\vec{w_k}}{||\\vec{w_k}||}) - \\frac{L}{2} || \\vec{x_k} - \\vec{x_k} - \\beta_k \\frac{\\vec{w_k}}{||\\vec{w_k}||} ||^2 = \\beta_k||\\nabla f(\\vec{x}_k)|| - \\beta_k \\frac{L}{2} \n",
    "$$\n",
    "As we can see gradient must be always posistive (and $> \\frac{L}{2}$),  so that we have a convergece, we get this when function is convex\n",
    "\n",
    "All methods in which $\\vec{u}_k = \\frac{\\vec{w}_k}{||\\vec{w}_k||}$, are named ***gradient methods***, the methods vary on the way we choose our $\\beta_k > 0$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68777a30-a1ca-4e9d-bf2c-ef35d7345a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mplib\n",
    "import math as m\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from onedim_optimize import upgraded_newton, brent_optimize, one_dim_optimizer, qubic_modified, newton_modified2\n",
    "from scipy.optimize import approx_fprime, minimize\n",
    "import matplotlib.animation as pltanimation\n",
    "from animations import Animate3D\n",
    "\n",
    "from test_functions import *\n",
    "from scipy.misc import derivative\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "def toOneParamFunc(f, x, w):\n",
    "    return lambda p: f(x + p*w) \n",
    "\n",
    "def argmin(f, a, b, eps, onedim_opti):\n",
    "#     fig, ax = plt.subplots()\n",
    "#     ax.plot(np.linspace(a, b, 1000), [f(y) for y in np.linspace(a, b, 1000)])\n",
    "    x, f_ev = onedim_opti(f, a, b, eps)\n",
    "#     ax.scatter(x, f(x))\n",
    "    return x, f_ev\n",
    "\n",
    "def approx_gradient(f, eps):\n",
    "    return lambda x: approx_fprime(x, f, eps)\n",
    "\n",
    "def optimization_result(title, fmin, xmin, K, f_ev, j_ev, h_ev = None, res=None):\n",
    "    print(f\"\"\"\n",
    "{title}\n",
    "Optimization {res}\n",
    "x minimum: {xmin},\n",
    "f minimum: {fmin},\n",
    "number of iterations: {K},\n",
    "number of function evaluations: {f_ev},\n",
    "number of gradient evaluations: {j_ev},\n",
    "{f\"number of hessian evaluations: {h_ev}\" if h_ev != None else ''}\n",
    "\"\"\") if res == 'succes' else print(f\"\"\"{title}\\nOptimization {res}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a2fc117-d476-4c87-a60f-37dd21fec44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sqrt1 = [\n",
    "    danilov,\n",
    "    danilov_gradient,\n",
    "    np.array([-2, 2]),\n",
    "    0.001,\n",
    "    'Square root func 1 test. Starting point (-2, 2)' \n",
    "]\n",
    "\n",
    "test_sqrt2 = [\n",
    "    danilov,\n",
    "    danilov_gradient,\n",
    "    np.array([4, 3]),\n",
    "    0.001,\n",
    "    'Square root func 1 test. Starting point (4, 3)' \n",
    "]\n",
    "\n",
    "test_rosen1 = [\n",
    "    rosenbrok,\n",
    "    rosen_gradient,\n",
    "    np.array([-2, -1]),\n",
    "    1e-4,\n",
    "    'Rosenbrock1 test. Starting point (-2, -1)'\n",
    "]\n",
    "\n",
    "test_rosen2 = [\n",
    "    rosenbrok,\n",
    "    rosen_gradient,\n",
    "    np.array([-3, 4]),\n",
    "    1e-4,\n",
    "    'Rosenbrock2 test. Starting point (-3, 4)'\n",
    "]\n",
    "\n",
    "test_rosen3 = [\n",
    "    rosenbrok,\n",
    "    rosen_gradient,\n",
    "    np.array([3, 3]),\n",
    "    1e-4,\n",
    "    'Rosenbrock3 test. Starting point (3, 3)'\n",
    "]\n",
    "\n",
    "\n",
    "test_himmel1 = [\n",
    "    himmelblau,\n",
    "    himmel_gradient,\n",
    "    np.array([0, -4]),\n",
    "    1e-4,\n",
    "    'Himmelblau1 test. Starting point (0, -4)'\n",
    "]\n",
    "\n",
    "test_himmel2 = [\n",
    "    himmelblau,\n",
    "    himmel_gradient,\n",
    "    np.array([10, 21]),\n",
    "    1e-4,\n",
    "    'Himmelblau1 test. Starting point (10, 21)'\n",
    "]\n",
    "\n",
    "test_himmel3 = [\n",
    "    himmelblau,\n",
    "    himmel_gradient,\n",
    "    np.array([-5, 17]),\n",
    "    1e-4,\n",
    "    'Himmelblau1 test. Starting point (-5, 17)'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# test_rastrigin = [\n",
    "#     rastrigin,\n",
    "#     approx_gradient(rastrigin, np.float64(1e-8)),\n",
    "#     np.array([2, 1]),\n",
    "#     1e-4\n",
    "# ]\n",
    "\n",
    "# test_ackley = [\n",
    "#     ackley,\n",
    "#     approx_gradient(ackley, np.float64(1e-9)),\n",
    "#     np.array([1, 1]),\n",
    "#     1e-4\n",
    "# ]\n",
    "\n",
    "# test_sphere = [\n",
    "#     sphere,\n",
    "#     approx_gradient(sphere, np.float64(1e-9)),\n",
    "#     np.array([-3, 3]),\n",
    "#     1e-5,\n",
    "#     [[-3, 3], [0, 10]]\n",
    "# ]\n",
    "\n",
    "# test_beale = [\n",
    "#     beale,\n",
    "#     approx_gradient(beale, np.float64(1e-9)),\n",
    "#     np.array([3, 1.5]),\n",
    "#     1e-3,\n",
    "#     [[-0.01, 800], [2.9, 1.6]]\n",
    "# ]\n",
    "\n",
    "# test_goldstein = [\n",
    "#     goldstein_price,np.array([2, 1]),\n",
    "#     approx_gradient(goldstein_price, np.float64(1e-9)),\n",
    "#     np.array([-1.3, 1]),\n",
    "#     1e-5,\n",
    "#     [[-1.5, 1], [0, 50000]]\n",
    "# ]\n",
    "\n",
    "# test_booth = [\n",
    "#     booth,\n",
    "#     approx_gradient(booth, np.float64(1e-8)),\n",
    "#     np.array([5, 3]),\n",
    "#     1e-5,\n",
    "#     [[0, 8], [0, 700]]\n",
    "# ]\n",
    "\n",
    "# test_bukin = [\n",
    "#     bukin,\n",
    "#     approx_gradient(bukin, np.float64(1e-8)),\n",
    "#     np.array([-10.5, 1.5]),\n",
    "#     1e-5\n",
    "# ]\n",
    "\n",
    "# test_himmel = [\n",
    "#     himmelblau,\n",
    "#     approx_gradient(himmelblau, np.float64(1e-8)),\n",
    "#     np.array([0, -4]),\n",
    "#     1e-5,\n",
    "#     [[-4, 4], [-0.1, 280]]\n",
    "# ]\n",
    "\n",
    "# test_egg = [\n",
    "#     eggholder,\n",
    "#     approx_gradient(eggholder, np.float64(1e-8)),\n",
    "#     np.array([353, -200]),\n",
    "#     1e-7\n",
    "# ]\n",
    "\n",
    "# test_cross = [\n",
    "#     cross,\n",
    "#     approx_gradient(cross, np.float64(1e-8)),\n",
    "#     np.array([2, -2]),\n",
    "#     1e-4\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da7e1e-bc40-4eca-96a5-e9add234d8a0",
   "metadata": {},
   "source": [
    "### Fastest descent method\n",
    "\n",
    "We will construct relaxational sequence, using this rule:\n",
    "$$\n",
    "\\vec{x}_{k+1} = \\vec{x}_k + \\lambda_k\\vec{w}_K\n",
    "$$\n",
    "\n",
    "Where $\\lambda_k$ is found from\n",
    "$$\n",
    "\\lambda_k = argmin\\{\\psi_k(\\lambda)\\} \\\\\n",
    "\\psi_k(\\lambda) = f(\\vec{x}_{k-1} + \\lambda\\vec{w}_k)\n",
    "$$\n",
    "\n",
    "Finding minimum of $\\psi_k(\\lambda)$ is a pretty complex task of one-dimension minimization. But it is guaranteed that $\\{|\\vec{w}_k|\\}$ convergace to 0.\n",
    "\n",
    "So at start we pick some small $\\epsilon$ and continuing procedure while $|\\vec{w}_k\\| > \\epsilon$, than on some N iteration we pick our $x_* = x_N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19911d95-3937-4870-a8ac-ef815ece89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastest_descent(f, gr, x, epsilon, title, onedim_opt):\n",
    "    try:\n",
    "        anim = Animate3D(f, x, title)\n",
    "        f_ev = 0\n",
    "        j_ev = 0\n",
    "        w = -gr(x) \n",
    "        phi = toOneParamFunc(f, x, w)\n",
    "        anim.add(x)\n",
    "        l, i = argmin(phi, 0, 100, np.divide(epsilon, 1e3), onedim_opt)\n",
    "        f_ev += i\n",
    "        j_ev += 1\n",
    "        k = 1\n",
    "    #     print(x, f(x), l, norm(w))\n",
    "        x = x + l*w\n",
    "        anim.add(x)\n",
    "        while(norm(w) > epsilon):\n",
    "            w = -gr(x) \n",
    "            phi = toOneParamFunc(f, x, w)\n",
    "            l, i = argmin(phi, 0, 100, np.divide(epsilon, 1e3), onedim_opt)\n",
    "            f_ev += i\n",
    "            j_ev += 1\n",
    "            k += 1\n",
    "    #         print(x, f(x), l, norm(w))\n",
    "            x = x + l*w\n",
    "            anim.add(x)\n",
    "        return f(x), x, k, f_ev, j_ev, anim, 'succes'\n",
    "    except Exception as e:\n",
    "        print(\"ERROR:\", e)\n",
    "        return f(x), x, k, f_ev, j_ev, anim, 'fail'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cdbf93e-3bf1-4f62-a866-b2809c946690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_sqrt1, onedim_opt=brent_optimize)\n",
    "# optimization_result(test_sqrt1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# # load_animation(anim, \"Sqrt\", \"Fastest-Desc\", test_num=1, duration=5000)\n",
    "\n",
    "# fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_sqrt2, onedim_opt=brent_optimize)\n",
    "# optimization_result(test_sqrt2[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# # load_animation(anim, \"Sqrt\", \"Fastest-Desc\", test_num=2, duration=5000)\n",
    "\n",
    "# fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_rosen1, onedim_opt=one_dim_optimizer)\n",
    "# optimization_result(test_rosen1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# # load_animation(anim, \"Rosenbrock\", \"Fastest-Desc\", test_num=1, duration=10000)\n",
    "\n",
    "# fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_rosen2, onedim_opt=upgraded_newton)\n",
    "# optimization_result(test_rosen2[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# # load_animation(anim, \"Rosenbrock\", \"Fastest-Desc\", test_num=2, duration=10000)\n",
    "\n",
    "# fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_rosen3, onedim_opt=upgraded_newton)\n",
    "# optimization_result(test_rosen3[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# # load_animation(anim, \"Rosenbrock\", \"Fastest-Desc\", test_num=3, duration=10000)\n",
    "\n",
    "# fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_himmel1, onedim_opt=brent_optimize)\n",
    "# optimization_result(test_himmel1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# # load_animation(anim, \"Himmelblau\", \"Fastest-Desc\", test_num=1, duration=6000)\n",
    "\n",
    "# fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_himmel2, onedim_opt=brent_optimize)\n",
    "# optimization_result(test_himmel2[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# # load_animation(anim, \"Himmelblau\", \"Fastest-Desc\", test_num=2, duration=6000)\n",
    "\n",
    "# fmin, xmin, K, f_ev, j_ev, anim, res = fastest_descent(*test_himmel3, onedim_opt=brent_optimize)\n",
    "# optimization_result(test_himmel3[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# # load_animation(anim, \"Himmelblau\", \"Fastest-Desc\", test_num=3, duration=6000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2c72e-20bd-4ee9-a5e4-520d68c55139",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conjugate gradient method\n",
    "\n",
    "#### Problem \n",
    "\n",
    "$$\n",
    "f(\\vec{x}) = \\frac{1}{2}(Q\\vec{x}, \\vec{x}) + (\\vec{c}, \\vec{x}) \\rightarrow min\n",
    "$$\n",
    "\n",
    "$Q$ is positive determined n-dimsensional matrix, $c \\in \\mathbb{R}$ - constant\n",
    "\n",
    "This function has single point of minimum $x_* = -Q^{-1}\\vec{c}$\n",
    "\n",
    "To find the inverted matrix $Q^{-1}$ we can use\n",
    "$$\n",
    "Q^{-1} = \\sum^n_{i=1}\\frac{p^i(p^i)^T}{(Qp^i, p^i)}\n",
    "$$\n",
    "Where $p^i \\in \\mathbb{R}$ is conjugate vector of matrix $Q$\n",
    "\n",
    "But constructing a system of conjugate vectors is a pretty complex problem.\n",
    "\n",
    "So we do another way, let's construct system of conjugate vectors on every iteration\n",
    "\n",
    "$\\vec{x}_0$ is a starting point, antrigradient in this point is $\\vec{w}_1 = -Qx_0 - c$ and let's choose $\\vec{p}_1 = \\vec{w}$\n",
    "\n",
    "Using $\\vec{x}_k = \\vec{x}_{k-1} + \\lambda_k\\vec{w}_k$\n",
    "\n",
    "We can find that \n",
    "$$\\lambda_1 = \\frac{|\\vec{w}_1|^2}{(Q\\vec{w}_1, \\vec{w}_1)} = \\frac{|\\vec{p}_1|^2}{(Q\\vec{p}_1, \\vec{p}_1)}$$\n",
    "(from minimization of quadratic function)\n",
    "\n",
    "And so $x_1 = x_0 + \\lambda_1\\vec{p}_1$\n",
    "\n",
    "On second iteration (k = 2) we evaluate antigradient $\\vec{w}_2 = -Q\\vec{x_1} - c$\n",
    "\n",
    "Let's assume, that\n",
    "$$\\vec{p}_2 = \\gamma_1\\vec{p}_1 + \\vec{w}_2$$\n",
    "\n",
    "If we product scalarly this equation on $Q\\vec{p}_1 \\not = 0$ and demand that $\\vec{p}_1, \\vec{p}_2$ are conjugate (ortogonal) over the matrix $Q$ ($(Q\\vec{p}_1, \\vec{p_2}) = 0$), we can find $\\gamma_1$\n",
    "$$\\gamma_1 = -\\frac{(Q\\vec{p}_1, \\vec{w}_2)}{(Q\\vec{p}_1, \\vec{p}_1)}$$\n",
    "\n",
    "Contniuing constructing this system of conjugate vectors, we can say, that on every k iteration we have system of equations:\n",
    "$$\n",
    "\\begin{cases}\n",
    "    p_{k+1} = \\gamma\\vec{p_k} + \\vec{w}_{k+1} \\\\\n",
    "    \\gamma_k = - \\frac{(Q\\vec{p}_k, \\vec{w}_{k+1})}{(Q\\vec{p}_k, \\vec{p}_k)} \\\\\n",
    "    \\vec{w}_{k+1} = \\vec{w}_k = \\lambda_kQ\\vec{p}_k \\\\\n",
    "    (Q\\vec{p}_{k+1}, \\vec{p}_i) = 0 \\\\\n",
    "    (\\vec{w}_{k+1}, \\vec{w}_i) = 0, i = \\overline{1, k} \\\\\n",
    "\\end{cases} \\\\\n",
    "\\mbox{also } \\\\\n",
    "\\lambda_k = \\frac{(\\vec{w}_k, \\vec{p}_k)}{(Q\\vec{p}_k, \\vec{p}_k)},\\\\\n",
    "\\vec{x}_k = \\vec{x_1} + \\lambda_k\\vec{p}_k\n",
    "$$\n",
    "\n",
    "With n steps we can find all $\\vec{p}_k$ conjugate vectors and evaluate our minimum $x_* = -Q^{-1}\\vec{c}$\n",
    "\n",
    "To use this method in our problems (non-quadratic function optimization, we need to remove matrix $Q$ from system of equations\n",
    "\n",
    "We can do this, by if on every iteration by doing minimization process:\n",
    "$$\n",
    "\\psi_k(\\lambda) = f(x_{k-1} + \\lambda)\n",
    "$$\n",
    "\n",
    "In fundament of constructing conjuguate directions $\\vec{p}_{k+1} = \\gamma_k\\vec{p}_k + \\vec{w}_{k+1}$ we assume, that $(\\vec{w}_{k+1}, \\vec{w}_i) = 0$\n",
    "\n",
    "Using this we can show that:\n",
    "$$\n",
    "\\begin{cases}\n",
    "    (Q\\vec{p}_k, \\vec{w}_{k+1}) = - \\frac{1}{\\lambda_k}|\\vec{w}_{k+1}|^2 \\\\\n",
    "    (Q\\vec{p}_k, \\vec{p}_{k}) = \\frac{1}{\\lambda_k}(\\vec{w}_k, \\vec{p}_k)\n",
    "\\end{cases} \\\\\n",
    "\\mbox{so from our system of equations we can evaluate $\\gamma$ using one of theese formulas: } \\\\\n",
    "\\gamma_k = \\frac{|\\vec{w}_{k+1}|^2}{|\\vec{w}_k|^2} \\\\\n",
    "\\gamma_k = \\frac{(\\vec{w}_{k+1} - \\vec{w}_k, \\vec{w}_{k+1})}{|\\vec{w}_k|^2} \\\\\n",
    "\\mbox{also if function twice differentiable, we can use Hessian instead of matrix Q:} \\\\\n",
    "\\gamma_k = - \\frac{(H(\\vec{x}_k)\\vec{p}_k, \\vec{w}_{k+1})}{(H(\\vec{x}_k)\\vec{p}_k, \\vec{p}_k)} \\\\\n",
    "$$\n",
    "\n",
    "This method is called ***conjaguate gradients method***\n",
    "\n",
    "Also as every $\\gamma_k$ is different and we need to minimize $\\psi_k(\\lambda)$ this turns us to inevitably errors, to minimize errors, we need to do **restarts** (set $\\gamma_k = 0$). It is common to restart every $n$ times, where $n$ is our dimension number. Also, with non-quadratic functions our procedure of optimization in general don't take $n$ steps, so we choose our $\\epsilon$ and iterate through $\\{\\vec{x}_k\\}$ till our |$\\vec{w}_{k+1|} < \\epsilon$, and then $x_{k-1} \\approx x_*$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "872bd742-b7b8-477b-8406-82ab2cb9ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(f, gr, x, epsilon, title, onedim_opt):\n",
    "    try:\n",
    "        anim = Animate3D(f, x, title)\n",
    "        w = -gr(x) \n",
    "        p = w\n",
    "        j_ev = 1\n",
    "        f_ev = 0\n",
    "        phi = toOneParamFunc(f, x, p)\n",
    "        l, i = argmin(phi, 0, 50, np.divide(epsilon, 1e-3), onedim_opt)\n",
    "        f_ev += i\n",
    "        print(x, f(x), l, p)\n",
    "        x = x + l*p\n",
    "        anim.add(x)\n",
    "        j_ev += 1\n",
    "        k = 1\n",
    "        while norm(w) > epsilon:\n",
    "            w_2 = -gr(x)\n",
    "            gamma = np.divide(np.dot(w_2 - w, w_2), np.power(norm(w), 2))\n",
    "            p = gamma*p + w_2\n",
    "            phi = toOneParamFunc(f, x, p)\n",
    "            l, i = argmin(phi, 0, 50, np.divide(epsilon, 1e-3), onedim_opt) \n",
    "            print(x, f(x), l, p)\n",
    "            x = x + l*p\n",
    "            anim.add(x)\n",
    "            w = w_2\n",
    "            j_ev += 1\n",
    "            f_ev += i\n",
    "            k += 1\n",
    "        return f(x), x, k+1, f_ev, j_ev, anim, 'succes'\n",
    "    except:\n",
    "        return f(x), x, k+1, f_ev, j_ev, anim, 'fail'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d6073a2-9bea-47cc-ba89-1b8db5589627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2  2]\n",
      "[-2  2] 14.0 0.6487564985914555 [ 1.66666667 -4.66666667]\n",
      "[-0.91873917 -1.02753033] 3.8378375525379633 0.4649861120758853 [1.26067893 0.1262156 ]\n",
      "[-0.33254097 -0.96884183] 3.4558392687309456 0.4883975906163525 [0.17483023 0.73176898]\n",
      "[-0.24715431 -0.61144762] 3.3215321425498185 0.3052302411964708 [-0.17803543  0.02779924]\n",
      "[-0.30149611 -0.60296245] 3.316624794849247 0.42579430111429595 [-3.39811932e-05 -1.41725905e-04]\n",
      "\n",
      "Square root func 1 test. Starting point (-2, 2)\n",
      "Optimization succes\n",
      "x minimum: [-0.30151058 -0.6030228 ],\n",
      "f minimum: 3.3166247903563653,\n",
      "number of iterations: 6,\n",
      "number of function evaluations: 91,\n",
      "number of gradient evaluations: 6,\n",
      "\n",
      "\n",
      "[4 3]\n",
      "[4 3] 30.396078054371138 0.9579175373908111 [-4.13785816 -4.35339362]\n",
      "[ 0.0362731 -1.1701921] 3.8546748659213144 0.36428186289794406 [-1.35565542  0.76502107]\n",
      "[-0.45756758 -0.8915088 ] 3.4221435325459257 0.534264663408643 [0.21208542 0.56453098]\n",
      "[-0.34425783 -0.58989984] 3.3199000040496136 0.30440257149223476 [ 0.14190958 -0.03874255]\n",
      "[-0.30106019 -0.60169318] 3.3166270580158907 0.432824759841783 [-0.00107412 -0.00306257]\n",
      "[-0.3015251  -0.60301873] 3.3166247906915656 0.3033690554920711 [ 4.52768448e-05 -1.30698568e-05]\n",
      "\n",
      "Square root func 1 test. Starting point (4, 3)\n",
      "Optimization succes\n",
      "x minimum: [-0.30151136 -0.60302269],\n",
      "f minimum: 3.3166247903554,\n",
      "number of iterations: 7,\n",
      "number of function evaluations: 136,\n",
      "number of gradient evaluations: 7,\n",
      "\n",
      "\n",
      "[-2 -1]\n",
      "[-2 -1] 2509 0.0005326357176186703 [4006 1000]\n",
      "[ 0.13373868 -0.46736428] 24.297195796413085 0.004773572785825058 [-21.87502449  97.63697043]\n",
      "[ 0.02931666 -0.0012871 ] 0.9426869141673023 0.14610953950524677 [1.89736353 0.51336359]\n",
      "[0.30653957 0.07372022] 0.5218785919101685 0.028685371892232556 [7.74037001 6.439977  ]\n",
      "[0.52857497 0.25845335] 0.2660821351977534 0.0030375094002315414 [0.43131487 7.44524841]\n",
      "[0.52988509 0.28106837] 0.22101644804546283 0.11185707434972816 [1.0706234  1.13118704]\n",
      "[0.64964189 0.40759964] 0.14358757462019456 0.00559512925013732 [19.1336882  26.32589669]\n",
      "[0.75669735 0.55489643] 0.09050551799553583 0.002138204514629541 [ 7.2398817  20.19958727]\n",
      "[0.7721777  0.59808728] 0.05223748393574 0.03202457471690847 [2.50695868 3.78141083]\n",
      "[0.85246198 0.71918536] 0.027401584089473466 0.0036359259253573163 [19.58048199 34.45126142]\n",
      "[0.92365517 0.84444759] 0.013382356458801533 0.0013677884291496496 [ 4.47681882 14.9962531 ]\n",
      "[0.92977851 0.86495929] 0.00495326320074011 0.05867866649090872 [0.76350621 1.40581125]\n",
      "[0.97458003 0.94745042] 0.0012011625550681 0.002690312578481579 [ 8.2265918 17.2157862]\n",
      "[0.99671214 0.99376627] 2.177846948101411e-05 0.0017999565393942464 [1.61630386 3.02612266]\n",
      "[0.99962141 0.99921316] 2.3219978434076367e-07 0.002903272035702337 [0.13281784 0.27552991]\n",
      "[1.00000702 1.0000131 ] 1.3808465389090122e-10 0.0014658093371872237 [-0.00478638 -0.00892969]\n",
      "[1.         1.00000001] 1.699194885561191e-17 0.003128082455715883 [-1.05705631e-06 -2.19280008e-06]\n",
      "\n",
      "Rosenbrock1 test. Starting point (-2, -1)\n",
      "Optimization succes\n",
      "x minimum: [1. 1.],\n",
      "f minimum: 2.815130258966867e-24,\n",
      "number of iterations: 18,\n",
      "number of function evaluations: 356,\n",
      "number of gradient evaluations: 18,\n",
      "\n",
      "\n",
      "[-3  4]\n",
      "[-3  4] 2516 0.0008664029139165134 [6008 1000]\n",
      "[2.20534871 4.86640291] 1.4536720623320714 0.0017077821130607553 [ 0.09459507 -0.56799564]\n",
      "[2.20551025 4.8654329 ] 1.4533889355642677 0.21221876537519693 [-0.82348686 -3.63275807]\n",
      "[2.03075089 4.09449347] 1.1492112818089921 0.0004299214402390623 [ -306.0840426  -1229.73241361]\n",
      "[1.8991588  3.56580514] 0.9765783536904968 0.0052403888321064015 [-139.87112762 -421.39478077]\n",
      "[1.1661797  1.35753264] 0.02821225656201798 0.00018459760747872145 [ 4.60655024 18.80065621]\n",
      "[1.16703006 1.36100319] 0.027990429446352216 0.039975105217925966 [-1.92277621 -4.47150159]\n",
      "[1.09016688 1.18225445] 0.01198570647485939 0.003665625024066437 [-24.90844205 -49.9675528 ]\n",
      "[0.99886187 0.99909213] 0.0001881903915674077 0.0008146326545410372 [ -5.24629551 -11.89804654]\n",
      "[0.99458807 0.9893996 ] 3.305929022376241e-05 0.00701395587359709 [0.78171174 1.53426684]\n",
      "[1.00007096 1.00016088] 4.095514420176553e-08 0.001173049495070835 [-0.05438222 -0.12512837]\n",
      "[1.00000717 1.00001409] 5.702626820640488e-11 0.007986558309323785 [-0.00089762 -0.00176552]\n",
      "[1.         0.99999999] 7.385409822550258e-17 0.001139467818416565 [2.37036821e-06 5.45671585e-06]\n",
      "\n",
      "Rosenbrock2 test. Starting point (-3, 4)\n",
      "Optimization succes\n",
      "x minimum: [1. 1.],\n",
      "f minimum: 8.542910501561122e-25,\n",
      "number of iterations: 14,\n",
      "number of function evaluations: 305,\n",
      "number of gradient evaluations: 14,\n",
      "\n",
      "\n",
      "[3 3]\n",
      "[3 3] 3604 0.0006876437035339202 [-7204  1200]\n",
      "[-1.95378524  3.82517244] 8.73108142055415 0.0018834044345383302 [-0.26338854 -1.57907347]\n",
      "[-1.95428131  3.82219841] 8.728667861281629 0.10914541226822624 [ 2.22553138 -8.6973259 ]\n",
      "[-1.71137477  2.87292519] 7.6637927369969026 0.0004582643297022828 [  362.34886552 -1234.19723156]\n",
      "[-1.54532321  2.30733662] 7.129712533113981 0.004127451822143787 [ 179.8969593  -409.39097191]\n",
      "[-0.80280717  0.61759511] 3.322497579371382 0.00041530103949659 [-19.60472059  77.86158387]\n",
      "[-0.81094904  0.64993111] 3.2854765524431286 0.018516286712425372 [ 10.87847714 -17.34938137]\n",
      "[-0.60952003  0.32868499] 2.773992936347828 0.0034783262262567914 [ 58.19859657 -62.46378666]\n",
      "[-0.40708633  0.11141556] 2.2747813447463945 0.0031710110912715824 [12.00513656 10.48676485]\n",
      "[-0.36901791  0.14466921] 1.8814265175976796 0.09211208497915646 [ 1.77931699 -1.44113174]\n",
      "[-0.20512131  0.01192356] 1.5432268247137166 0.003981980513847276 [ 26.98577507 -11.87068619]\n",
      "[-0.09766448 -0.03534528] 1.4063213726693886 0.03182284645405756 [14.14659844  4.49082519]\n",
      "[0.35252055 0.10756556] 0.44713594779444066 0.0025959413250778414 [-3.04873666  2.70990702]\n",
      "[0.34460621 0.11460032] 0.4312658645155078 0.15230764259447785 [0.92614846 0.66366278]\n",
      "[0.4856657  0.21568123] 0.3053031395854098 0.011200263189455434 [14.69917147 16.64464743]\n",
      "[0.65030029 0.40210566] 0.1654906822915897 0.002244171150597422 [ 0.67229872 10.2483883 ]\n",
      "[0.65180904 0.4251048 ] 0.12124318343402125 0.12629229260372685 [0.83607803 1.08683384]\n",
      "[0.75739925 0.56236354] 0.07160173607307646 0.0049708988470489055 [20.12397289 32.23313106]\n",
      "[0.85743349 0.72259117] 0.03620375923399209 0.0015339173039296502 [ 3.45706102 14.52313475]\n",
      "[0.86273633 0.74486846] 0.018872059812815502 0.08004356073258362 [0.83063198 1.42144647]\n",
      "[0.92922307 0.85864609] 0.0073224287224261456 0.0045264060477424265 [14.68222062 28.90419011]\n",
      "[0.99568076 0.9894782 ] 0.00038041205275661977 0.0008862088104817915 [-4.35637677 -6.72152715]\n",
      "[0.99182011 0.98352152] 7.035547788555242e-05 0.03074424082679481 [0.26599404 0.53589538]\n",
      "[0.99999789 0.99999722] 2.10663852129208e-10 0.0010045717407987783 [0.00310358 0.00479981]\n",
      "[1.00000101 1.00000204] 1.0647820861637178e-12 0.031786967164376345 [-3.17011590e-05 -6.41012569e-05]\n",
      "\n",
      "Rosenbrock3 test. Starting point (3, 3)\n",
      "Optimization succes\n",
      "x minimum: [1. 1.],\n",
      "f minimum: 3.1713530224938986e-21,\n",
      "number of iterations: 26,\n",
      "number of function evaluations: 522,\n",
      "number of gradient evaluations: 26,\n",
      "\n",
      "\n",
      "[ 0 -4]\n",
      "[ 0 -4] 306 0.0403880350677067 [-18 174]\n",
      "[-0.72698463  3.0275181 ] 57.48314661691331 0.08354784416261435 [-24.8819624    0.91957773]\n",
      "[-2.80581895  3.10434684] 0.029040733681487078 0.012115205275281616 [-0.41100711  2.16085451]\n",
      "[-2.81079838  3.13052603] 0.0010805242261572319 0.015164129248806547 [0.37441959 0.05273163]\n",
      "[-2.80512064  3.13132566] 7.117613688688944e-09 0.012562135887066363 [ 0.00020211 -0.00104655]\n",
      "[-2.8051181   3.13131252] 3.916089947929094e-15 0.015202020472868876 [7.11774446e-07 9.97551059e-08]\n",
      "\n",
      "Himmelblau1 test. Starting point (0, -4)\n",
      "Optimization succes\n",
      "x minimum: [-2.80511809  3.13131252],\n",
      "f minimum: 4.585153668489987e-23,\n",
      "number of iterations: 7,\n",
      "number of function evaluations: 115,\n",
      "number of gradient evaluations: 7,\n",
      "\n",
      "\n",
      "[10 21]\n",
      "[10 21] 209236 0.0006529818109318818 [ -5288 -37516]\n",
      "[ 6.54703218 -3.49726562] 943.3695564700097 0.013052241676846335 [-768.62474581   92.37181795]\n",
      "[-3.48524376 -2.29160633] 28.702794571505997 0.02177679674899083 [ -9.14035304 -45.24708542]\n",
      "[-3.68429137 -3.27694291] 0.49704754045518024 0.008594313726650453 [-11.12870351  -1.15874129]\n",
      "[-3.77993494 -3.2869015 ] 0.0005668111310860951 0.011621945584274944 [0.05528872 0.31984775]\n",
      "[-3.77929237 -3.28318424] 1.7833944218723463e-08 0.008410195283520593 [-0.00212582 -0.0002077 ]\n",
      "[-3.77931025 -3.28318599] 1.6620616427703254e-16 0.01168167385096448 [-2.98178179e-08 -1.72418408e-07]\n",
      "\n",
      "Himmelblau1 test. Starting point (10, 21)\n",
      "Optimization succes\n",
      "x minimum: [-3.77931025 -3.28318599],\n",
      "f minimum: 4.1836114770467326e-23,\n",
      "number of iterations: 8,\n",
      "number of function evaluations: 137,\n",
      "number of gradient evaluations: 8,\n",
      "\n",
      "\n",
      "[-5 17]\n",
      "[-5 17] 77690 0.0010925044301647207 [    66 -18898]\n",
      "[-4.92789471 -3.64614872] 94.75833442408556 0.005642584557248962 [187.25361594  -1.20135353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.87130035 -3.65292746] 6.2252273369625035 0.010387152501625715 [ 6.59202743 35.4196919 ]\n",
      "[-3.80282795 -3.28501772] 0.031278088196601585 0.008273027743975269 [2.83770674 0.19568018]\n",
      "[-3.77935153 -3.28339885] 1.8498904157638678e-06 0.011864374438975596 [0.00345391 0.01793936]\n",
      "[-3.77931055 -3.28318601] 4.84972310757051e-12 0.0083237121670089 [3.52438012e-05 2.49646753e-06]\n",
      "\n",
      "Himmelblau1 test. Starting point (-5, 17)\n",
      "Optimization succes\n",
      "x minimum: [-3.77931025 -3.28318599],\n",
      "f minimum: 1.153209283170618e-22,\n",
      "number of iterations: 7,\n",
      "number of function evaluations: 114,\n",
      "number of gradient evaluations: 7,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_sqrt1, onedim_opt=brent_optimize)\n",
    "optimization_result(test_sqrt1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# load_animation(anim, \"Sqrt\", \"Conj-Grad\", test_num=1, duration=5000)\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_sqrt2, onedim_opt=brent_optimize)\n",
    "optimization_result(test_sqrt2[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# load_animation(anim, \"Sqrt\", \"Conj-Grad\", test_num=2, duration=5000)\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_rosen1, onedim_opt=brent_optimize)\n",
    "optimization_result(test_rosen1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# load_animation(anim, \"Rosenbrock\", \"Conj-Grad\", test_num=1, duration=5000)\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_rosen2, onedim_opt=brent_optimize)\n",
    "optimization_result(test_rosen2[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# load_animation(anim, \"Rosenbrock\", \"Conj-Grad\", test_num=2, duration=7000)\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_rosen3, onedim_opt=brent_optimize)\n",
    "optimization_result(test_rosen3[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# load_animation(anim, \"Rosenbrock\", \"Conj-Grad\", test_num=3, duration=7000)\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_himmel1, onedim_opt=brent_optimize)\n",
    "optimization_result(test_himmel1[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# load_animation(anim, \"Himmelblau\", \"Conj-Grad\", test_num=1, duration=5000)\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_himmel2, onedim_opt=brent_optimize)\n",
    "optimization_result(test_himmel2[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# load_animation(anim, \"Himmelblau\", \"Conj-Grad\", test_num=2, duration=5000)\n",
    "\n",
    "fmin, xmin, K, f_ev, j_ev, anim, res = conjugate_gradient(*test_himmel3, onedim_opt=brent_optimize)\n",
    "optimization_result(test_himmel3[4], fmin, xmin, K, f_ev, j_ev, res=res)\n",
    "# load_animation(anim, \"Himmelblau\", \"Conj-Grad\", test_num=3, duration=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70df562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
